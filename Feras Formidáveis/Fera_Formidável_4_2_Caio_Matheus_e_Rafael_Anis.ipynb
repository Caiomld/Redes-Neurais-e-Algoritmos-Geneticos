{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsLApC23Laod"
      },
      "source": [
        "## **INTRODUÇÃO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbTBbcyILePt"
      },
      "source": [
        "O objetivo deste notebook é implementar uma estratégia de Parada Antecipada (Early Stopping) em uma rede neural MLP. Para isso, vamos treinar uma MLP com o dataset didático \"Iris\", do seaborn. Os dados serão divididos em treino, validação e teste.\n",
        "\n",
        "Para o Early Stopping, enquanto otimizamos os parâmetros da nossa MLP com os dados de treino, vamos comparando a perda do teste com a perda de validação. Esperamos que, de início, as duas perdas diminuem ao passar das épocas. Mas, enquanto a perda do treino deve sempre diminuir, é esperado que em alguma época a perda da validação aumente, e é aí que aplicamos um early stopping, evitando um overfitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **AUTORES E CONTRIBUIÇÕES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Autores:**\n",
        "\n",
        "* Caio Matheus Leão Dantas\n",
        "* Rafael Anis Shaikhzadeh Santos\n",
        "\n",
        "**Contribuições:** Ambos discutiram o problema juntos, Caio Matheus realizou a primeira versão do código e Rafael Anis revisou o código e fez alterações para a versão final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNee12hsLd8R"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm-JzsVhLhXg"
      },
      "source": [
        "## **CÓDIGO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsmeP5TxfsrU"
      },
      "source": [
        "#### ***Bibliotecas Importadas***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiY1wZ9lgvfS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from graphviz import Digraph\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funções importadas do material de aula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WvV2UPv5mIn"
      },
      "source": [
        "#### FUNÇÃO GRAFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykS2OtFUoYfm"
      },
      "outputs": [],
      "source": [
        "def _tracar(folha):\n",
        "    \"\"\"Função modificada da criada por Andrej Karpathy para construção de grafo.\n",
        "\n",
        "    Referência: https://github.com/karpathy/micrograd\n",
        "\n",
        "    \"\"\"\n",
        "    vertices = set()\n",
        "    arestas = set()\n",
        "\n",
        "    def construir(v):\n",
        "        \"\"\"Função recursiva para traçar o grafo.\"\"\"\n",
        "        if v not in vertices:\n",
        "            vertices.add(v)\n",
        "            for progenitor in v.progenitor:\n",
        "                arestas.add((progenitor, v))\n",
        "                construir(progenitor)\n",
        "\n",
        "    construir(folha)\n",
        "\n",
        "    return vertices, arestas\n",
        "\n",
        "\n",
        "def plota_grafo(folha):\n",
        "    \"\"\"Função modificada da criada por Andrej Karpathy para construção de grafo.\n",
        "\n",
        "    Referência: https://github.com/karpathy/micrograd\n",
        "\n",
        "    \"\"\"\n",
        "    grafo = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})\n",
        "    vertices, arestas = _tracar(folha)\n",
        "\n",
        "    for v in vertices:\n",
        "        id_vertice = str(id(v))\n",
        "\n",
        "        if hasattr(v, \"rotulo\") and (hasattr(v, \"grad\")):\n",
        "            texto = \"{ \" + f\"{v.rotulo} | data {v.data:.3f} | grad {v.grad:.3f}\" + \" }\"\n",
        "\n",
        "        elif hasattr(v, \"rotulo\"):\n",
        "            texto = \"{ \" + f\"{v.rotulo} | data {v.data:.3f}\" + \" }\"\n",
        "\n",
        "        else:\n",
        "            texto = \"{ \" + f\"data {v.data:.3f}\" + \" }\"\n",
        "\n",
        "        grafo.node(name=id_vertice, label=texto, shape=\"record\")\n",
        "\n",
        "        if v.operador_mae:\n",
        "            grafo.node(name=id_vertice + v.operador_mae, label=v.operador_mae)\n",
        "            grafo.edge(id_vertice + v.operador_mae, id_vertice)\n",
        "\n",
        "    for vertice1, vertice2 in arestas:\n",
        "        grafo.edge(str(id(vertice1)), str(id(vertice2)) + vertice2.operador_mae)\n",
        "\n",
        "    return grafo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-KuktP-5oUE"
      },
      "source": [
        "#### CLASSE DA REDE NEURAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DcBh571nKEU"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_dados_entrada, neuronios_c1, neuronios_c2, num_targets):\n",
        "        super().__init__()\n",
        "\n",
        "        self.camadas = nn.Sequential(\n",
        "            nn.Linear(num_dados_entrada, neuronios_c1),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(neuronios_c1, neuronios_c2),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(neuronios_c2, num_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.camadas(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adLBmemGf80P"
      },
      "source": [
        "#### Tratando dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ4bDi_K5vbD"
      },
      "source": [
        "Importando nosso DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "JmxgEqqUneOV",
        "outputId": "312a24b2-2db1-4d76-85b8-ecb61ce777dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3524896872134512,\n        \"min\": 4.3,\n        \"max\": 5.8,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          5.8,\n          5.2,\n          5.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3790643690962886,\n        \"min\": 2.3,\n        \"max\": 4.4,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          3.5,\n          3.0,\n          3.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1736639964801841,\n        \"min\": 1.0,\n        \"max\": 1.9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.0,\n          1.3,\n          1.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10538558938004569,\n        \"min\": 0.1,\n        \"max\": 0.6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.2,\n          0.4,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"setosa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-11f66938-f61a-4750-8128-0419d6ca0b8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5.2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>5.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>5.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4.5</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.6</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.4</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>5.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11f66938-f61a-4750-8128-0419d6ca0b8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11f66938-f61a-4750-8128-0419d6ca0b8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11f66938-f61a-4750-8128-0419d6ca0b8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2a45cb94-0092-4f0f-8d4d-3226d8e1e38c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a45cb94-0092-4f0f-8d4d-3226d8e1e38c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2a45cb94-0092-4f0f-8d4d-3226d8e1e38c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_bf74e805-ba71-4568-ae7e-af4fb365f164\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bf74e805-ba71-4568-ae7e-af4fb365f164 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width species\n",
              "0            5.1          3.5           1.4          0.2  setosa\n",
              "1            4.9          3.0           1.4          0.2  setosa\n",
              "2            4.7          3.2           1.3          0.2  setosa\n",
              "3            4.6          3.1           1.5          0.2  setosa\n",
              "4            5.0          3.6           1.4          0.2  setosa\n",
              "5            5.4          3.9           1.7          0.4  setosa\n",
              "6            4.6          3.4           1.4          0.3  setosa\n",
              "7            5.0          3.4           1.5          0.2  setosa\n",
              "8            4.4          2.9           1.4          0.2  setosa\n",
              "9            4.9          3.1           1.5          0.1  setosa\n",
              "10           5.4          3.7           1.5          0.2  setosa\n",
              "11           4.8          3.4           1.6          0.2  setosa\n",
              "12           4.8          3.0           1.4          0.1  setosa\n",
              "13           4.3          3.0           1.1          0.1  setosa\n",
              "14           5.8          4.0           1.2          0.2  setosa\n",
              "15           5.7          4.4           1.5          0.4  setosa\n",
              "16           5.4          3.9           1.3          0.4  setosa\n",
              "17           5.1          3.5           1.4          0.3  setosa\n",
              "18           5.7          3.8           1.7          0.3  setosa\n",
              "19           5.1          3.8           1.5          0.3  setosa\n",
              "20           5.4          3.4           1.7          0.2  setosa\n",
              "21           5.1          3.7           1.5          0.4  setosa\n",
              "22           4.6          3.6           1.0          0.2  setosa\n",
              "23           5.1          3.3           1.7          0.5  setosa\n",
              "24           4.8          3.4           1.9          0.2  setosa\n",
              "25           5.0          3.0           1.6          0.2  setosa\n",
              "26           5.0          3.4           1.6          0.4  setosa\n",
              "27           5.2          3.5           1.5          0.2  setosa\n",
              "28           5.2          3.4           1.4          0.2  setosa\n",
              "29           4.7          3.2           1.6          0.2  setosa\n",
              "30           4.8          3.1           1.6          0.2  setosa\n",
              "31           5.4          3.4           1.5          0.4  setosa\n",
              "32           5.2          4.1           1.5          0.1  setosa\n",
              "33           5.5          4.2           1.4          0.2  setosa\n",
              "34           4.9          3.1           1.5          0.2  setosa\n",
              "35           5.0          3.2           1.2          0.2  setosa\n",
              "36           5.5          3.5           1.3          0.2  setosa\n",
              "37           4.9          3.6           1.4          0.1  setosa\n",
              "38           4.4          3.0           1.3          0.2  setosa\n",
              "39           5.1          3.4           1.5          0.2  setosa\n",
              "40           5.0          3.5           1.3          0.3  setosa\n",
              "41           4.5          2.3           1.3          0.3  setosa\n",
              "42           4.4          3.2           1.3          0.2  setosa\n",
              "43           5.0          3.5           1.6          0.6  setosa\n",
              "44           5.1          3.8           1.9          0.4  setosa\n",
              "45           4.8          3.0           1.4          0.3  setosa\n",
              "46           5.1          3.8           1.6          0.2  setosa\n",
              "47           4.6          3.2           1.4          0.2  setosa\n",
              "48           5.3          3.7           1.5          0.2  setosa\n",
              "49           5.0          3.3           1.4          0.2  setosa"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = sns.load_dataset(\"iris\")\n",
        "df = df.dropna()\n",
        "df = df[df[\"species\"] == \"setosa\"] #só analisaremos as plantas da espécie setosa\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzKZhnvc5yEr"
      },
      "source": [
        "Definindo os nossos atributos e target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "SqEpaDH2pkgU",
        "outputId": "817c458a-beb7-4f2a-e2b3-f260598dfd3f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3524896872134512,\n        \"min\": 4.3,\n        \"max\": 5.8,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          5.8,\n          5.2,\n          5.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3790643690962886,\n        \"min\": 2.3,\n        \"max\": 4.4,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          3.5,\n          3.0,\n          3.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1736639964801841,\n        \"min\": 1.0,\n        \"max\": 1.9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.0,\n          1.3,\n          1.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10538558938004569,\n        \"min\": 0.1,\n        \"max\": 0.6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.2,\n          0.4,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3692df20-60a1-4279-ab30-02105845d47b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5.2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>5.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>5.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4.5</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>5.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3692df20-60a1-4279-ab30-02105845d47b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3692df20-60a1-4279-ab30-02105845d47b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3692df20-60a1-4279-ab30-02105845d47b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4cf10ce9-85af-486c-87c9-9f1b247bb306\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4cf10ce9-85af-486c-87c9-9f1b247bb306')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4cf10ce9-85af-486c-87c9-9f1b247bb306 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_db8fa8e7-0f93-446e-baa3-8e1fd4f5296f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_db8fa8e7-0f93-446e-baa3-8e1fd4f5296f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width\n",
              "0            5.1          3.5           1.4          0.2\n",
              "1            4.9          3.0           1.4          0.2\n",
              "2            4.7          3.2           1.3          0.2\n",
              "3            4.6          3.1           1.5          0.2\n",
              "4            5.0          3.6           1.4          0.2\n",
              "5            5.4          3.9           1.7          0.4\n",
              "6            4.6          3.4           1.4          0.3\n",
              "7            5.0          3.4           1.5          0.2\n",
              "8            4.4          2.9           1.4          0.2\n",
              "9            4.9          3.1           1.5          0.1\n",
              "10           5.4          3.7           1.5          0.2\n",
              "11           4.8          3.4           1.6          0.2\n",
              "12           4.8          3.0           1.4          0.1\n",
              "13           4.3          3.0           1.1          0.1\n",
              "14           5.8          4.0           1.2          0.2\n",
              "15           5.7          4.4           1.5          0.4\n",
              "16           5.4          3.9           1.3          0.4\n",
              "17           5.1          3.5           1.4          0.3\n",
              "18           5.7          3.8           1.7          0.3\n",
              "19           5.1          3.8           1.5          0.3\n",
              "20           5.4          3.4           1.7          0.2\n",
              "21           5.1          3.7           1.5          0.4\n",
              "22           4.6          3.6           1.0          0.2\n",
              "23           5.1          3.3           1.7          0.5\n",
              "24           4.8          3.4           1.9          0.2\n",
              "25           5.0          3.0           1.6          0.2\n",
              "26           5.0          3.4           1.6          0.4\n",
              "27           5.2          3.5           1.5          0.2\n",
              "28           5.2          3.4           1.4          0.2\n",
              "29           4.7          3.2           1.6          0.2\n",
              "30           4.8          3.1           1.6          0.2\n",
              "31           5.4          3.4           1.5          0.4\n",
              "32           5.2          4.1           1.5          0.1\n",
              "33           5.5          4.2           1.4          0.2\n",
              "34           4.9          3.1           1.5          0.2\n",
              "35           5.0          3.2           1.2          0.2\n",
              "36           5.5          3.5           1.3          0.2\n",
              "37           4.9          3.6           1.4          0.1\n",
              "38           4.4          3.0           1.3          0.2\n",
              "39           5.1          3.4           1.5          0.2\n",
              "40           5.0          3.5           1.3          0.3\n",
              "41           4.5          2.3           1.3          0.3\n",
              "42           4.4          3.2           1.3          0.2\n",
              "43           5.0          3.5           1.6          0.6\n",
              "44           5.1          3.8           1.9          0.4\n",
              "45           4.8          3.0           1.4          0.3\n",
              "46           5.1          3.8           1.6          0.2\n",
              "47           4.6          3.2           1.4          0.2\n",
              "48           5.3          3.7           1.5          0.2\n",
              "49           5.0          3.3           1.4          0.2"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = [\n",
        "    \"sepal_length\",\n",
        "    \"sepal_width\",\n",
        "    \"petal_length\"\n",
        "]\n",
        "y = [\"petal_width\"]\n",
        "\n",
        "df = df.reindex(X + y, axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq7EdHcKg6JX"
      },
      "source": [
        "#### Split de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA2pOxMLhF6g"
      },
      "source": [
        "Vamos fazer isso em dois passos, primeiro dividir em `teste` e `treino/validação`, e depois dividir o treino/validação em `treino` e `validação`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEDxq0FwgYkf"
      },
      "outputs": [],
      "source": [
        "TAMANHO_TESTE = 0.1\n",
        "TAMANHO_VALIDACAO = 0.1\n",
        "TAMANHO_TREINO = 1 - TAMANHO_TESTE - TAMANHO_VALIDACAO\n",
        "TAMANHO_TREINO, TAMANHO_VALIDACAO, TAMANHO_TESTE\n",
        "SEMENTE_ALEATORIA = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos os nossos DataFrames de teste e de treino/validação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-1r_HcO2z-e"
      },
      "outputs": [],
      "source": [
        "indices = df.index\n",
        "indices_treino_val, indices_teste = train_test_split(\n",
        "    indices, test_size=TAMANHO_TESTE, random_state=SEMENTE_ALEATORIA\n",
        ")\n",
        "\n",
        "df_treino_val = df.loc[indices_treino_val]\n",
        "df_teste = df.loc[indices_teste]\n",
        "\n",
        "X_teste = df_teste.reindex(X, axis=1).values\n",
        "y_teste = df_teste.reindex(y, axis=1).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "2IQMhzKk4Ztd",
        "outputId": "38134fe7-916d-4621-ff8a-26f9d4affd63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_treino_val\",\n  \"rows\": 45,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3605271112964987,\n        \"min\": 4.3,\n        \"max\": 5.8,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          5.0,\n          5.7,\n          4.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.39271137266728373,\n        \"min\": 2.3,\n        \"max\": 4.4,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          3.2,\n          4.4,\n          3.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16462384426628512,\n        \"min\": 1.0,\n        \"max\": 1.9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.7,\n          1.4,\n          1.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10090499582190258,\n        \"min\": 0.1,\n        \"max\": 0.6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3,\n          0.6,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_treino_val"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a6b79488-fe09-4405-b989-aa51c5f1211f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>5.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5.2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4.5</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6b79488-fe09-4405-b989-aa51c5f1211f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a6b79488-fe09-4405-b989-aa51c5f1211f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a6b79488-fe09-4405-b989-aa51c5f1211f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6140f591-7d4a-44b6-a1b4-207ffe9a02ed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6140f591-7d4a-44b6-a1b4-207ffe9a02ed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6140f591-7d4a-44b6-a1b4-207ffe9a02ed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4472ec09-5225-45f1-a979-4244006fdedc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_treino_val')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4472ec09-5225-45f1-a979-4244006fdedc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_treino_val');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width\n",
              "22           4.6          3.6           1.0          0.2\n",
              "33           5.5          4.2           1.4          0.2\n",
              "0            5.1          3.5           1.4          0.2\n",
              "17           5.1          3.5           1.4          0.3\n",
              "45           4.8          3.0           1.4          0.3\n",
              "39           5.1          3.4           1.5          0.2\n",
              "38           4.4          3.0           1.3          0.2\n",
              "16           5.4          3.9           1.3          0.4\n",
              "12           4.8          3.0           1.4          0.1\n",
              "11           4.8          3.4           1.6          0.2\n",
              "1            4.9          3.0           1.4          0.2\n",
              "32           5.2          4.1           1.5          0.1\n",
              "46           5.1          3.8           1.6          0.2\n",
              "30           4.8          3.1           1.6          0.2\n",
              "31           5.4          3.4           1.5          0.4\n",
              "2            4.7          3.2           1.3          0.2\n",
              "28           5.2          3.4           1.4          0.2\n",
              "42           4.4          3.2           1.3          0.2\n",
              "7            5.0          3.4           1.5          0.2\n",
              "25           5.0          3.0           1.6          0.2\n",
              "47           4.6          3.2           1.4          0.2\n",
              "4            5.0          3.6           1.4          0.2\n",
              "43           5.0          3.5           1.6          0.6\n",
              "37           4.9          3.6           1.4          0.1\n",
              "24           4.8          3.4           1.9          0.2\n",
              "35           5.0          3.2           1.2          0.2\n",
              "6            4.6          3.4           1.4          0.3\n",
              "14           5.8          4.0           1.2          0.2\n",
              "18           5.7          3.8           1.7          0.3\n",
              "34           4.9          3.1           1.5          0.2\n",
              "29           4.7          3.2           1.6          0.2\n",
              "15           5.7          4.4           1.5          0.4\n",
              "9            4.9          3.1           1.5          0.1\n",
              "13           4.3          3.0           1.1          0.1\n",
              "27           5.2          3.5           1.5          0.2\n",
              "10           5.4          3.7           1.5          0.2\n",
              "21           5.1          3.7           1.5          0.4\n",
              "40           5.0          3.5           1.3          0.3\n",
              "19           5.1          3.8           1.5          0.3\n",
              "8            4.4          2.9           1.4          0.2\n",
              "26           5.0          3.4           1.6          0.4\n",
              "5            5.4          3.9           1.7          0.4\n",
              "41           4.5          2.3           1.3          0.3\n",
              "20           5.4          3.4           1.7          0.2\n",
              "3            4.6          3.1           1.5          0.2"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_treino_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "_J1y5YPO44SL",
        "outputId": "8bca59fb-d3d9-44c9-8eaf-9fc6cc831fe2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_teste\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20000000000000007,\n        \"min\": 5.0,\n        \"max\": 5.5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5.0,\n          5.3,\n          5.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22803508501982767,\n        \"min\": 3.3,\n        \"max\": 3.8,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.3,\n          3.7,\n          3.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24083189157584586,\n        \"min\": 1.3,\n        \"max\": 1.9,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.4,\n          1.5,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1414213562373095,\n        \"min\": 0.2,\n        \"max\": 0.5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4,\n          0.2,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_teste"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c84590a4-752f-470f-80fd-84045fa0d1d0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>5.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>5.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c84590a4-752f-470f-80fd-84045fa0d1d0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c84590a4-752f-470f-80fd-84045fa0d1d0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c84590a4-752f-470f-80fd-84045fa0d1d0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0ca7260a-9192-4229-9682-68330e3adc00\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ca7260a-9192-4229-9682-68330e3adc00')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0ca7260a-9192-4229-9682-68330e3adc00 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8298fc30-b034-4b02-a7be-004d0d53a9ce\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_teste')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8298fc30-b034-4b02-a7be-004d0d53a9ce button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_teste');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width\n",
              "44           5.1          3.8           1.9          0.4\n",
              "49           5.0          3.3           1.4          0.2\n",
              "36           5.5          3.5           1.3          0.2\n",
              "23           5.1          3.3           1.7          0.5\n",
              "48           5.3          3.7           1.5          0.2"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDMsiNP56G9N"
      },
      "source": [
        "E definimos, também, os nossos DataFrames de treino e validação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWuW0UGN4xZs"
      },
      "outputs": [],
      "source": [
        "indices = df_treino_val.index\n",
        "indices_treino, indices_val = train_test_split(\n",
        "    indices, test_size=TAMANHO_TESTE, random_state=SEMENTE_ALEATORIA\n",
        ")\n",
        "\n",
        "df_treino = df.loc[indices_treino]\n",
        "df_val = df.loc[indices_val]\n",
        "\n",
        "X_treino = df_treino.reindex(X, axis=1).values\n",
        "y_treino = df_treino.reindex(y, axis=1).values\n",
        "\n",
        "X_val = df_val.reindex(X, axis=1).values\n",
        "y_val = df_val.reindex(y, axis=1).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ulbRgvb441r9",
        "outputId": "1b15bb06-aaf5-4684-9883-25b5f4b11c1a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_treino\",\n  \"rows\": 40,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.34626209133752417,\n        \"min\": 4.3,\n        \"max\": 5.8,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          5.4,\n          4.3,\n          4.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3990694303707191,\n        \"min\": 2.3,\n        \"max\": 4.4,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          3.2,\n          3.9,\n          3.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.166332999331662,\n        \"min\": 1.0,\n        \"max\": 1.9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.1,\n          1.5,\n          1.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10609623111312795,\n        \"min\": 0.1,\n        \"max\": 0.6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2,\n          0.3,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_treino"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-682cde1b-0953-444d-9b45-985ed676313d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5.2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>5.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4.5</td>\n",
              "      <td>2.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.7</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-682cde1b-0953-444d-9b45-985ed676313d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-682cde1b-0953-444d-9b45-985ed676313d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-682cde1b-0953-444d-9b45-985ed676313d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7bd95fb-9002-4bfe-bd45-0bbd89fb38e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7bd95fb-9002-4bfe-bd45-0bbd89fb38e9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7bd95fb-9002-4bfe-bd45-0bbd89fb38e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7228725e-3c4d-4b25-9457-49b1bf47657a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_treino')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7228725e-3c4d-4b25-9457-49b1bf47657a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_treino');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width\n",
              "37           4.9          3.6           1.4          0.1\n",
              "9            4.9          3.1           1.5          0.1\n",
              "22           4.6          3.6           1.0          0.2\n",
              "15           5.7          4.4           1.5          0.4\n",
              "43           5.0          3.5           1.6          0.6\n",
              "28           5.2          3.4           1.4          0.2\n",
              "32           5.2          4.1           1.5          0.1\n",
              "33           5.5          4.2           1.4          0.2\n",
              "27           5.2          3.5           1.5          0.2\n",
              "41           4.5          2.3           1.3          0.3\n",
              "18           5.7          3.8           1.7          0.3\n",
              "26           5.0          3.4           1.6          0.4\n",
              "29           4.7          3.2           1.6          0.2\n",
              "35           5.0          3.2           1.2          0.2\n",
              "21           5.1          3.7           1.5          0.4\n",
              "16           5.4          3.9           1.3          0.4\n",
              "40           5.0          3.5           1.3          0.3\n",
              "45           4.8          3.0           1.4          0.3\n",
              "19           5.1          3.8           1.5          0.3\n",
              "13           4.3          3.0           1.1          0.1\n",
              "24           4.8          3.4           1.9          0.2\n",
              "38           4.4          3.0           1.3          0.2\n",
              "31           5.4          3.4           1.5          0.4\n",
              "7            5.0          3.4           1.5          0.2\n",
              "10           5.4          3.7           1.5          0.2\n",
              "0            5.1          3.5           1.4          0.2\n",
              "34           4.9          3.1           1.5          0.2\n",
              "2            4.7          3.2           1.3          0.2\n",
              "11           4.8          3.4           1.6          0.2\n",
              "30           4.8          3.1           1.6          0.2\n",
              "14           5.8          4.0           1.2          0.2\n",
              "1            4.9          3.0           1.4          0.2\n",
              "4            5.0          3.6           1.4          0.2\n",
              "25           5.0          3.0           1.6          0.2\n",
              "12           4.8          3.0           1.4          0.1\n",
              "6            4.6          3.4           1.4          0.3\n",
              "39           5.1          3.4           1.5          0.2\n",
              "5            5.4          3.9           1.7          0.4\n",
              "47           4.6          3.2           1.4          0.2\n",
              "17           5.1          3.5           1.4          0.3"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "0pOieEFB4s4d",
        "outputId": "b7051c9d-57a5-42a3-87cd-5b6079de113d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_val\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4494441010848846,\n        \"min\": 4.4,\n        \"max\": 5.4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4.6,\n          5.4,\n          4.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3420526275297413,\n        \"min\": 2.9,\n        \"max\": 3.8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.1,\n          3.2,\n          3.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15811388300841897,\n        \"min\": 1.3,\n        \"max\": 1.7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.5,\n          1.3,\n          1.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.2,\n        \"max\": 0.2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_val"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-faa312f7-1058-4717-abae-a6197b8dcad2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faa312f7-1058-4717-abae-a6197b8dcad2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-faa312f7-1058-4717-abae-a6197b8dcad2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-faa312f7-1058-4717-abae-a6197b8dcad2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a52ec86f-cd64-43e0-9e69-fc2a234c8d2c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a52ec86f-cd64-43e0-9e69-fc2a234c8d2c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a52ec86f-cd64-43e0-9e69-fc2a234c8d2c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_81fa2c6b-df12-45d6-a39c-cc3bf55832c0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_val')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_81fa2c6b-df12-45d6-a39c-cc3bf55832c0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_val');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width\n",
              "8            4.4          2.9           1.4          0.2\n",
              "3            4.6          3.1           1.5          0.2\n",
              "46           5.1          3.8           1.6          0.2\n",
              "20           5.4          3.4           1.7          0.2\n",
              "42           4.4          3.2           1.3          0.2"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDZQZfUQ6MDf"
      },
      "source": [
        "#### Normalização e transformação dos vetores em tensores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz5pJe7Y5R2Y"
      },
      "outputs": [],
      "source": [
        "x_scaler = StandardScaler()\n",
        "x_scaler.fit(X_treino)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_scaler.fit(y_treino)\n",
        "\n",
        "X_treino = x_scaler.transform(X_treino)\n",
        "y_treino = y_scaler.transform(y_treino)\n",
        "\n",
        "X_val = x_scaler.transform(X_val)\n",
        "y_val = y_scaler.transform(y_val)\n",
        "\n",
        "X_teste = x_scaler.transform(X_teste)\n",
        "y_teste = y_scaler.transform(y_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizamos a forma de tensores para os nossos dados e a utilização mais funcional deles na nossa rede:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVrHqQl85cOz"
      },
      "outputs": [],
      "source": [
        "X_treino = torch.tensor(X_treino, dtype=torch.float32)\n",
        "y_treino = torch.tensor(y_treino, dtype=torch.float32)\n",
        "\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "X_teste = torch.tensor(X_teste, dtype=torch.float32)\n",
        "y_teste = torch.tensor(y_teste, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNOAv8rkhZ_h"
      },
      "source": [
        "#### Trabalhando com a MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U4WUuCdhhJs"
      },
      "source": [
        "Vamos finalmente definir a nossa MLP e usá-la:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJOIQRVhsVZ7"
      },
      "outputs": [],
      "source": [
        "NUM_DADOS_DE_ENTRADA = 3\n",
        "NUM_DADOS_DE_SAIDA = 1\n",
        "NEURONIOS_C1 = 3\n",
        "NEURONIOS_C2 = 2\n",
        "\n",
        "minha_mlp = MLP(\n",
        "    NUM_DADOS_DE_ENTRADA, NEURONIOS_C1, NEURONIOS_C2, NUM_DADOS_DE_SAIDA\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CscXXOdMC0T"
      },
      "source": [
        "Prevemos, portanto, o nosso y baseado na rede que definimos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UVelgdLns0JS",
        "outputId": "6371392b-f2d9-43dc-f636-54ec4ea33837"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.4959],\n",
              "        [0.4939],\n",
              "        [0.5227],\n",
              "        [0.4789],\n",
              "        [0.4802],\n",
              "        [0.4992],\n",
              "        [0.4813],\n",
              "        [0.4897],\n",
              "        [0.4895],\n",
              "        [0.5162],\n",
              "        [0.4702],\n",
              "        [0.4816],\n",
              "        [0.4834],\n",
              "        [0.5153],\n",
              "        [0.4865],\n",
              "        [0.5016],\n",
              "        [0.5054],\n",
              "        [0.5030],\n",
              "        [0.4851],\n",
              "        [0.5212],\n",
              "        [0.4579],\n",
              "        [0.5092],\n",
              "        [0.4913],\n",
              "        [0.4902],\n",
              "        [0.4874],\n",
              "        [0.4977],\n",
              "        [0.4939],\n",
              "        [0.5080],\n",
              "        [0.4810],\n",
              "        [0.4851],\n",
              "        [0.5084],\n",
              "        [0.5033],\n",
              "        [0.4962],\n",
              "        [0.4870],\n",
              "        [0.5030],\n",
              "        [0.4975],\n",
              "        [0.4905],\n",
              "        [0.4678],\n",
              "        [0.5000],\n",
              "        [0.4977]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_prev = minha_mlp(X_treino)\n",
        "y_prev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ec48OZ6MFbv"
      },
      "source": [
        "Podemos agora buscar otimizá-la, de modo a definir uma taxa de aprendizado, um otimizador (que nesse caso será o de `descida do gradiente estocástico (SGD)` da própria biblioteca *Torch*) e a função de perda, que aqui será o erro quadrático médio (MSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBuK04P5s5zD"
      },
      "outputs": [],
      "source": [
        "TAXA_DE_APRENDIZADO = 0.001\n",
        "\n",
        "otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)\n",
        "fn_perda = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxW-GSYWiaBR"
      },
      "source": [
        "Perfeito, podemos seguir pra melhor parte. Vamos treinar nossa MLP com um Early Stopping! Esse treino será feito com loops em épocas, exatamente como sempre vimos. Calculamos a perda do treino e usamos essa para otimizar os parâmetros.\n",
        "\n",
        "Mas aqui, teremos um passo a mais, para cada época também vamos analisar a perda da validação, nos mantendo alerta para se sua perda também está diminuindo. Se em alguma época a perda subir, está na hora de parar nosso treino, evitando um overfitting.\n",
        "\n",
        "Porém, ao invés de parar instantaneamente, vamos ser pacientes! Vamos rodar mais algumas épocas, e ver se perda cai de novo e alcança um novo melhor valor de perda. Se mesmo, depois de sermos pacientes nada melhora, aí sim! Paramos o nosso código, tendo guardado os parâmetros da melhor época."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqsjw6WfMaFc"
      },
      "source": [
        "DEFININDO O VALOR DE PACIÊNCIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqwP88YqKMn4"
      },
      "outputs": [],
      "source": [
        "patience = 30\n",
        "melhor_val_loss = float('inf')\n",
        "contador_paciencia = 0\n",
        "melhores_pesos = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dkIsNeWwcsj",
        "outputId": "afd68d96-6048-4735-828e-3c246f4d7736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 - Perda do treino:  1.2523325595535606 / Perda da validação:  0.8443724088522165\n",
            "1 - Perda do treino:  1.2508454789722125 / Perda da validação:  0.8416423485490376\n",
            "2 - Perda do treino:  1.249367484630612 / Perda da validação:  0.8389252016646035\n",
            "3 - Perda do treino:  1.2478985192085519 / Perda da validação:  0.8362208982307304\n",
            "4 - Perda do treino:  1.2464385257659942 / Perda da validação:  0.833529368707673\n",
            "5 - Perda do treino:  1.2449874477403524 / Perda da validação:  0.8308505439811732\n",
            "6 - Perda do treino:  1.2435452289437965 / Perda da validação:  0.8281843553595344\n",
            "7 - Perda do treino:  1.2421118135605762 / Perda da validação:  0.8255307345707157\n",
            "8 - Perda do treino:  1.2406871461443694 / Perda da validação:  0.8228896137594509\n",
            "9 - Perda do treino:  1.2392711716156473 / Perda da validação:  0.82026092548439\n",
            "10 - Perda do treino:  1.2378638352590645 / Perda da validação:  0.8176446027152597\n",
            "11 - Perda do treino:  1.2364650827208667 / Perda da validação:  0.8150405788300507\n",
            "12 - Perda do treino:  1.2350748600063182 / Perda da validação:  0.8124487876122224\n",
            "13 - Perda do treino:  1.2336931134771538 / Perda da validação:  0.8098691632479318\n",
            "14 - Perda do treino:  1.232319789849047 / Perda da validação:  0.8073016403232833\n",
            "15 - Perda do treino:  1.2309548361890976 / Perda da validação:  0.804746153821599\n",
            "16 - Perda do treino:  1.2295981999133425 / Perda da validação:  0.8022026391207099\n",
            "17 - Perda do treino:  1.2282498287842833 / Perda da validação:  0.79967103199027\n",
            "18 - Perda do treino:  1.2269096709084333 / Perda da validação:  0.7971512685890888\n",
            "19 - Perda do treino:  1.2255776747338853 / Perda da validação:  0.7946432854624846\n",
            "20 - Perda do treino:  1.224253789047896 / Perda da validação:  0.7921470195396593\n",
            "21 - Perda do treino:  1.2229379629744919 / Perda da validação:  0.7896624081310926\n",
            "22 - Perda do treino:  1.2216301459720926 / Perda da validação:  0.7871893889259551\n",
            "23 - Perda do treino:  1.2203302878311506 / Perda da validação:  0.7847278999895437\n",
            "24 - Perda do treino:  1.219038338671813 / Perda da validação:  0.7822778797607344\n",
            "25 - Perda do treino:  1.2177542489416007 / Perda da validação:  0.7798392670494545\n",
            "26 - Perda do treino:  1.2164779694131023 / Perda da validação:  0.7774120010341763\n",
            "27 - Perda do treino:  1.215209451181689 / Perda da validação:  0.7749960212594276\n",
            "28 - Perda do treino:  1.2139486456632473 / Perda da validação:  0.7725912676333215\n",
            "29 - Perda do treino:  1.2126955045919279 / Perda da validação:  0.770197680425107\n",
            "30 - Perda do treino:  1.2114499800179126 / Perda da validação:  0.7678152002627345\n",
            "31 - Perda do treino:  1.2102120243051964 / Perda da validação:  0.7654437681304433\n",
            "32 - Perda do treino:  1.2089815901293919 / Perda da validação:  0.763083325366366\n",
            "33 - Perda do treino:  1.2077586304755452 / Perda da validação:  0.7607338136601502\n",
            "34 - Perda do treino:  1.2065430986359709 / Perda da validação:  0.7583951750505994\n",
            "35 - Perda do treino:  1.205334948208104 / Perda da validação:  0.7560673519233313\n",
            "36 - Perda do treino:  1.2041341330923676 / Perda da validação:  0.7537502870084534\n",
            "37 - Perda do treino:  1.2029406074900575 / Perda da validação:  0.7514439233782573\n",
            "38 - Perda do treino:  1.2017543259012429 / Perda da validação:  0.7491482044449288\n",
            "39 - Perda do treino:  1.2005752431226822 / Perda da validação:  0.746863073958276\n",
            "40 - Perda do treino:  1.1994033142457574 / Perda da validação:  0.7445884760034748\n",
            "41 - Perda do treino:  1.19823849465442 / Perda da validação:  0.7423243549988301\n",
            "42 - Perda do treino:  1.1970807400231578 / Perda da validação:  0.7400706556935563\n",
            "43 - Perda do treino:  1.195930006314972 / Perda da validação:  0.7378273231655712\n",
            "44 - Perda do treino:  1.1947862497793742 / Perda da validação:  0.7355943028193084\n",
            "45 - Perda do treino:  1.1936494269503959 / Perda da validação:  0.7333715403835459\n",
            "46 - Perda do treino:  1.1925194946446138 / Perda da validação:  0.7311589819092503\n",
            "47 - Perda do treino:  1.1913964099591907 / Perda da validação:  0.7289565737674384\n",
            "48 - Perda do treino:  1.1902801302699302 / Perda da validação:  0.7267642626470519\n",
            "49 - Perda do treino:  1.1891706132293487 / Perda da validação:  0.7245819955528516\n",
            "50 - Perda do treino:  1.188067816764757 / Perda da validação:  0.7224097198033248\n",
            "51 - Perda do treino:  1.1869716990763621 / Perda da validação:  0.7202473830286086\n",
            "52 - Perda do treino:  1.1858822186353797 / Perda da validação:  0.7180949331684293\n",
            "53 - Perda do treino:  1.1847993341821639 / Perda da validação:  0.7159523184700574\n",
            "54 - Perda do treino:  1.183723004724346 / Perda da validação:  0.7138194874862748\n",
            "55 - Perda do treino:  1.1826531895349934 / Perda da validação:  0.7116963890733625\n",
            "56 - Perda do treino:  1.1815898481507785 / Perda da validação:  0.7095829723890974\n",
            "57 - Perda do treino:  1.1805329403701628 / Perda da validação:  0.7074791868907677\n",
            "58 - Perda do treino:  1.1794824262515935 / Perda da validação:  0.7053849823332017\n",
            "59 - Perda do treino:  1.178438266111716 / Perda da validação:  0.7033003087668107\n",
            "60 - Perda do treino:  1.177400420523597 / Perda da validação:  0.7012251165356477\n",
            "61 - Perda do treino:  1.1763688503149639 / Perda da validação:  0.6991593562754783\n",
            "62 - Perda do treino:  1.175343516566452 / Perda da validação:  0.6971029789118678\n",
            "63 - Perda do treino:  1.1743243806098747 / Perda da validação:  0.6950559356582817\n",
            "64 - Perda do treino:  1.173311404026496 / Perda da validação:  0.6930181780141993\n",
            "65 - Perda do treino:  1.1723045486453203 / Perda da validação:  0.690989657763242\n",
            "66 - Perda do treino:  1.171303776541399 / Perda da validação:  0.6889703269713152\n",
            "67 - Perda do treino:  1.1703090500341415 / Perda da validação:  0.6869601379847643\n",
            "68 - Perda do treino:  1.1693203316856469 / Perda da validação:  0.6849590434285423\n",
            "69 - Perda do treino:  1.1683375842990422 / Perda da validação:  0.6829669962043929\n",
            "70 - Perda do treino:  1.167360770916836 / Perda da validação:  0.6809839494890464\n",
            "71 - Perda do treino:  1.1663898548192833 / Perda da validação:  0.6790098567324275\n",
            "72 - Perda do treino:  1.165424799522763 / Perda da validação:  0.6770446716558773\n",
            "73 - Perda do treino:  1.164465568778167 / Perda da validação:  0.675088348250388\n",
            "74 - Perda do treino:  1.1635121265693023 / Perda da validação:  0.6731408407748514\n",
            "75 - Perda do treino:  1.1625644371113026 / Perda da validação:  0.6712021037543168\n",
            "76 - Perda do treino:  1.161622464849055 / Perda da validação:  0.6692720919782662\n",
            "77 - Perda do treino:  1.1606861744556343 / Perda da validação:  0.6673507604988986\n",
            "78 - Perda do treino:  1.1597555308307534 / Perda da validação:  0.6654380646294275\n",
            "79 - Perda do treino:  1.158830499099222 / Perda da validação:  0.6635339599423918\n",
            "80 - Perda do treino:  1.1579110446094165 / Perda da validação:  0.6616384022679775\n",
            "81 - Perda do treino:  1.1569971329317645 / Perda da validação:  0.6597513476923529\n",
            "82 - Perda do treino:  1.1560887298572362 / Perda da validação:  0.6578727525560135\n",
            "83 - Perda do treino:  1.1551858013958494 / Perda da validação:  0.6560025734521421\n",
            "84 - Perda do treino:  1.154288313775186 / Perda da validação:  0.6541407672249784\n",
            "85 - Perda do treino:  1.1533962334389178 / Perda da validação:  0.6522872909682003\n",
            "86 - Perda do treino:  1.1525095270453432 / Perda da validação:  0.6504421020233186\n",
            "87 - Perda do treino:  1.1516281614659363 / Perda da validação:  0.6486051579780806\n",
            "88 - Perda do treino:  1.1507521037839044 / Perda da validação:  0.6467764166648886\n",
            "89 - Perda do treino:  1.1498813212927588 / Perda da validação:  0.6449558361592258\n",
            "90 - Perda do treino:  1.1490157814948934 / Perda da validação:  0.6431433747780966\n",
            "91 - Perda do treino:  1.1481554521001747 / Perda da validação:  0.6413389910784771\n",
            "92 - Perda do treino:  1.1473003010245428 / Perda da validação:  0.6395426438557756\n",
            "93 - Perda do treino:  1.1464502963886214 / Perda da validação:  0.6377542921423058\n",
            "94 - Perda do treino:  1.1456054065163397 / Perda da validação:  0.6359738952057694\n",
            "95 - Perda do treino:  1.1447655999335624 / Perda da validação:  0.6342014125477512\n",
            "96 - Perda do treino:  1.14393084536673 / Perda da validação:  0.632436803902223\n",
            "97 - Perda do treino:  1.1431011117415117 / Perda da validação:  0.6306800292340593\n",
            "98 - Perda do treino:  1.1422763681814623 / Perda da validação:  0.628931048737564\n",
            "99 - Perda do treino:  1.1414565840066966 / Perda da validação:  0.627189822835006\n",
            "100 - Perda do treino:  1.1406417287325663 / Perda da validação:  0.6254563121751668\n",
            "101 - Perda do treino:  1.1398317720683508 / Perda da validação:  0.6237304776318973\n",
            "102 - Perda do treino:  1.1390266839159564 / Perda da validação:  0.6220122803026853\n",
            "103 - Perda do treino:  1.138226434368624 / Perda da validação:  0.620301681507234\n",
            "104 - Perda do treino:  1.1374309937096474 / Perda da validação:  0.6185986427860474\n",
            "105 - Perda do treino:  1.1366403324111005 / Perda da validação:  0.6169031258990308\n",
            "106 - Perda do treino:  1.135854421132573 / Perda da validação:  0.6152150928240964\n",
            "107 - Perda do treino:  1.135073230719917 / Perda da validação:  0.6135345057557807\n",
            "108 - Perda do treino:  1.1342967322039996 / Perda da validação:  0.6118613271038731\n",
            "109 - Perda do treino:  1.1335248967994684 / Perda da validação:  0.6101955194920512\n",
            "110 - Perda do treino:  1.1327576959035226 / Perda da validação:  0.6085370457565273\n",
            "111 - Perda do treino:  1.131995101094695 / Perda da validação:  0.6068858689447059\n",
            "112 - Perda do treino:  1.1312370841316408 / Perda da validação:  0.6052419523138463\n",
            "113 - Perda do treino:  1.1304836169519379 / Perda da validação:  0.60360525932974\n",
            "114 - Perda do treino:  1.1297346716708934 / Perda da validação:  0.6019757536653931\n",
            "115 - Perda do treino:  1.1289902205803588 / Perda da validação:  0.6003533991997201\n",
            "116 - Perda do treino:  1.1282502361475555 / Perda da validação:  0.5987381600162465\n",
            "117 - Perda do treino:  1.1275146910139076 / Perda da validação:  0.5971300004018195\n",
            "118 - Perda do treino:  1.1267835579938825 / Perda da validação:  0.595528884845329\n",
            "119 - Perda do treino:  1.1260568100738406 / Perda da validação:  0.593934778036437\n",
            "120 - Perda do treino:  1.1253344204108933 / Perda da validação:  0.5923476448643161\n",
            "121 - Perda do treino:  1.1246163623317682 / Perda da validação:  0.590767450416396\n",
            "122 - Perda do treino:  1.123902609331684 / Perda da validação:  0.5891941599771198\n",
            "123 - Perda do treino:  1.1231931350732318 / Perda da validação:  0.587627739026708\n",
            "124 - Perda do treino:  1.122487913385265 / Perda da validação:  0.5860681532399319\n",
            "125 - Perda do treino:  1.121786918261798 / Perda da validação:  0.5845153684848955\n",
            "126 - Perda do treino:  1.121090123860912 / Perda da validação:  0.5829693508218246\n",
            "127 - Perda do treino:  1.1203975045036665 / Perda da validação:  0.581430066501866\n",
            "128 - Perda do treino:  1.1197090346730234 / Perda da validação:  0.5798974819658943\n",
            "129 - Perda do treino:  1.1190246890127749 / Perda da validação:  0.5783715638433267\n",
            "130 - Perda do treino:  1.118344442326479 / Perda da validação:  0.5768522789509459\n",
            "131 - Perda do treino:  1.1176682695764044 / Perda da validação:  0.5753395942917324\n",
            "132 - Perda do treino:  1.1169961458824829 / Perda da validação:  0.5738334770537029\n",
            "133 - Perda do treino:  1.1163280465212664 / Perda da validação:  0.5723338946087585\n",
            "134 - Perda do treino:  1.115663946924896 / Perda da validação:  0.57084081451154\n",
            "135 - Perda do treino:  1.1150038226800727 / Perda da validação:  0.569354204498291\n",
            "136 - Perda do treino:  1.1143476495270412 / Perda da validação:  0.5678740324857289\n",
            "137 - Perda do treino:  1.113695403358576 / Perda da validação:  0.5664002665699247\n",
            "138 - Perda do treino:  1.113047060218977 / Perda da validação:  0.5649328750251881\n",
            "139 - Perda do treino:  1.1124025963030735 / Perda da validação:  0.5634718263029634\n",
            "140 - Perda do treino:  1.1117619879552314 / Perda da validação:  0.5620170890307304\n",
            "141 - Perda do treino:  1.1111252116683716 / Perda da validação:  0.5605686320109137\n",
            "142 - Perda do treino:  1.1104922440829914 / Perda da validação:  0.5591264242198002\n",
            "143 - Perda do treino:  1.1098630619861958 / Perda da validação:  0.5576904348064629\n",
            "144 - Perda do treino:  1.1092376423107355 / Perda da validação:  0.5562606330916928\n",
            "145 - Perda do treino:  1.1086159621340488 / Perda da validação:  0.5548369885669377\n",
            "146 - Perda do treino:  1.107997998677313 / Perda da validação:  0.5534194708932486\n",
            "147 - Perda do treino:  1.107383729304502 / Perda da validação:  0.5520080499002329\n",
            "148 - Perda do treino:  1.1067731315214484 / Perda da validação:  0.5506026955850152\n",
            "149 - Perda do treino:  1.106166182974917 / Perda da validação:  0.5492033781112042\n",
            "150 - Perda do treino:  1.1055628614516784 / Perda da validação:  0.5478100678078686\n",
            "151 - Perda do treino:  1.1049631448775934 / Perda da validação:  0.5464227351685176\n",
            "152 - Perda do treino:  1.1043670113167041 / Perda da validação:  0.5450413508500903\n",
            "153 - Perda do treino:  1.1037744389703281 / Perda da validação:  0.5436658856719508\n",
            "154 - Perda do treino:  1.1031854061761621 / Perda da validação:  0.5422963106148898\n",
            "155 - Perda do treino:  1.1025998914073898 / Perda da validação:  0.5409325968201351\n",
            "156 - Perda do treino:  1.1020178732717976 / Perda da validação:  0.539574715588366\n",
            "157 - Perda do treino:  1.1014393305108952 / Perda da validação:  0.5382226383787364\n",
            "158 - Perda do treino:  1.100864241999042 / Perda da validação:  0.5368763368079039\n",
            "159 - Perda do treino:  1.1002925867425823 / Perda da validação:  0.5355357826490652\n",
            "160 - Perda do treino:  1.0997243438789832 / Perda da validação:  0.5342009478309977\n",
            "161 - Perda do treino:  1.0991594926759798 / Perda da validação:  0.5328718044371091\n",
            "162 - Perda do treino:  1.098598012530727 / Perda da validação:  0.5315483247044922\n",
            "163 - Perda do treino:  1.0980398829689566 / Perda da validação:  0.5302304810229853\n",
            "164 - Perda do treino:  1.0974850836441397 / Perda da validação:  0.5289182459342415\n",
            "165 - Perda do treino:  1.096933594336656 / Perda da validação:  0.5276115921308013\n",
            "166 - Perda do treino:  1.0963853949529683 / Perda da validação:  0.5263104924551737\n",
            "167 - Perda do treino:  1.0958404655248024 / Perda da validação:  0.5250149198989223\n",
            "168 - Perda do treino:  1.0952987862083334 / Perda da validação:  0.5237248476017576\n",
            "169 - Perda do treino:  1.0947603372833767 / Perda da validação:  0.5224402488506362\n",
            "170 - Perda do treino:  1.0942250991525864 / Perda da validação:  0.521161097078865\n",
            "171 - Perda do treino:  1.0936930523406558 / Perda da validação:  0.5198873658652124\n",
            "172 - Perda do treino:  1.093164177493528 / Perda da validação:  0.5186190289330247\n",
            "173 - Perda do treino:  1.092638455377608 / Perda da validação:  0.5173560601493481\n",
            "174 - Perda do treino:  1.092115866878982 / Perda da validação:  0.516098433524058\n",
            "175 - Perda do treino:  1.0915963930026433 / Perda da validação:  0.5148461232089933\n",
            "176 - Perda do treino:  1.091080014871719 / Perda da validação:  0.5135991034970955\n",
            "177 - Perda do treino:  1.0905667137267088 / Perda da validação:  0.5123573488215553\n",
            "178 - Perda do treino:  1.0900564709247218 / Perda da validação:  0.5111208337549638\n",
            "179 - Perda do treino:  1.0895492679387258 / Perda da validação:  0.5098895330084698\n",
            "180 - Perda do treino:  1.0890450863567942 / Perda da validação:  0.5086634214309425\n",
            "181 - Perda do treino:  1.0885439078813657 / Perda da validação:  0.5074424740081394\n",
            "182 - Perda do treino:  1.088045714328502 / Perda da validação:  0.506226665861881\n",
            "183 - Perda do treino:  1.0875504876271562 / Perda da validação:  0.5050159722492295\n",
            "184 - Perda do treino:  1.087058209818442 / Perda da validação:  0.5038103685616733\n",
            "185 - Perda do treino:  1.086568863054913 / Perda da validação:  0.5026098303243187\n",
            "186 - Perda do treino:  1.086082429599839 / Perda da validação:  0.5014143331950836\n",
            "187 - Perda do treino:  1.0855988918264967 / Perda da validação:  0.5002238529638995\n",
            "188 - Perda do treino:  1.0851182322174575 / Perda da validação:  0.49903836555191783\n",
            "189 - Perda do treino:  1.0846404333638842 / Perda da validação:  0.49785784701072017\n",
            "190 - Perda do treino:  1.0841654779648322 / Perda da validação:  0.49668227352153693\n",
            "191 - Perda do treino:  1.0836933488265532 / Perda da validação:  0.49551162139446747\n",
            "192 - Perda do treino:  1.0832240288618074 / Perda da validação:  0.4943458670677073\n",
            "193 - Perda do treino:  1.082757501089176 / Perda da validação:  0.4931849871067813\n",
            "194 - Perda do treino:  1.0822937486323816 / Perda da validação:  0.4920289582037795\n",
            "195 - Perda do treino:  1.0818327547196132 / Perda da validação:  0.4908777571766002\n",
            "196 - Perda do treino:  1.0813745026828532 / Perda da validação:  0.4897313609681969\n",
            "197 - Perda do treino:  1.0809189759572115 / Perda da validação:  0.48858974664582994\n",
            "198 - Perda do treino:  1.080466158080263 / Perda da validação:  0.48745289140032416\n",
            "199 - Perda do treino:  1.0800160326913901 / Perda da validação:  0.48632077254533046\n",
            "200 - Perda do treino:  1.0795685835311293 / Perda da validação:  0.4851933675165929\n",
            "201 - Perda do treino:  1.0791237944405219 / Perda da validação:  0.48407065387121984\n",
            "202 - Perda do treino:  1.07868164936047 / Perda da validação:  0.4829526092869608\n",
            "203 - Perda do treino:  1.0782421323310971 / Perda da validação:  0.4818392115614869\n",
            "204 - Perda do treino:  1.0778052274911105 / Perda da validação:  0.4807304386116774\n",
            "205 - Perda do treino:  1.0773709190771719 / Perda da validação:  0.47962626847290946\n",
            "206 - Perda do treino:  1.0769391914232682 / Perda da validação:  0.47852667929835413\n",
            "207 - Perda do treino:  1.0765100289600908 / Perda da validação:  0.4774316493582752\n",
            "208 - Perda do treino:  1.076083416214415 / Perda da validação:  0.47634115703933383\n",
            "209 - Perda do treino:  1.0756593378084869 / Perda da validação:  0.47525518084389784\n",
            "210 - Perda do treino:  1.0752377784594112 / Perda da validação:  0.4741736993893542\n",
            "211 - Perda do treino:  1.0748187229785464 / Perda da validação:  0.4730966914074277\n",
            "212 - Perda do treino:  1.0744021562709034 / Perda da validação:  0.4720241357435027\n",
            "213 - Perda do treino:  1.0739880633345449 / Perda da validação:  0.47095601135594994\n",
            "214 - Perda do treino:  1.073576429259993 / Perda da validação:  0.46989229731545734\n",
            "215 - Perda do treino:  1.073167239229639 / Perda da validação:  0.468832972804366\n",
            "216 - Perda do treino:  1.0727604785171567 / Perda da validação:  0.46777801711600964\n",
            "217 - Perda do treino:  1.072356132486921 / Perda da validação:  0.4667274096540585\n",
            "218 - Perda do treino:  1.071954186593429 / Perda da validação:  0.4656811299318676\n",
            "219 - Perda do treino:  1.0715546263807256 / Perda da validação:  0.4646391575718291\n",
            "220 - Perda do treino:  1.0711574374818338 / Perda da validação:  0.4636014723047298\n",
            "221 - Perda do treino:  1.0707626056181865 / Perda da validação:  0.46256805396911077\n",
            "222 - Perda do treino:  1.0703701165990656 / Perda da validação:  0.46153888251063335\n",
            "223 - Perda do treino:  1.0699799563210408 / Perda da validação:  0.4605139379814478\n",
            "224 - Perda do treino:  1.0695921107674162 / Perda da validação:  0.4594932005395663\n",
            "225 - Perda do treino:  1.069206566007678 / Perda da validação:  0.4584766504482408\n",
            "226 - Perda do treino:  1.068823308196944 / Perda da validação:  0.4574642680753434\n",
            "227 - Perda do treino:  1.0684423235754248 / Perda da validação:  0.45645603389275247\n",
            "228 - Perda do treino:  1.0680635984678775 / Perda da validação:  0.45545192847574106\n",
            "229 - Perda do treino:  1.0676871192830724 / Perda da validação:  0.4544519325023705\n",
            "230 - Perda do treino:  1.0673128725132575 / Perda da validação:  0.45345602675288754\n",
            "231 - Perda do treino:  1.0669408447336306 / Perda da validação:  0.4524641921091247\n",
            "232 - Perda do treino:  1.0665710226018097 / Perda da validação:  0.45147640955390556\n",
            "233 - Perda do treino:  1.066203392857315 / Perda da validação:  0.45049266017045275\n",
            "234 - Perda do treino:  1.0658379423210445 / Perda da validação:  0.4495129251418016\n",
            "235 - Perda do treino:  1.0654746578947616 / Perda da validação:  0.44853718575021456\n",
            "236 - Perda do treino:  1.0651135265605807 / Perda da validação:  0.4475654233766022\n",
            "237 - Perda do treino:  1.0647545353804595 / Perda da validação:  0.44659761949994703\n",
            "238 - Perda do treino:  1.0643976714956933 / Perda da validação:  0.44563375569672975\n",
            "239 - Perda do treino:  1.064042922126411 / Perda da validação:  0.4446738136403613\n",
            "240 - Perda do treino:  1.063690274571078 / Perda da validação:  0.4437177751006166\n",
            "241 - Perda do treino:  1.0633397162059999 / Perda da validação:  0.44276562194307356\n",
            "242 - Perda do treino:  1.0629912344848307 / Perda da validação:  0.44181733612855434\n",
            "243 - Perda do treino:  1.062644816938082 / Perda da validação:  0.44087289971257093\n",
            "244 - Perda do treino:  1.0623004511726397 / Perda da validação:  0.439932294844774\n",
            "245 - Perda do treino:  1.0619581248712797 / Perda da validação:  0.4389955037684053\n",
            "246 - Perda do treino:  1.0616178257921884 / Perda da validação:  0.438062508819754\n",
            "247 - Perda do treino:  1.061279541768488 / Perda da validação:  0.4371332924276154\n",
            "248 - Perda do treino:  1.0609432607077611 / Perda da validação:  0.4362078371127541\n",
            "249 - Perda do treino:  1.0606089705915838 / Perda da validação:  0.43528612548737033\n",
            "250 - Perda do treino:  1.060276659475055 / Perda da validação:  0.43436814025456927\n",
            "251 - Perda do treino:  1.0599463154863369 / Perda da validação:  0.43345386420783444\n",
            "252 - Perda do treino:  1.0596179268261912 / Perda da validação:  0.43254328023050415\n",
            "253 - Perda do treino:  1.0592914817675225 / Perda da validação:  0.43163637129524995\n",
            "254 - Perda do treino:  1.0589669686549237 / Perda da validação:  0.43073312046356166\n",
            "255 - Perda do treino:  1.058644375904225 / Perda da validação:  0.42983351088523136\n",
            "256 - Perda do treino:  1.0583236920020433 / Perda da validação:  0.42893752579784516\n",
            "257 - Perda do treino:  1.0580049055053395 / Perda da validação:  0.42804514852627396\n",
            "258 - Perda do treino:  1.0576880050409732 / Perda da validação:  0.4271563624821704\n",
            "259 - Perda do treino:  1.0573729793052637 / Perda da validação:  0.426271151163468\n",
            "260 - Perda do treino:  1.0570598170635537 / Perda da validação:  0.42538949815388305\n",
            "261 - Perda do treino:  1.0567485071497744 / Perda da validação:  0.4245113871224203\n",
            "262 - Perda do treino:  1.056439038466015 / Perda da validação:  0.42363680182288144\n",
            "263 - Perda do treino:  1.0561313999820936 / Perda da validação:  0.4227657260933767\n",
            "264 - Perda do treino:  1.0558255807351322 / Perda da validação:  0.4218981438558399\n",
            "265 - Perda do treino:  1.055521569829134 / Perda da validação:  0.4210340391155455\n",
            "266 - Perda do treino:  1.0552193564345633 / Perda da validação:  0.4201733959606305\n",
            "267 - Perda do treino:  1.0549189297879278 / Perda da validação:  0.41931619856161717\n",
            "268 - Perda do treino:  1.054620279191365 / Perda da validação:  0.4184624311709406\n",
            "269 - Perda do treino:  1.0543233940122299 / Perda da validação:  0.41761207812247836\n",
            "270 - Perda do treino:  1.0540282636826859 / Perda da validação:  0.4167651238310831\n",
            "271 - Perda do treino:  1.0537348776992994 / Perda da validação:  0.4159215527921182\n",
            "272 - Perda do treino:  1.0534432256226338 / Perda da validação:  0.4150813495809967\n",
            "273 - Perda do treino:  1.0531532970768513 / Perda da validação:  0.41424449885272246\n",
            "274 - Perda do treino:  1.052865081749312 / Perda da validação:  0.41341098534143483\n",
            "275 - Perda do treino:  1.0525785693901804 / Perda da validação:  0.41258079385995555\n",
            "276 - Perda do treino:  1.0522937498120297 / Perda da validação:  0.4117539092993387\n",
            "277 - Perda do treino:  1.0520106128894529 / Perda da validação:  0.4109303166284245\n",
            "278 - Perda do treino:  1.051729148558674 / Perda da validação:  0.4101100008933936\n",
            "279 - Perda do treino:  1.0514493468171622 / Perda da validação:  0.4092929472173267\n",
            "280 - Perda do treino:  1.0511711977232499 / Perda da validação:  0.4084791407997647\n",
            "281 - Perda do treino:  1.0508946913957504 / Perda da validação:  0.4076685669162738\n",
            "282 - Perda do treino:  1.0506198180135808 / Perda da validação:  0.40686121091801103\n",
            "283 - Perda do treino:  1.0503465678153876 / Perda da validação:  0.40605705823129457\n",
            "284 - Perda do treino:  1.050074931099171 / Perda da validação:  0.4052560943571749\n",
            "285 - Perda do treino:  1.0498048982219157 / Perda da validação:  0.4044583048710102\n",
            "286 - Perda do treino:  1.049536459599222 / Perda da validação:  0.40366367542204334\n",
            "287 - Perda do treino:  1.049269605704941 / Perda da validação:  0.40287219173298167\n",
            "288 - Perda do treino:  1.0490043270708092 / Perda da validação:  0.4020838395995804\n",
            "289 - Perda do treino:  1.0487406142860887 / Perda da validação:  0.4012986048902267\n",
            "290 - Perda do treino:  1.048478457997208 / Perda da validação:  0.4005164735455284\n",
            "291 - Perda do treino:  1.0482178489074052 / Perda da validação:  0.39973743157790376\n",
            "292 - Perda do treino:  1.0479587777763746 / Perda da validação:  0.39896146507117436\n",
            "293 - Perda do treino:  1.0477012354199133 / Perda da validação:  0.39818856018016047\n",
            "294 - Perda do treino:  1.0474452127095728 / Perda da validação:  0.3974187031302795\n",
            "295 - Perda do treino:  1.047190700572312 / Perda da validação:  0.39665188021714537\n",
            "296 - Perda do treino:  1.0469376899901508 / Perda da validação:  0.3958880778061721\n",
            "297 - Perda do treino:  1.0466861719998284 / Perda da validação:  0.39512728233217903\n",
            "298 - Perda do treino:  1.0464361376924614 / Perda da validação:  0.3943694802989981\n",
            "299 - Perda do treino:  1.0461875782132073 / Perda da validação:  0.3936146582790846\n",
            "300 - Perda do treino:  1.045940484760926 / Perda da validação:  0.3928628029131292\n",
            "301 - Perda do treino:  1.0456948485878472 / Perda da validação:  0.3921139009096732\n",
            "302 - Perda do treino:  1.045450660999239 / Perda da validação:  0.3913679390447261\n",
            "303 - Perda do treino:  1.0452079133530758 / Perda da validação:  0.39062490416138484\n",
            "304 - Perda do treino:  1.0449665970597135 / Perda da validação:  0.3898847831694557\n",
            "305 - Perda do treino:  1.0447267035815622 / Perda da validação:  0.38914756304507947\n",
            "306 - Perda do treino:  1.0444882244327622 / Perda da validação:  0.3884132308303572\n",
            "307 - Perda do treino:  1.0442511511788664 / Perda da validação:  0.38768177363297973\n",
            "308 - Perda do treino:  1.0440154754365154 / Perda da validação:  0.3869531786258587\n",
            "309 - Perda do treino:  1.0437811888731254 / Perda da validação:  0.38622743304676044\n",
            "310 - Perda do treino:  1.04354828320657 / Perda da validação:  0.38550452419794057\n",
            "311 - Perda do treino:  1.0433167502048684 / Perda da validação:  0.3847844394457841\n",
            "312 - Perda do treino:  1.0430865816858745 / Perda da validação:  0.3840671662204432\n",
            "313 - Perda do treino:  1.042857769516966 / Perda da validação:  0.3833526920154815\n",
            "314 - Perda do treino:  1.0426303056147406 / Perda da validação:  0.38264100438751775\n",
            "315 - Perda do treino:  1.0424041819447072 / Perda da validação:  0.381932090955873\n",
            "316 - Perda do treino:  1.042179390520985 / Perda da validação:  0.38122593940221916\n",
            "317 - Perda do treino:  1.041955923406002 / Perda da validação:  0.38052253747023074\n",
            "318 - Perda do treino:  1.0417337727101956 / Perda da validação:  0.3798218729652369\n",
            "319 - Perda do treino:  1.0415129305917143 / Perda da validação:  0.3791239337538787\n",
            "320 - Perda do treino:  1.0412933892561247 / Perda da validação:  0.37842870776376447\n",
            "321 - Perda do treino:  1.041075140956115 / Perda da validação:  0.37773618298313066\n",
            "322 - Perda do treino:  1.0408581779912047 / Perda da validação:  0.37704634746050303\n",
            "323 - Perda do treino:  1.0406424927074565 / Perda da validação:  0.3763591893043604\n",
            "324 - Perda do treino:  1.0404280774971848 / Perda da validação:  0.3756746966828004\n",
            "325 - Perda do treino:  1.040214924798673 / Perda da validação:  0.3749928578232072\n",
            "326 - Perda do treino:  1.0400030270958873 / Perda da validação:  0.37431366101192126\n",
            "327 - Perda do treino:  1.0397923769181947 / Perda da validação:  0.3736370945939111\n",
            "328 - Perda do treino:  1.0395829668400824 / Perda da validação:  0.37296314697244715\n",
            "329 - Perda do treino:  1.0393747894808807 / Perda da validação:  0.3722918066087778\n",
            "330 - Perda do treino:  1.039167837504482 / Perda da validação:  0.3716230620218062\n",
            "331 - Perda do treino:  1.038962103619069 / Perda da validação:  0.3709569017877716\n",
            "332 - Perda do treino:  1.0387575805768416 / Perda da validação:  0.3702933145399291\n",
            "333 - Perda do treino:  1.0385542611737413 / Perda da validação:  0.3696322889682345\n",
            "334 - Perda do treino:  1.038352138249185 / Perda da validação:  0.36897381381902955\n",
            "335 - Perda do treino:  1.0381512046857955 / Perda da validação:  0.368317877894729\n",
            "336 - Perda do treino:  1.0379514534091343 / Perda da validação:  0.36766447005351\n",
            "337 - Perda do treino:  1.0377528773874365 / Perda da validação:  0.36701357920900357\n",
            "338 - Perda do treino:  1.0375554696313505 / Perda da validação:  0.36636519432998754\n",
            "339 - Perda do treino:  1.0373592231936726 / Perda da validação:  0.36571930444008094\n",
            "340 - Perda do treino:  1.0371641311690891 / Perda da validação:  0.3650758986174413\n",
            "341 - Perda do treino:  1.0369701866939187 / Perda da validação:  0.3644349659944635\n",
            "342 - Perda do treino:  1.0367773829458549 / Perda da validação:  0.3637964957574787\n",
            "343 - Perda do treino:  1.0365857131437115 / Perda da validação:  0.3631604771464584\n",
            "344 - Perda do treino:  1.0363951705471688 / Perda da validação:  0.362526899454717\n",
            "345 - Perda do treino:  1.0362057484565237 / Perda da validação:  0.3618957520286187\n",
            "346 - Perda do treino:  1.0360174402124376 / Perda da validação:  0.36126702426728363\n",
            "347 - Perda do treino:  1.0358302391956886 / Perda da validação:  0.3606407056222987\n",
            "348 - Perda do treino:  1.0356441388269255 / Perda da validação:  0.3600167855974277\n",
            "349 - Perda do treino:  1.0354591325664213 / Perda da validação:  0.35939525374832415\n",
            "350 - Perda do treino:  1.0352752139138304 / Perda da validação:  0.3587760996822463\n",
            "351 - Perda do treino:  1.035092376407945 / Perda da validação:  0.3581593130577728\n",
            "352 - Perda do treino:  1.034910613626457 / Perda da validação:  0.3575448835845211\n",
            "353 - Perda do treino:  1.034729919185716 / Perda da validação:  0.35693280102286645\n",
            "354 - Perda do treino:  1.0345502867404939 / Perda da validação:  0.3563230551836642\n",
            "355 - Perda do treino:  1.034371709983747 / Perda da validação:  0.35571563592797195\n",
            "356 - Perda do treino:  1.0341941826463832 / Perda da validação:  0.3551105331667742\n",
            "357 - Perda do treino:  1.0340176984970273 / Perda da validação:  0.35450773686070935\n",
            "358 - Perda do treino:  1.0338422513417913 / Perda da validação:  0.3539072370197969\n",
            "359 - Perda do treino:  1.0336678350240407 / Perda da validação:  0.3533090237031671\n",
            "360 - Perda do treino:  1.0334944434241702 / Perda da validação:  0.3527130870187927\n",
            "361 - Perda do treino:  1.033322070459374 / Perda da validação:  0.3521194171232212\n",
            "362 - Perda do treino:  1.0331507100834194 / Perda da validação:  0.3515280042213091\n",
            "363 - Perda do treino:  1.0329803562864242 / Perda da validação:  0.35093883856595826\n",
            "364 - Perda do treino:  1.032811003094633 / Perda da validação:  0.3503519104578536\n",
            "365 - Perda do treino:  1.0326426445701946 / Perda da validação:  0.3497672102452015\n",
            "366 - Perda do treino:  1.0324752748109436 / Perda da validação:  0.349184728323471\n",
            "367 - Perda do treino:  1.0323088879501807 / Perda da validação:  0.34860445513513594\n",
            "368 - Perda do treino:  1.0321434781564554 / Perda da validação:  0.34802638116941853\n",
            "369 - Perda do treino:  1.03197903963335 / Perda da validação:  0.3474504969620348\n",
            "370 - Perda do treino:  1.0318155666192645 / Perda da validação:  0.3468767930949412\n",
            "371 - Perda do treino:  1.031653053387205 / Perda da validação:  0.3463052601960832\n",
            "372 - Perda do treino:  1.03149149424457 / Perda da validação:  0.34573588893914453\n",
            "373 - Perda do treino:  1.0313308835329398 / Perda da validação:  0.34516867004329915\n",
            "374 - Perda do treino:  1.0311712156278692 / Perda da validação:  0.3446035942729634\n",
            "375 - Perda do treino:  1.0310124849386768 / Perda da validação:  0.34404065243755055\n",
            "376 - Perda do treino:  1.0308546859082404 / Perda da validação:  0.3434798353912262\n",
            "377 - Perda do treino:  1.030697813012791 / Perda da validação:  0.34292113403266594\n",
            "378 - Perda do treino:  1.0305418607617092 / Perda da validação:  0.34236453930481325\n",
            "379 - Perda do treino:  1.0303868236973213 / Perda da validação:  0.34181004219464\n",
            "380 - Perda do treino:  1.0302326963946986 / Perda da validação:  0.3412576337329075\n",
            "381 - Perda do treino:  1.030079473461458 / Perda da validação:  0.34070730499392954\n",
            "382 - Perda do treino:  1.0299271495375613 / Perda da validação:  0.3401590470953367\n",
            "383 - Perda do treino:  1.029775719295119 / Perda da validação:  0.3396128511978416\n",
            "384 - Perda do treino:  1.0296251774381928 / Perda da validação:  0.33906870850500637\n",
            "385 - Perda do treino:  1.0294755187026015 / Perda da validação:  0.3385266102630109\n",
            "386 - Perda do treino:  1.0293267378557256 / Perda da validação:  0.33798654776042236\n",
            "387 - Perda do treino:  1.029178829696316 / Perda da validação:  0.3374485123279663\n",
            "388 - Perda do treino:  1.0290317890543013 / Perda da validação:  0.33691249533829964\n",
            "389 - Perda do treino:  1.0288856107905981 / Perda da validação:  0.3363784882057839\n",
            "390 - Perda do treino:  1.028740289796922 / Perda da validação:  0.33584648238626047\n",
            "391 - Perda do treino:  1.0285958209955983 / Perda da validação:  0.3353164693768275\n",
            "392 - Perda do treino:  1.0284521993393774 / Perda da validação:  0.3347884407156171\n",
            "393 - Perda do treino:  1.0283094198112472 / Perda da validação:  0.33426238798157437\n",
            "394 - Perda do treino:  1.02816747742425 / Perda da validação:  0.3337383027942387\n",
            "395 - Perda do treino:  1.028026367221298 / Perda da validação:  0.3332161768135243\n",
            "396 - Perda do treino:  1.027886084274992 / Perda da validação:  0.33269600173950375\n",
            "397 - Perda do treino:  1.0277466236874409 / Perda da validação:  0.33217776931219245\n",
            "398 - Perda do treino:  1.0276079805900804 / Perda da validação:  0.3316614713113335\n",
            "399 - Perda do treino:  1.027470150143495 / Perda da validação:  0.33114709955618493\n",
            "400 - Perda do treino:  1.0273331275372413 / Perda da validação:  0.3306346459053074\n",
            "401 - Perda do treino:  1.0271969079896692 / Perda da validação:  0.3301241022563538\n",
            "402 - Perda do treino:  1.027061486747749 / Perda da validação:  0.32961546054585944\n",
            "403 - Perda do treino:  1.0269268590868958 / Perda da validação:  0.329108712749034\n",
            "404 - Perda do treino:  1.0267930203107956 / Perda da validação:  0.32860385087955407\n",
            "405 - Perda do treino:  1.0266599657512354 / Perda da validação:  0.3281008669893577\n",
            "406 - Perda do treino:  1.02652769076793 / Perda da validação:  0.32759975316843964\n",
            "407 - Perda do treino:  1.0263961907483525 / Perda da validação:  0.3271005015446476\n",
            "408 - Perda do treino:  1.0262654611075668 / Perda da validação:  0.32660310428348066\n",
            "409 - Perda do treino:  1.0261354972880574 / Perda da validação:  0.32610755358788734\n",
            "410 - Perda do treino:  1.0260062947595645 / Perda da validação:  0.32561384169806634\n",
            "411 - Perda do treino:  1.025877849018918 / Perda da validação:  0.32512196089126727\n",
            "412 - Perda do treino:  1.025750155589871 / Perda da validação:  0.32463190348159354\n",
            "413 - Perda do treino:  1.0256232100229385 / Perda da validação:  0.3241436618198058\n",
            "414 - Perda do treino:  1.025497007895233 / Perda da validação:  0.3236572282931264\n",
            "415 - Perda do treino:  1.025371544810304 / Perda da validação:  0.3231725953250453\n",
            "416 - Perda do treino:  1.0252468163979764 / Perda da validação:  0.32268975537512745\n",
            "417 - Perda do treino:  1.0251228183141916 / Perda da validação:  0.32220870093882026\n",
            "418 - Perda do treino:  1.024999546240847 / Perda da validação:  0.32172942454726355\n",
            "419 - Perda do treino:  1.0248769958856425 / Perda da validação:  0.3212519187670991\n",
            "420 - Perda do treino:  1.024755162981918 / Perda da validação:  0.3207761762002824\n",
            "421 - Perda do treino:  1.0246340432885035 / Perda da validação:  0.32030218948389555\n",
            "422 - Perda do treino:  1.0245136325895596 / Perda da validação:  0.3198299512899603\n",
            "423 - Perda do treino:  1.0243939266944266 / Perda da validação:  0.319359454325253\n",
            "424 - Perda do treino:  1.0242749214374711 / Perda da validação:  0.3188906913311202\n",
            "425 - Perda do treino:  1.0241566126779333 / Perda da validação:  0.3184236550832959\n",
            "426 - Perda do treino:  1.0240389962997765 / Perda da validação:  0.317958338391719\n",
            "427 - Perda do treino:  1.023922068211538 / Perda da validação:  0.3174947341003523\n",
            "428 - Perda do treino:  1.0238058243461778 / Perda da validação:  0.31703283508700275\n",
            "429 - Perda do treino:  1.0236902606609337 / Perda da validação:  0.3165726342631422\n",
            "430 - Perda do treino:  1.0235753731371713 / Perda da validação:  0.3161141245737298\n",
            "431 - Perda do treino:  1.0234611577802384 / Perda da validação:  0.31565729899703465\n",
            "432 - Perda do treino:  1.0233476106193211 / Perda da validação:  0.3152021505444604\n",
            "433 - Perda do treino:  1.023234727707297 / Perda da validação:  0.3147486722603702\n",
            "434 - Perda do treino:  1.0231225051205932 / Perda da validação:  0.31429685722191253\n",
            "435 - Perda do treino:  1.0230109389590434 / Perda da validação:  0.31384669853884883\n",
            "436 - Perda do treino:  1.0229000253457454 / Perda da validação:  0.3133981893533816\n",
            "437 - Perda do treino:  1.022789760426921 / Perda da validação:  0.31295132283998295\n",
            "438 - Perda do treino:  1.0226801403717753 / Perda da validação:  0.3125060922052256\n",
            "439 - Perda do treino:  1.0225711613723585 / Perda da validação:  0.3120624906876134\n",
            "440 - Perda do treino:  1.0224628196434256 / Perda da validação:  0.31162051155741344\n",
            "441 - Perda do treino:  1.0223551114223013 / Perda da validação:  0.3111801481164893\n",
            "442 - Perda do treino:  1.0222480329687418 / Perda da validação:  0.31074139369813486\n",
            "443 - Perda do treino:  1.0221415805647998 / Perda da validação:  0.3103042416669096\n",
            "444 - Perda do treino:  1.0220357505146893 / Perda da validação:  0.3098686854184738\n",
            "445 - Perda do treino:  1.0219305391446507 / Perda da validação:  0.30943471837942627\n",
            "446 - Perda do treino:  1.02182594280282 / Perda da validação:  0.3090023340071412\n",
            "447 - Perda do treino:  1.0217219578590933 / Perda da validação:  0.30857152578960784\n",
            "448 - Perda do treino:  1.0216185807049964 / Perda da validação:  0.30814228724526943\n",
            "449 - Perda do treino:  1.0215158077535562 / Perda da validação:  0.3077146119228639\n",
            "450 - Perda do treino:  1.021413635439167 / Perda da validação:  0.30728849340126557\n",
            "451 - Perda do treino:  1.021312060217463 / Perda da validação:  0.3068639252893274\n",
            "452 - Perda do treino:  1.0212110785651913 / Perda da validação:  0.30644090122572387\n",
            "453 - Perda do treino:  1.0211106869800815 / Perda da validação:  0.3060194148787961\n",
            "454 - Perda do treino:  1.021010881980721 / Perda da validação:  0.30559945994639587\n",
            "455 - Perda do treino:  1.0209116601064288 / Perda da validação:  0.3051810301557324\n",
            "456 - Perda do treino:  1.020813017917129 / Perda da validação:  0.30476411926321867\n",
            "457 - Perda do treino:  1.0207149519932277 / Perda da validação:  0.30434872105431926\n",
            "458 - Perda do treino:  1.0206174589354884 / Perda da validação:  0.30393482934339905\n",
            "459 - Perda do treino:  1.0205205353649098 / Perda da validação:  0.30352243797357253\n",
            "460 - Perda do treino:  1.0204241779226026 / Perda da validação:  0.30311154081655417\n",
            "461 - Perda do treino:  1.02032838326967 / Perda da validação:  0.30270213177250954\n",
            "462 - Perda do treino:  1.020233148087084 / Perda da validação:  0.3022942047699075\n",
            "463 - Perda do treino:  1.0201384690755684 / Perda da validação:  0.30188775376537247\n",
            "464 - Perda do treino:  1.0200443429554784 / Perda da validação:  0.30148277274353896\n",
            "465 - Perda do treino:  1.0199507664666814 / Perda da validação:  0.30107925571690536\n",
            "466 - Perda do treino:  1.0198577363684411 / Perda da validação:  0.30067719672568965\n",
            "467 - Perda do treino:  1.0197652494392986 / Perda da validação:  0.3002765898376856\n",
            "468 - Perda do treino:  1.0196733024769578 / Perda da validação:  0.2998774291481194\n",
            "469 - Perda do treino:  1.0195818922981679 / Perda da validação:  0.2994797087795079\n",
            "470 - Perda do treino:  1.0194910157386112 / Perda da validação:  0.2990834228815171\n",
            "471 - Perda do treino:  1.0194006696527862 / Perda da validação:  0.29868856563082125\n",
            "472 - Perda do treino:  1.0193108509138957 / Perda da validação:  0.2982951312309633\n",
            "473 - Perda do treino:  1.0192215564137341 / Perda da validação:  0.29790311391221597\n",
            "474 - Perda do treino:  1.0191327830625752 / Perda da validação:  0.29751250793144346\n",
            "475 - Perda do treino:  1.0190445277890599 / Perda da validação:  0.2971233075719638\n",
            "476 - Perda do treino:  1.018956787540087 / Perda da validação:  0.29673550714341285\n",
            "477 - Perda do treino:  1.018869559280702 / Perda da validação:  0.2963491009816074\n",
            "478 - Perda do treino:  1.018782839993989 / Perda da validação:  0.2959640834484111\n",
            "479 - Perda do treino:  1.0186966266809612 / Perda da validação:  0.2955804489315993\n",
            "480 - Perda do treino:  1.0186109163604524 / Perda da validação:  0.29519819184472607\n",
            "481 - Perda do treino:  1.0185257060690112 / Perda da validação:  0.29481730662699085\n",
            "482 - Perda do treino:  1.0184409928607936 / Perda da validação:  0.29443778774310697\n",
            "483 - Perda do treino:  1.0183567738074562 / Perda da validação:  0.29405962968316973\n",
            "484 - Perda do treino:  1.0182730459980522 / Perda da validação:  0.293682826962526\n",
            "485 - Perda do treino:  1.018189806538926 / Perda da validação:  0.29330737412164465\n",
            "486 - Perda do treino:  1.0181070525536096 / Perda da validação:  0.29293326572598694\n",
            "487 - Perda do treino:  1.0180247811827183 / Perda da validação:  0.2925604963658785\n",
            "488 - Perda do treino:  1.0179429895838488 / Perda da validação:  0.2921890606563815\n",
            "489 - Perda do treino:  1.0178616749314768 / Perda da validação:  0.2918189532371678\n",
            "490 - Perda do treino:  1.0177808344168555 / Perda da validação:  0.29145016877239244\n",
            "491 - Perda do treino:  1.0177004652479142 / Perda da validação:  0.29108270195056846\n",
            "492 - Perda do treino:  1.017620564649159 / Perda da validação:  0.2907165474844418\n",
            "493 - Perda do treino:  1.0175411298615722 / Perda da validação:  0.2903517001108675\n",
            "494 - Perda do treino:  1.0174621581425136 / Perda da validação:  0.2899881545906858\n",
            "495 - Perda do treino:  1.0173836467656228 / Perda da validação:  0.28962590570859986\n",
            "496 - Perda do treino:  1.0173055930207198 / Perda da validação:  0.28926494827305343\n",
            "497 - Perda do treino:  1.0172279942137092 / Perda da validação:  0.28890527711610964\n",
            "498 - Perda do treino:  1.0171508476664823 / Perda da validação:  0.2885468870933303\n",
            "499 - Perda do treino:  1.0170741507168226 / Perda da validação:  0.28818977308365595\n",
            "500 - Perda do treino:  1.016997900718309 / Perda da validação:  0.2878339299892862\n",
            "501 - Perda do treino:  1.016922095040221 / Perda da validação:  0.28747935273556174\n",
            "502 - Perda do treino:  1.0168467310674454 / Perda da validação:  0.2871260362708453\n",
            "503 - Perda do treino:  1.016771806200382 / Perda da validação:  0.2867739755664057\n",
            "504 - Perda do treino:  1.01669731785485 / Perda da validação:  0.2864231656163\n",
            "505 - Perda do treino:  1.0166232634619956 / Perda da validação:  0.2860736014372578\n",
            "506 - Perda do treino:  1.016549640468201 / Perda da validação:  0.28572527806856624\n",
            "507 - Perda do treino:  1.0164764463349918 / Perda da validação:  0.28537819057195474\n",
            "508 - Perda do treino:  1.0164036785389459 / Perda da validação:  0.2850323340314814\n",
            "509 - Perda do treino:  1.0163313345716045 / Perda da validação:  0.28468770355341916\n",
            "510 - Perda do treino:  1.0162594119393815 / Perda da validação:  0.28434429426614294\n",
            "511 - Perda do treino:  1.0161879081634742 / Perda da validação:  0.284002101320018\n",
            "512 - Perda do treino:  1.0161168207797746 / Perda da validação:  0.2836611198872877\n",
            "513 - Perda do treino:  1.016046147338781 / Perda da validação:  0.28332134516196283\n",
            "514 - Perda do treino:  1.0159758854055123 / Perda da validação:  0.2829827723597117\n",
            "515 - Perda do treino:  1.0159060325594174 / Perda da validação:  0.28264539671774946\n",
            "516 - Perda do treino:  1.0158365863942922 / Perda da validação:  0.2823092134947299\n",
            "517 - Perda do treino:  1.0157675445181915 / Perda da validação:  0.2819742179706365\n",
            "518 - Perda do treino:  1.0156989045533442 / Perda da validação:  0.2816404054466749\n",
            "519 - Perda do treino:  1.015630664136068 / Perda da validação:  0.28130777124516476\n",
            "520 - Perda do treino:  1.0155628209166851 / Perda da validação:  0.2809763107094344\n",
            "521 - Perda do treino:  1.0154953725594384 / Perda da validação:  0.2806460192037135\n",
            "522 - Perda do treino:  1.015428316742409 / Perda da validação:  0.280316892113028\n",
            "523 - Perda do treino:  1.0153616511574308 / Perda da validação:  0.2799889248430956\n",
            "524 - Perda do treino:  1.0152953735100103 / Perda da validação:  0.2796621128202205\n",
            "525 - Perda do treino:  1.0152294815192442 / Perda da validação:  0.27933645149119063\n",
            "526 - Perda do treino:  1.0151639729177373 / Perda da validação:  0.27901193632317367\n",
            "527 - Perda do treino:  1.0150988454515222 / Perda da validação:  0.2786885628036149\n",
            "528 - Perda do treino:  1.0150340968799791 / Perda da validação:  0.2783663264401348\n",
            "529 - Perda do treino:  1.0149697249757554 / Perda da validação:  0.27804522276042787\n",
            "530 - Perda do treino:  1.0149057275246869 / Perda da validação:  0.27772524731216147\n",
            "531 - Perda do treino:  1.014842102325717 / Perda da validação:  0.27740639566287556\n",
            "532 - Perda do treino:  1.0147788471908208 / Perda da validação:  0.277088663399883\n",
            "533 - Perda do treino:  1.0147159599449254 / Perda da validação:  0.2767720461301699\n",
            "534 - Perda do treino:  1.0146534384258334 / Perda da validação:  0.27645653948029747\n",
            "535 - Perda do treino:  1.0145912804841446 / Perda da validação:  0.27614213909630314\n",
            "536 - Perda do treino:  1.0145294839831807 / Perda da validação:  0.27582884064360375\n",
            "537 - Perda do treino:  1.0144680467989082 / Perda da validação:  0.27551663980689756\n",
            "538 - Perda do treino:  1.0144069668198639 / Perda da validação:  0.2752055322900686\n",
            "539 - Perda do treino:  1.0143462419470783 / Perda da validação:  0.27489551381608973\n",
            "540 - Perda do treino:  1.0142858700940016 / Perda da validação:  0.2745865801269282\n",
            "541 - Perda do treino:  1.01422584918643 / Perda da validação:  0.27427872698344963\n",
            "542 - Perda do treino:  1.0141661771624306 / Perda da validação:  0.27397195016532405\n",
            "543 - Perda do treino:  1.0141068519722691 / Perda da validação:  0.27366624547093227\n",
            "544 - Perda do treino:  1.0140478715783368 / Perda da validação:  0.27336160871727183\n",
            "545 - Perda do treino:  1.0139892339550767 / Perda da validação:  0.27305803573986437\n",
            "546 - Perda do treino:  1.0139309370889134 / Perda da validação:  0.27275552239266376\n",
            "547 - Perda do treino:  1.0138729789781804 / Perda da validação:  0.27245406454796306\n",
            "548 - Perda do treino:  1.013815357633049 / Perda da validação:  0.27215365809630454\n",
            "549 - Perda do treino:  1.0137580710754581 / Perda da validação:  0.2718542989463874\n",
            "550 - Perda do treino:  1.0137011173390431 / Perda da validação:  0.2715559830249787\n",
            "551 - Perda do treino:  1.0136444944690663 / Perda da validação:  0.27125870627682236\n",
            "552 - Perda do treino:  1.013588200522348 / Perda da validação:  0.2709624646645505\n",
            "553 - Perda do treino:  1.0135322335671972 / Perda da validação:  0.27066725416859444\n",
            "554 - Perda do treino:  1.013476591683342 / Perda da validação:  0.27037307078709577\n",
            "555 - Perda do treino:  1.0134212729618626 / Perda da validação:  0.2700799105358193\n",
            "556 - Perda do treino:  1.013366275505123 / Perda da validação:  0.26978776944806465\n",
            "557 - Perda do treino:  1.013311597426704 / Perda da validação:  0.2694966435745799\n",
            "558 - Perda do treino:  1.0132572368513353 / Perda da validação:  0.2692065289834753\n",
            "559 - Perda do treino:  1.0132031919148308 / Perda da validação:  0.26891742176013633\n",
            "560 - Perda do treino:  1.01314946076402 / Perda da validação:  0.2686293180071389\n",
            "561 - Perda do treino:  1.0130960415566839 / Perda da validação:  0.2683422138441641\n",
            "562 - Perda do treino:  1.0130429324614891 / Perda da validação:  0.2680561054079137\n",
            "563 - Perda do treino:  1.0129901316579233 / Perda da validação:  0.26777098885202566\n",
            "564 - Perda do treino:  1.0129376373362302 / Perda da validação:  0.2674868603469903\n",
            "565 - Perda do treino:  1.0128854476973452 / Perda da validação:  0.2672037160800681\n",
            "566 - Perda do treino:  1.0128335609528327 / Perda da validação:  0.2669215522552054\n",
            "567 - Perda do treino:  1.0127819753248204 / Perda da validação:  0.26664036509295336\n",
            "568 - Perda do treino:  1.0127306890459393 / Perda da validação:  0.26636015083038506\n",
            "569 - Perda do treino:  1.012679700359259 / Perda da validação:  0.26608090572101484\n",
            "570 - Perda do treino:  1.0126290075182258 / Perda da validação:  0.2658026260347168\n",
            "571 - Perda do treino:  1.0125786087866007 / Perda da validação:  0.2655253080576447\n",
            "572 - Perda do treino:  1.0125285024383994 / Perda da validação:  0.265248948092151\n",
            "573 - Perda do treino:  1.0124786867578286 / Perda da validação:  0.2649735424567082\n",
            "574 - Perda do treino:  1.0124291600392277 / Perda da validação:  0.26469908748582854\n",
            "575 - Perda do treino:  1.012379920587006 / Perda da validação:  0.2644255795299865\n",
            "576 - Perda do treino:  1.0123309667155855 / Perda da validação:  0.2641530149555387\n",
            "577 - Perda do treino:  1.0122822967493383 / Perda da validação:  0.26388139014464745\n",
            "578 - Perda do treino:  1.0122339090225294 / Perda da validação:  0.2636107014952023\n",
            "579 - Perda do treino:  1.0121858018792573 / Perda da validação:  0.26334094542074327\n",
            "580 - Perda do treino:  1.0121379736733946 / Perda da validação:  0.26307211835038397\n",
            "581 - Perda do treino:  1.0120904227685303 / Perda da validação:  0.26280421672873533\n",
            "582 - Perda do treino:  1.0120431475379121 / Perda da validação:  0.26253723701582965\n",
            "583 - Perda do treino:  1.011996146364389 / Perda da validação:  0.2622711756870456\n",
            "584 - Perda do treino:  1.011949417640353 / Perda da validação:  0.26200602923303223\n",
            "585 - Perda do treino:  1.0119029597676836 / Perda da validação:  0.26174179415963517\n",
            "586 - Perda do treino:  1.0118567711576911 / Perda da validação:  0.2614784669878219\n",
            "587 - Perda do treino:  1.011810850231059 / Perda da validação:  0.26121604425360806\n",
            "588 - Perda do treino:  1.0117651954177904 / Perda da validação:  0.26095452250798373\n",
            "589 - Perda do treino:  1.0117198051571505 / Perda da validação:  0.2606938983168408\n",
            "590 - Perda do treino:  1.0116746778976133 / Perda da validação:  0.2604341682608999\n",
            "591 - Perda do treino:  1.0116298120968046 / Perda da validação:  0.26017532893563844\n",
            "592 - Perda do treino:  1.0115852062214494 / Perda da validação:  0.2599173769512183\n",
            "593 - Perda do treino:  1.0115408587473176 / Perda da validação:  0.259660308932415\n",
            "594 - Perda do treino:  1.011496768159169 / Perda da validação:  0.2594041215185454\n",
            "595 - Perda do treino:  1.0114529329506996 / Perda da validação:  0.25914881136339846\n",
            "596 - Perda do treino:  1.0114093516244917 / Perda da validação:  0.2588943751351637\n",
            "597 - Perda do treino:  1.0113660226919563 / Perda da validação:  0.2586408095163618\n",
            "598 - Perda do treino:  1.0113229446732839 / Perda da validação:  0.2583881112037747\n",
            "599 - Perda do treino:  1.011280116097391 / Perda da validação:  0.2581362769083767\n",
            "600 - Perda do treino:  1.0112375355018688 / Perda da validação:  0.25788530335526527\n",
            "601 - Perda do treino:  1.0111952014329304 / Perda da validação:  0.25763518728359264\n",
            "602 - Perda do treino:  1.0111531124453617 / Perda da validação:  0.25738592544649785\n",
            "603 - Perda do treino:  1.0111112671024673 / Perda da validação:  0.25713751461103873\n",
            "604 - Perda do treino:  1.011069663976024 / Perda da validação:  0.25688995155812483\n",
            "605 - Perda do treino:  1.0110283016462251 / Perda da validação:  0.25664323308244996\n",
            "606 - Perda do treino:  1.0109871787016356 / Perda da validação:  0.25639735599242597\n",
            "607 - Perda do treino:  1.0109462937391396 / Perda da validação:  0.256152317110116\n",
            "608 - Perda do treino:  1.0109056453638905 / Perda da validação:  0.25590811327116886\n",
            "609 - Perda do treino:  1.0108652321892642 / Perda da validação:  0.2556647413247532\n",
            "610 - Perda do treino:  1.010825052836807 / Perda da validação:  0.25542219813349243\n",
            "611 - Perda do treino:  1.0107851059361903 / Perda da validação:  0.2551804805733996\n",
            "612 - Perda do treino:  1.01074539012516 / Perda da validação:  0.2549395855338129\n",
            "613 - Perda do treino:  1.0107059040494897 / Perda da validação:  0.25469950991733165\n",
            "614 - Perda do treino:  1.010666646362932 / Perda da validação:  0.25446025063975203\n",
            "615 - Perda do treino:  1.0106276157271723 / Perda da validação:  0.2542218046300037\n",
            "616 - Perda do treino:  1.010588810811781 / Perda da validação:  0.2539841688300871\n",
            "617 - Perda do treino:  1.0105502302941667 / Perda da validação:  0.2537473401950097\n",
            "618 - Perda do treino:  1.0105118728595295 / Perda da validação:  0.2535113156927239\n",
            "619 - Perda do treino:  1.0104737372008148 / Perda da validação:  0.2532760923040651\n",
            "620 - Perda do treino:  1.010435822018668 / Perda da validação:  0.2530416670226894\n",
            "621 - Perda do treino:  1.0103981260213877 / Perda da validação:  0.2528080368550123\n",
            "622 - Perda do treino:  1.0103606479248808 / Perda da validação:  0.2525751988201474\n",
            "623 - Perda do treino:  1.0103233864526167 / Perda da validação:  0.2523431499498455\n",
            "624 - Perda do treino:  1.010286340335584 / Perda da validação:  0.25211188728843414\n",
            "625 - Perda do treino:  1.0102495083122442 / Perda da validação:  0.25188140789275726\n",
            "626 - Perda do treino:  1.0102128891284878 / Perda da validação:  0.2516517088321155\n",
            "627 - Perda do treino:  1.0101764815375909 / Perda da validação:  0.251422787188206\n",
            "628 - Perda do treino:  1.01014028430017 / Perda da validação:  0.2511946400550639\n",
            "629 - Perda do treino:  1.0101042961841402 / Perda da validação:  0.2509672645390029\n",
            "630 - Perda do treino:  1.01006851596467 / Perda da validação:  0.25074065775855664\n",
            "631 - Perda do treino:  1.0100329424241397 / Perda da validação:  0.2505148168444204\n",
            "632 - Perda do treino:  1.0099975743520975 / Perda da validação:  0.2502897389393932\n",
            "633 - Perda do treino:  1.0099624105452185 / Perda da validação:  0.2500654211983197\n",
            "634 - Perda do treino:  1.0099274498072612 / Perda da validação:  0.24984186078803333\n",
            "635 - Perda do treino:  1.0098926909490247 / Perda da validação:  0.24961905488729852\n",
            "636 - Perda do treino:  1.0098581327883092 / Perda da validação:  0.24939700068675436\n",
            "637 - Perda do treino:  1.0098237741498726 / Perda da validação:  0.2491756953888578\n",
            "638 - Perda do treino:  1.0097896138653901 / Perda da validação:  0.2489551362078278\n",
            "639 - Perda do treino:  1.009755650773413 / Perda da validação:  0.24873532036958884\n",
            "640 - Perda do treino:  1.0097218837193274 / Perda da validação:  0.24851624511171555\n",
            "641 - Perda do treino:  1.0096883115553152 / Perda da validação:  0.24829790768337787\n",
            "642 - Perda do treino:  1.0096549331403113 / Perda da validação:  0.248080305345285\n",
            "643 - Perda do treino:  1.0096217473399665 / Perda da validação:  0.24786343536963154\n",
            "644 - Perda do treino:  1.0095887530266048 / Perda da validação:  0.24764729504004235\n",
            "645 - Perda do treino:  1.0095559490791859 / Perda da validação:  0.247431881651519\n",
            "646 - Perda do treino:  1.0095233343832652 / Perda da validação:  0.24721719251038538\n",
            "647 - Perda do treino:  1.0094909078309553 / Perda da validação:  0.24700322493423416\n",
            "648 - Perda do treino:  1.0094586683208846 / Perda da validação:  0.2467899762518737\n",
            "649 - Perda do treino:  1.0094266147581625 / Perda da validação:  0.24657744380327493\n",
            "650 - Perda do treino:  1.0093947460543384 / Perda da validação:  0.24636562493951808\n",
            "651 - Perda do treino:  1.0093630611273645 / Perda da validação:  0.2461545170227411\n",
            "652 - Perda do treino:  1.0093315589015566 / Perda da validação:  0.24594411742608663\n",
            "653 - Perda do treino:  1.0093002383075587 / Perda da validação:  0.2457344235336504\n",
            "654 - Perda do treino:  1.0092690982823038 / Perda da validação:  0.24552543274042954\n",
            "655 - Perda do treino:  1.0092381377689765 / Perda da validação:  0.24531714245227104\n",
            "656 - Perda do treino:  1.0092073557169765 / Perda da validação:  0.24510955008582092\n",
            "657 - Perda do treino:  1.009176751081883 / Perda da validação:  0.2449026530684729\n",
            "658 - Perda do treino:  1.0091463228254152 / Perda da validação:  0.24469644883831826\n",
            "659 - Perda do treino:  1.009116069915399 / Perda da validação:  0.24449093484409518\n",
            "660 - Perda do treino:  1.0090859913257284 / Perda da validação:  0.2442861085451388\n",
            "661 - Perda do treino:  1.0090560860363311 / Perda da validação:  0.24408196741133142\n",
            "662 - Perda do treino:  1.0090263530331325 / Perda da validação:  0.2438785089230528\n",
            "663 - Perda do treino:  1.0089967913080198 / Perda da validação:  0.24367573057113118\n",
            "664 - Perda do treino:  1.0089673998588065 / Perda da validação:  0.24347362985679383\n",
            "665 - Perda do treino:  1.0089381776891984 / Perda da validação:  0.24327220429161867\n",
            "666 - Perda do treino:  1.0089091238087569 / Perda da validação:  0.24307145139748526\n",
            "667 - Perda do treino:  1.0088802372328665 / Perda da validação:  0.24287136870652715\n",
            "668 - Perda do treino:  1.008851516982699 / Perda da validação:  0.2426719537610834\n",
            "669 - Perda do treino:  1.0088229620851792 / Perda da validação:  0.2424732041136509\n",
            "670 - Perda do treino:  1.0087945715729512 / Perda da validação:  0.24227511732683685\n",
            "671 - Perda do treino:  1.0087663444843449 / Perda da validação:  0.2420776909733115\n",
            "672 - Perda do treino:  1.0087382798633417 / Perda da validação:  0.24188092263576136\n",
            "673 - Perda do treino:  1.0087103767595411 / Perda da validação:  0.24168480990684182\n",
            "674 - Perda do treino:  1.0086826342281283 / Perda da validação:  0.24148935038913133\n",
            "675 - Perda do treino:  1.0086550513298398 / Perda da validação:  0.24129454169508452\n",
            "676 - Perda do treino:  1.008627627130932 / Perda da validação:  0.24110038144698637\n",
            "677 - Perda do treino:  1.008600360703147 / Perda da validação:  0.24090686727690636\n",
            "678 - Perda do treino:  1.0085732511236822 / Perda da validação:  0.2407139968266531\n",
            "679 - Perda do treino:  1.0085462974751551 / Perda da validação:  0.24052176774772854\n",
            "680 - Perda do treino:  1.0085194988455748 / Perda da validação:  0.24033017770128326\n",
            "681 - Perda do treino:  1.008492854328307 / Perda da validação:  0.24013922435807128\n",
            "682 - Perda do treino:  1.0084663630220436 / Perda da validação:  0.23994890539840572\n",
            "683 - Perda do treino:  1.0084400240307725 / Perda da validação:  0.23975921851211418\n",
            "684 - Perda do treino:  1.0084138364637432 / Perda da validação:  0.2395701613984945\n",
            "685 - Perda do treino:  1.0083877994354389 / Perda da validação:  0.23938173176627098\n",
            "686 - Perda do treino:  1.0083619120655425 / Perda da validação:  0.23919392733355052\n",
            "687 - Perda do treino:  1.0083361734789098 / Perda da validação:  0.23900674582777928\n",
            "688 - Perda do treino:  1.0083105828055345 / Perda da validação:  0.23882018498569918\n",
            "689 - Perda do treino:  1.0082851391805208 / Perda da validação:  0.23863424255330495\n",
            "690 - Perda do treino:  1.0082598417440516 / Perda da validação:  0.2384489162858013\n",
            "691 - Perda do treino:  1.0082346896413608 / Perda da validação:  0.23826420394756037\n",
            "692 - Perda do treino:  1.0082096820227 / Perda da validação:  0.23808010331207913\n",
            "693 - Perda do treino:  1.008184818043312 / Perda da validação:  0.2378966121619374\n",
            "694 - Perda do treino:  1.0081600968633992 / Perda da validação:  0.2377137282887559\n",
            "695 - Perda do treino:  1.008135517648095 / Perda da validação:  0.2375314494931542\n",
            "696 - Perda do treino:  1.0081110795674355 / Perda da validação:  0.2373497735847097\n",
            "697 - Perda do treino:  1.008086781796329 / Perda da validação:  0.23716869838191598\n",
            "698 - Perda do treino:  1.0080626235145287 / Perda da validação:  0.23698822171214157\n",
            "699 - Perda do treino:  1.0080386039066023 / Perda da validação:  0.2368083414115894\n",
            "700 - Perda do treino:  1.0080147221619054 / Perda da validação:  0.23662905532525597\n",
            "701 - Perda do treino:  1.0079909774745526 / Perda da validação:  0.23645036130689082\n",
            "702 - Perda do treino:  1.0079673690433881 / Perda da validação:  0.2362722572189563\n",
            "703 - Perda do treino:  1.0079438960719607 / Perda da validação:  0.23609474093258762\n",
            "704 - Perda do treino:  1.0079205577684922 / Perda da validação:  0.23591781032755282\n",
            "705 - Perda do treino:  1.007897353345853 / Perda da validação:  0.23574146329221318\n",
            "706 - Perda do treino:  1.007874282021533 / Perda da validação:  0.2355656977234839\n",
            "707 - Perda do treino:  1.007851343017615 / Perda da validação:  0.23539051152679463\n",
            "708 - Perda do treino:  1.0078285355607473 / Perda da validação:  0.23521590261605088\n",
            "709 - Perda do treino:  1.0078058588821162 / Perda da validação:  0.23504186891359424\n",
            "710 - Perda do treino:  1.007783312217421 / Perda da validação:  0.23486840835016493\n",
            "711 - Perda do treino:  1.0077608948068444 / Perda da validação:  0.23469551886486242\n",
            "712 - Perda do treino:  1.0077386058950295 / Perda da validação:  0.2345231984051078\n",
            "713 - Perda do treino:  1.0077164447310505 / Perda da validação:  0.2343514449266051\n",
            "714 - Perda do treino:  1.0076944105683885 / Perda da validação:  0.2341802563933042\n",
            "715 - Perda do treino:  1.0076725026649043 / Perda da validação:  0.23400963077736234\n",
            "716 - Perda do treino:  1.0076507202828136 / Perda da validação:  0.23383956605910727\n",
            "717 - Perda do treino:  1.0076290626886597 / Perda da validação:  0.23367006022699974\n",
            "718 - Perda do treino:  1.0076075291532898 / Perda da validação:  0.23350111127759626\n",
            "719 - Perda do treino:  1.007586118951829 / Perda da validação:  0.23333271721551255\n",
            "720 - Perda do treino:  1.0075648313636534 / Perda da validação:  0.23316487605338634\n",
            "721 - Perda do treino:  1.007543665672368 / Perda da validação:  0.23299758581184146\n",
            "722 - Perda do treino:  1.0075226211657793 / Perda da validação:  0.23283084451945074\n",
            "723 - Perda do treino:  1.0075016971358717 / Perda da validação:  0.23266465021270055\n",
            "724 - Perda do treino:  1.0074808928787824 / Perda da validação:  0.23249900093595444\n",
            "725 - Perda do treino:  1.0074602076947774 / Perda da validação:  0.2323338947414173\n",
            "726 - Perda do treino:  1.0074396408882265 / Perda da validação:  0.23216932968910015\n",
            "727 - Perda do treino:  1.0074191917675788 / Perda da validação:  0.2320053038467841\n",
            "728 - Perda do treino:  1.00739885964534 / Perda da validação:  0.23184181528998563\n",
            "729 - Perda do treino:  1.0073786438380476 / Perda da validação:  0.23167886210192132\n",
            "730 - Perda do treino:  1.0073585436662473 / Perda da validação:  0.23151644237347288\n",
            "731 - Perda do treino:  1.0073385584544687 / Perda da validação:  0.23135455420315254\n",
            "732 - Perda do treino:  1.0073186875312026 / Perda da validação:  0.23119319569706837\n",
            "733 - Perda do treino:  1.0072989302288777 / Perda da validação:  0.23103236496889004\n",
            "734 - Perda do treino:  1.0072792858838366 / Perda da validação:  0.23087206013981473\n",
            "735 - Perda do treino:  1.0072597538363142 / Perda da validação:  0.23071227933853283\n",
            "736 - Perda do treino:  1.007240333430412 / Perda da validação:  0.23055302070119416\n",
            "737 - Perda do treino:  1.0072210240140786 / Perda da validação:  0.2303942823713745\n",
            "738 - Perda do treino:  1.0072018249390855 / Perda da validação:  0.23023606250004186\n",
            "739 - Perda do treino:  1.0071827355610032 / Perda da validação:  0.23007835924552328\n",
            "740 - Perda do treino:  1.0071637552391814 / Perda da validação:  0.22992117077347146\n",
            "741 - Perda do treino:  1.007144883336725 / Perda da validação:  0.22976449525683199\n",
            "742 - Perda do treino:  1.0071261192204721 / Perda da validação:  0.2296083308758104\n",
            "743 - Perda do treino:  1.0071074622609733 / Perda da validação:  0.2294526758178394\n",
            "744 - Perda do treino:  1.0070889118324675 / Perda da validação:  0.22929752827754663\n",
            "745 - Perda do treino:  1.007070467312863 / Perda da validação:  0.22914288645672204\n",
            "746 - Perda do treino:  1.007052128083713 / Perda da validação:  0.2289887485642857\n",
            "747 - Perda do treino:  1.0070338935301968 / Perda da validação:  0.22883511281625601\n",
            "748 - Perda do treino:  1.0070157630410956 / Perda da validação:  0.22868197743571797\n",
            "749 - Perda do treino:  1.0069977360087745 / Perda da validação:  0.22852934065279076\n",
            "750 - Perda do treino:  1.0069798118291586 / Perda da validação:  0.22837720070459694\n",
            "751 - Perda do treino:  1.006961989901713 / Perda da validação:  0.22822555583523085\n",
            "752 - Perda do treino:  1.006944269629423 / Perda da validação:  0.22807440429572706\n",
            "753 - Perda do treino:  1.0069266504187713 / Perda da validação:  0.2279237443440299\n",
            "754 - Perda do treino:  1.0069091316797196 / Perda da validação:  0.22777357424496203\n",
            "755 - Perda do treino:  1.0068917128256865 / Perda da validação:  0.22762389227019392\n",
            "756 - Perda do treino:  1.0068743932735273 / Perda da validação:  0.22747469669821307\n",
            "757 - Perda do treino:  1.006857172443515 / Perda da validação:  0.22732598581429397\n",
            "758 - Perda do treino:  1.0068400497593182 / Perda da validação:  0.22717775791046724\n",
            "759 - Perda do treino:  1.0068230246479835 / Perda da validação:  0.22703001128548989\n",
            "760 - Perda do treino:  1.0068060965399128 / Perda da validação:  0.2268827442448152\n",
            "761 - Perda do treino:  1.0067892648688466 / Perda da validação:  0.22673595510056269\n",
            "762 - Perda do treino:  1.0067725290718414 / Perda da validação:  0.2265896421714889\n",
            "763 - Perda do treino:  1.0067558885892525 / Perda da validação:  0.22644380378295725\n",
            "764 - Perda do treino:  1.006739342864713 / Perda da validação:  0.22629843826690904\n",
            "765 - Perda do treino:  1.0067228913451156 / Perda da validação:  0.22615354396183412\n",
            "766 - Perda do treino:  1.006706533480593 / Perda da validação:  0.22600911921274172\n",
            "767 - Perda do treino:  1.0066902687244974 / Perda da validação:  0.2258651623711314\n",
            "768 - Perda do treino:  1.0066740965333856 / Perda da validação:  0.22572167179496444\n",
            "769 - Perda do treino:  1.0066580163669943 / Perda da validação:  0.22557864584863507\n",
            "770 - Perda do treino:  1.0066420276882273 / Perda da validação:  0.22543608290294173\n",
            "771 - Perda do treino:  1.0066261299631318 / Perda da validação:  0.2252939813350591\n",
            "772 - Perda do treino:  1.0066103226608842 / Perda da validação:  0.2251523395285094\n",
            "773 - Perda do treino:  1.006594605253768 / Perda da validação:  0.2250111558731347\n",
            "774 - Perda do treino:  1.006578977217158 / Perda da validação:  0.2248704287650686\n",
            "775 - Perda do treino:  1.006563438029501 / Perda da validação:  0.2247301566067086\n",
            "776 - Perda do treino:  1.0065479871722982 / Perda da validação:  0.22459033780668816\n",
            "777 - Perda do treino:  1.0065326241300872 / Perda da validação:  0.22445097077984966\n",
            "778 - Perda do treino:  1.0065173483904233 / Perda da validação:  0.22431205394721632\n",
            "779 - Perda do treino:  1.006502159443863 / Perda da validação:  0.2241735857359654\n",
            "780 - Perda do treino:  1.0064870567839452 / Perda da validação:  0.2240355645794009\n",
            "781 - Perda do treino:  1.0064720399071752 / Perda da validação:  0.22389798891692642\n",
            "782 - Perda do treino:  1.0064571083130047 / Perda da validação:  0.2237608571940184\n",
            "783 - Perda do treino:  1.006442261503818 / Perda da validação:  0.22362416786219944\n",
            "784 - Perda do treino:  1.00642749898491 / Perda da validação:  0.2234879193790115\n",
            "785 - Perda do treino:  1.0064128202644749 / Perda da validação:  0.22335211020798956\n",
            "786 - Perda do treino:  1.006398224853584 / Perda da validação:  0.2232167388186353\n",
            "787 - Perda do treino:  1.0063837122661714 / Perda da validação:  0.2230818036863905\n",
            "788 - Perda do treino:  1.0063692820190169 / Perda da validação:  0.22294730329261164\n",
            "789 - Perda do treino:  1.0063549336317283 / Perda da validação:  0.22281323612454332\n",
            "790 - Perda do treino:  1.0063406666267265 / Perda da validação:  0.2226796006752924\n",
            "791 - Perda do treino:  1.006326480529227 / Perda da validação:  0.22254639544380295\n",
            "792 - Perda do treino:  1.0063123748672242 / Perda da validação:  0.22241361893483003\n",
            "793 - Perda do treino:  1.0062983491714768 / Perda da validação:  0.22228126965891434\n",
            "794 - Perda do treino:  1.0062844029754878 / Perda da validação:  0.222149346132357\n",
            "795 - Perda do treino:  1.0062705358154915 / Perda da validação:  0.22201784687719445\n",
            "796 - Perda do treino:  1.0062567472304373 / Perda da validação:  0.2218867704211732\n",
            "797 - Perda do treino:  1.0062430367619712 / Perda da validação:  0.2217561152977247\n",
            "798 - Perda do treino:  1.0062294039544235 / Perda da validação:  0.22162588004594103\n",
            "799 - Perda do treino:  1.0062158483547896 / Perda da validação:  0.22149606321054946\n",
            "800 - Perda do treino:  1.006202369512717 / Perda da validação:  0.22136666334188862\n",
            "801 - Perda do treino:  1.0061889669804873 / Perda da validação:  0.22123767899588329\n",
            "802 - Perda do treino:  1.0061756403130035 / Perda da validação:  0.22110910873402068\n",
            "803 - Perda do treino:  1.0061623890677727 / Perda da validação:  0.22098095112332583\n",
            "804 - Perda do treino:  1.0061492128048901 / Perda da validação:  0.2208532047363374\n",
            "805 - Perda do treino:  1.0061361110870266 / Perda da validação:  0.22072586815108405\n",
            "806 - Perda do treino:  1.0061230834794104 / Perda da validação:  0.22059893995106034\n",
            "807 - Perda do treino:  1.0061101295498147 / Perda da validação:  0.22047241872520282\n",
            "808 - Perda do treino:  1.0060972488685413 / Perda da validação:  0.22034630306786657\n",
            "809 - Perda do treino:  1.006084441008405 / Perda da validação:  0.22022059157880175\n",
            "810 - Perda do treino:  1.0060717055447217 / Perda da validação:  0.2200952828631299\n",
            "811 - Perda do treino:  1.00605904205529 / Perda da validação:  0.21997037553132093\n",
            "812 - Perda do treino:  1.0060464501203792 / Perda da validação:  0.2198458681991696\n",
            "813 - Perda do treino:  1.0060339293227145 / Perda da validação:  0.21972175948777287\n",
            "814 - Perda do treino:  1.0060214792474613 / Perda da validação:  0.21959804802350638\n",
            "815 - Perda do treino:  1.0060090994822124 / Perda da validação:  0.2194747324380022\n",
            "816 - Perda do treino:  1.0059967896169724 / Perda da validação:  0.21935181136812565\n",
            "817 - Perda do treino:  1.0059845492441446 / Perda da validação:  0.21922928345595274\n",
            "818 - Perda do treino:  1.0059723779585157 / Perda da validação:  0.2191071473487478\n",
            "819 - Perda do treino:  1.0059602753572434 / Perda da validação:  0.21898540169894107\n",
            "820 - Perda do treino:  1.0059482410398415 / Perda da validação:  0.21886404516410612\n",
            "821 - Perda do treino:  1.0059362746081657 / Perda da validação:  0.21874307640693794\n",
            "822 - Perda do treino:  1.0059243756664003 / Perda da validação:  0.21862249409523074\n",
            "823 - Perda do treino:  1.0059125438210452 / Perda da validação:  0.21850229690185588\n",
            "824 - Perda do treino:  1.0059007786809013 / Perda da validação:  0.21838248350474024\n",
            "825 - Perda do treino:  1.005889079857057 / Perda da validação:  0.21826305258684425\n",
            "826 - Perda do treino:  1.005877446962875 / Perda da validação:  0.21814400283614033\n",
            "827 - Perda do treino:  1.0058658796139803 / Perda da validação:  0.21802533294559107\n",
            "828 - Perda do treino:  1.005854377428244 / Perda da validação:  0.21790704161312827\n",
            "829 - Perda do treino:  1.005842940025773 / Perda da validação:  0.21778912754163104\n",
            "830 - Perda do treino:  1.0058315670288942 / Perda da validação:  0.21767158943890505\n",
            "831 - Perda do treino:  1.005820258062145 / Perda da validação:  0.2175544260176609\n",
            "832 - Perda do treino:  1.0058090127522563 / Perda da validação:  0.21743763599549365\n",
            "833 - Perda do treino:  1.0057978307281423 / Perda da validação:  0.21732121809486138\n",
            "834 - Perda do treino:  1.0057867116208876 / Perda da validação:  0.2172051710430644\n",
            "835 - Perda do treino:  1.0057756550637318 / Perda da validação:  0.2170894935722248\n",
            "836 - Perda do treino:  1.0057646606920614 / Perda da validação:  0.21697418441926564\n",
            "837 - Perda do treino:  1.0057537281433924 / Perda da validação:  0.21685924232589046\n",
            "838 - Perda do treino:  1.0057428570573612 / Perda da validação:  0.21674466603856263\n",
            "839 - Perda do treino:  1.0057320470757107 / Perda da validação:  0.2166304543084853\n",
            "840 - Perda do treino:  1.005721297842278 / Perda da validação:  0.21651660589158112\n",
            "841 - Perda do treino:  1.005710609002983 / Perda da validação:  0.21640311954847175\n",
            "842 - Perda do treino:  1.0056999802058142 / Perda da validação:  0.2162899940444582\n",
            "843 - Perda do treino:  1.0056894111008194 / Perda da validação:  0.21617722814950083\n",
            "844 - Perda do treino:  1.005678901340091 / Perda da validação:  0.21606482063819915\n",
            "845 - Perda do treino:  1.0056684505777553 / Perda da validação:  0.2159527702897727\n",
            "846 - Perda do treino:  1.0056580584699608 / Perda da validação:  0.21584107588804052\n",
            "847 - Perda do treino:  1.005647724674865 / Perda da validação:  0.21572973622140235\n",
            "848 - Perda do treino:  1.0056374488526236 / Perda da validação:  0.2156187500828187\n",
            "849 - Perda do treino:  1.0056272306653797 / Perda da validação:  0.2155081162697917\n",
            "850 - Perda do treino:  1.0056170697772502 / Perda da validação:  0.2153978335843456\n",
            "851 - Perda do treino:  1.0056069658543147 / Perda da validação:  0.21528790083300783\n",
            "852 - Perda do treino:  1.0055969185646056 / Perda da validação:  0.21517831682678956\n",
            "853 - Perda do treino:  1.0055869275780942 / Perda da validação:  0.21506908038116695\n",
            "854 - Perda do treino:  1.0055769925666813 / Perda da validação:  0.21496019031606214\n",
            "855 - Perda do treino:  1.0055671132041852 / Perda da validação:  0.21485164545582439\n",
            "856 - Perda do treino:  1.0055572891663296 / Perda da validação:  0.21474344462921122\n",
            "857 - Perda do treino:  1.0055475201307342 / Perda da validação:  0.21463558666937016\n",
            "858 - Perda do treino:  1.0055378057769015 / Perda da validação:  0.21452807041381958\n",
            "859 - Perda do treino:  1.0055281457862082 / Perda da validação:  0.21442089470443068\n",
            "860 - Perda do treino:  1.005518539841891 / Perda da validação:  0.21431405838740897\n",
            "861 - Perda do treino:  1.00550898762904 / Perda da validação:  0.2142075603132759\n",
            "862 - Perda do treino:  1.0054994888345827 / Perda da validação:  0.2141013993368507\n",
            "863 - Perda do treino:  1.0054900431472777 / Perda da validação:  0.2139955743172321\n",
            "864 - Perda do treino:  1.0054806502577016 / Perda da validação:  0.2138900841177806\n",
            "865 - Perda do treino:  1.0054713098582386 / Perda da validação:  0.2137849276061002\n",
            "866 - Perda do treino:  1.005462021643071 / Perda da validação:  0.21368010365402065\n",
            "867 - Perda do treino:  1.0054527853081665 / Perda da validação:  0.21357561113757972\n",
            "868 - Perda do treino:  1.0054436005512704 / Perda da validação:  0.2134714489370054\n",
            "869 - Perda do treino:  1.005434467071893 / Perda da validação:  0.2133676159366983\n",
            "870 - Perda do treino:  1.005425384571301 / Perda da validação:  0.21326411102521417\n",
            "871 - Perda do treino:  1.0054163527525048 / Perda da validação:  0.21316093309524636\n",
            "872 - Perda do treino:  1.0054073713202514 / Perda da validação:  0.2130580810436086\n",
            "873 - Perda do treino:  1.0053984399810112 / Perda da validação:  0.21295555377121733\n",
            "874 - Perda do treino:  1.0053895584429702 / Perda da validação:  0.21285335018307502\n",
            "875 - Perda do treino:  1.0053807264160188 / Perda da validação:  0.21275146918825275\n",
            "876 - Perda do treino:  1.005371943611742 / Perda da validação:  0.21264990969987316\n",
            "877 - Perda do treino:  1.0053632097434093 / Perda da validação:  0.21254867063509336\n",
            "878 - Perda do treino:  1.0053545245259652 / Perda da validação:  0.21244775091508866\n",
            "879 - Perda do treino:  1.0053458876760195 / Perda da validação:  0.21234714946503477\n",
            "880 - Perda do treino:  1.0053372989118365 / Perda da validação:  0.21224686521409195\n",
            "881 - Perda do treino:  1.0053287579533268 / Perda da validação:  0.2121468970953881\n",
            "882 - Perda do treino:  1.0053202645220363 / Perda da validação:  0.2120472440460019\n",
            "883 - Perda do treino:  1.0053118183411378 / Perda da validação:  0.2119479050069466\n",
            "884 - Perda do treino:  1.00530341913542 / Perda da validação:  0.21184887892315368\n",
            "885 - Perda do treino:  1.00529506663128 / Perda da validação:  0.21175016474345618\n",
            "886 - Perda do treino:  1.005286760556712 / Perda da validação:  0.2116517614205729\n",
            "887 - Perda do treino:  1.005278500641299 / Perda da validação:  0.2115536679110918\n",
            "888 - Perda do treino:  1.0052702866162029 / Perda da validação:  0.2114558831754541\n",
            "889 - Perda do treino:  1.0052621182141561 / Perda da validação:  0.21135840617793816\n",
            "890 - Perda do treino:  1.0052539951694512 / Perda da validação:  0.2112612358866437\n",
            "891 - Perda do treino:  1.0052459172179327 / Perda da validação:  0.21116437127347565\n",
            "892 - Perda do treino:  1.0052378840969876 / Perda da validação:  0.2110678113141286\n",
            "893 - Perda do treino:  1.005229895545536 / Perda da validação:  0.21097155498807094\n",
            "894 - Perda do treino:  1.0052219513040233 / Perda da validação:  0.21087560127852917\n",
            "895 - Perda do treino:  1.0052140511144096 / Perda da validação:  0.2107799491724723\n",
            "896 - Perda do treino:  1.0052061947201623 / Perda da validação:  0.21068459766059658\n",
            "897 - Perda do treino:  1.0051983818662458 / Perda da validação:  0.21058954573730984\n",
            "898 - Perda do treino:  1.0051906122991148 / Perda da validação:  0.21049479240071603\n",
            "899 - Perda do treino:  1.0051828857667033 / Perda da validação:  0.21040033665260016\n",
            "900 - Perda do treino:  1.0051752020184177 / Perda da validação:  0.2103061774984129\n",
            "901 - Perda do treino:  1.0051675608051265 / Perda da validação:  0.21021231394725573\n",
            "902 - Perda do treino:  1.0051599618791538 / Perda da validação:  0.21011874501186534\n",
            "903 - Perda do treino:  1.0051524049942695 / Perda da validação:  0.21002546970859903\n",
            "904 - Perda do treino:  1.0051448899056799 / Perda da validação:  0.20993248705741965\n",
            "905 - Perda do treino:  1.0051374163700213 / Perda da validação:  0.2098397960818808\n",
            "906 - Perda do treino:  1.005129984145351 / Perda da validação:  0.20974739580911175\n",
            "907 - Perda do treino:  1.0051225929911376 / Perda da validação:  0.20965528526980312\n",
            "908 - Perda do treino:  1.0051152426682548 / Perda da validação:  0.2095634634981919\n",
            "909 - Perda do treino:  1.0051079329389718 / Perda da validação:  0.209471929532047\n",
            "910 - Perda do treino:  1.0051006635669453 / Perda da validação:  0.20938068241265456\n",
            "911 - Perda do treino:  1.005093434317212 / Perda da validação:  0.2092897211848037\n",
            "912 - Perda do treino:  1.0050862449561797 / Perda da validação:  0.20919904489677207\n",
            "913 - Perda do treino:  1.0050790952516198 / Perda da validação:  0.20910865260031136\n",
            "914 - Perda do treino:  1.0050719849726595 / Perda da validação:  0.20901854335063325\n",
            "915 - Perda do treino:  1.0050649138897731 / Perda da validação:  0.20892871620639503\n",
            "916 - Perda do treino:  1.0050578817747753 / Perda da validação:  0.20883917022968573\n",
            "917 - Perda do treino:  1.0050508884008116 / Perda da validação:  0.20874990448601172\n",
            "918 - Perda do treino:  1.0050439335423518 / Perda da validação:  0.20866091804428283\n",
            "919 - Perda do treino:  1.0050370169751826 / Perda da validação:  0.20857220997679882\n",
            "920 - Perda do treino:  1.0050301384763984 / Perda da validação:  0.20848377935923473\n",
            "921 - Perda do treino:  1.0050232978243951 / Perda da validação:  0.20839562527062766\n",
            "922 - Perda do treino:  1.0050164947988622 / Perda da validação:  0.2083077467933629\n",
            "923 - Perda do treino:  1.0050097291807734 / Perda da validação:  0.2082201430131602\n",
            "924 - Perda do treino:  1.0050030007523822 / Perda da validação:  0.2081328130190602\n",
            "925 - Perda do treino:  1.0049963092972125 / Perda da validação:  0.20804575590341073\n",
            "926 - Perda do treino:  1.0049896546000503 / Perda da validação:  0.20795897076185357\n",
            "927 - Perda do treino:  1.0049830364469388 / Perda da validação:  0.20787245669331084\n",
            "928 - Perda do treino:  1.0049764546251692 / Perda da validação:  0.20778621279997161\n",
            "929 - Perda do treino:  1.0049699089232735 / Perda da validação:  0.20770023818727884\n",
            "930 - Perda do treino:  1.004963399131019 / Perda da validação:  0.20761453196391577\n",
            "931 - Perda do treino:  1.0049569250393977 / Perda da validação:  0.20752909324179303\n",
            "932 - Perda do treino:  1.0049504864406231 / Perda da validação:  0.2074439211360354\n",
            "933 - Perda do treino:  1.0049440831281196 / Perda da validação:  0.20735901476496882\n",
            "934 - Perda do treino:  1.0049377148965175 / Perda da validação:  0.20727437325010722\n",
            "935 - Perda do treino:  1.0049313815416459 / Perda da validação:  0.20718999571613989\n",
            "936 - Perda do treino:  1.0049250828605243 / Perda da validação:  0.20710588129091803\n",
            "937 - Perda do treino:  1.0049188186513565 / Perda da validação:  0.20702202910544268\n",
            "938 - Perda do treino:  1.0049125887135244 / Perda da validação:  0.20693843829385145\n",
            "939 - Perda do treino:  1.0049063928475792 / Perda da validação:  0.20685510799340587\n",
            "940 - Perda do treino:  1.0049002308552375 / Perda da validação:  0.2067720373444788\n",
            "941 - Perda do treino:  1.0048941025393703 / Perda da validação:  0.20668922549054186\n",
            "942 - Perda do treino:  1.0048880077040003 / Perda da validação:  0.206606671578153\n",
            "943 - Perda do treino:  1.0048819461542933 / Perda da validação:  0.2065243747569437\n",
            "944 - Perda do treino:  1.0048759176965512 / Perda da validação:  0.20644233417960695\n",
            "945 - Perda do treino:  1.0048699221382058 / Perda da validação:  0.2063605490018845\n",
            "946 - Perda do treino:  1.0048639592878132 / Perda da validação:  0.2062790183825549\n",
            "947 - Perda do treino:  1.0048580289550446 / Perda da validação:  0.20619774148342113\n",
            "948 - Perda do treino:  1.0048521309506837 / Perda da validação:  0.2061167174692981\n",
            "949 - Perda do treino:  1.004846265086616 / Perda da validação:  0.20603594550800114\n",
            "950 - Perda do treino:  1.0048404311758246 / Perda da validação:  0.2059554247703333\n",
            "951 - Perda do treino:  1.0048346290323844 / Perda da validação:  0.2058751544300738\n",
            "952 - Perda do treino:  1.0048288584714535 / Perda da validação:  0.20579513366396554\n",
            "953 - Perda do treino:  1.0048231193092696 / Perda da validação:  0.2057153616517038\n",
            "954 - Perda do treino:  1.004817411363141 / Perda da validação:  0.20563583757592388\n",
            "955 - Perda do treino:  1.004811734451442 / Perda da validação:  0.2055565606221895\n",
            "956 - Perda do treino:  1.0048060883936063 / Perda da validação:  0.205477529978981\n",
            "957 - Perda do treino:  1.0048004730101208 / Perda da validação:  0.20539874483768367\n",
            "958 - Perda do treino:  1.0047948881225188 / Perda da validação:  0.20532020439257614\n",
            "959 - Perda do treino:  1.0047893335533753 / Perda da validação:  0.20524190784081853\n",
            "960 - Perda do treino:  1.0047838091262995 / Perda da validação:  0.20516385438244122\n",
            "961 - Perda do treino:  1.0047783146659293 / Perda da validação:  0.2050860432203332\n",
            "962 - Perda do treino:  1.0047728499979256 / Perda da validação:  0.20500847356023075\n",
            "963 - Perda do treino:  1.0047674149489656 / Perda da validação:  0.2049311446107057\n",
            "964 - Perda do treino:  1.0047620093467378 / Perda da validação:  0.2048540555831547\n",
            "965 - Perda do treino:  1.0047566330199351 / Perda da validação:  0.20477720569178728\n",
            "966 - Perda do treino:  1.0047512857982492 / Perda da validação:  0.20470059415361513\n",
            "967 - Perda do treino:  1.0047459675123658 / Perda da validação:  0.2046242201884405\n",
            "968 - Perda do treino:  1.0047406779939574 / Perda da validação:  0.20454808301884544\n",
            "969 - Perda do treino:  1.0047354170756773 / Perda da validação:  0.20447218187018046\n",
            "970 - Perda do treino:  1.004730184591156 / Perda da validação:  0.20439651597055356\n",
            "971 - Perda do treino:  1.0047249803749934 / Perda da validação:  0.20432108455081935\n",
            "972 - Perda do treino:  1.0047198042627534 / Perda da validação:  0.20424588684456793\n",
            "973 - Perda do treino:  1.0047146560909597 / Perda da validação:  0.2041709220881142\n",
            "974 - Perda do treino:  1.0047095356970885 / Perda da validação:  0.2040961895204867\n",
            "975 - Perda do treino:  1.004704442919564 / Perda da validação:  0.20402168838341744\n",
            "976 - Perda do treino:  1.004699377597752 / Perda da validação:  0.2039474179213304\n",
            "977 - Perda do treino:  1.0046943395719548 / Perda da validação:  0.20387337738133135\n",
            "978 - Perda do treino:  1.004689328683407 / Perda da validação:  0.20379956601319713\n",
            "979 - Perda do treino:  1.0046843447742675 / Perda da validação:  0.20372598306936487\n",
            "980 - Perda do treino:  1.0046793876876154 / Perda da validação:  0.20365262780492155\n",
            "981 - Perda do treino:  1.0046744572674462 / Perda da validação:  0.20357949947759363\n",
            "982 - Perda do treino:  1.0046695533586631 / Perda da validação:  0.2035065973477363\n",
            "983 - Perda do treino:  1.004664675807075 / Perda da validação:  0.20343392067832342\n",
            "984 - Perda do treino:  1.0046598244593883 / Perda da validação:  0.20336146873493682\n",
            "985 - Perda do treino:  1.004654999163204 / Perda da validação:  0.20328924078575628\n",
            "986 - Perda do treino:  1.0046501997670116 / Perda da validação:  0.20321723610154904\n",
            "987 - Perda do treino:  1.004645426120183 / Perda da validação:  0.20314545395565978\n",
            "988 - Perda do treino:  1.0046406780729689 / Perda da validação:  0.20307389362400013\n",
            "989 - Perda do treino:  1.0046359554764925 / Perda da validação:  0.20300255438503897\n",
            "990 - Perda do treino:  1.0046312581827457 / Perda da validação:  0.20293143551979198\n",
            "991 - Perda do treino:  1.0046265860445822 / Perda da validação:  0.20286053631181167\n",
            "992 - Perda do treino:  1.0046219389157138 / Perda da validação:  0.20278985604717764\n",
            "993 - Perda do treino:  1.0046173166507049 / Perda da validação:  0.20271939401448624\n",
            "994 - Perda do treino:  1.0046127191049679 / Perda da validação:  0.20264914950484095\n",
            "995 - Perda do treino:  1.0046081461347574 / Perda da validação:  0.20257912181184254\n",
            "996 - Perda do treino:  1.0046035975971668 / Perda da validação:  0.20250931023157884\n",
            "997 - Perda do treino:  1.0045990733501213 / Perda da validação:  0.20243971406261552\n",
            "998 - Perda do treino:  1.0045945732523758 / Perda da validação:  0.20237033260598586\n",
            "999 - Perda do treino:  1.0045900971635064 / Perda da validação:  0.20230116516518154\n",
            "1000 - Perda do treino:  1.004585644943909 / Perda da validação:  0.20223221104614267\n",
            "1001 - Perda do treino:  1.0045812164547931 / Perda da validação:  0.20216346955724812\n",
            "1002 - Perda do treino:  1.0045768115581768 / Perda da validação:  0.20209494000930625\n",
            "1003 - Perda do treino:  1.0045724301168821 / Perda da validação:  0.20202662171554522\n",
            "1004 - Perda do treino:  1.0045680719945314 / Perda da validação:  0.20195851399160344\n",
            "1005 - Perda do treino:  1.0045637370555407 / Perda da validação:  0.20189061615552045\n",
            "1006 - Perda do treino:  1.0045594251651166 / Perda da validação:  0.20182292752772707\n",
            "1007 - Perda do treino:  1.0045551361892524 / Perda da validação:  0.20175544743103635\n",
            "1008 - Perda do treino:  1.0045508699947203 / Perda da validação:  0.2016881751906343\n",
            "1009 - Perda do treino:  1.0045466264490708 / Perda da validação:  0.2016211101340705\n",
            "1010 - Perda do treino:  1.0045424054206251 / Perda da validação:  0.20155425159124868\n",
            "1011 - Perda do treino:  1.0045382067784716 / Perda da validação:  0.20148759889441809\n",
            "1012 - Perda do treino:  1.0045340303924628 / Perda da validação:  0.20142115137816363\n",
            "1013 - Perda do treino:  1.0045298761332089 / Perda da validação:  0.20135490837939746\n",
            "1014 - Perda do treino:  1.0045257438720732 / Perda da validação:  0.2012888692373494\n",
            "1015 - Perda do treino:  1.00452163348117 / Perda da validação:  0.20122303329355812\n",
            "1016 - Perda do treino:  1.004517544833358 / Perda da validação:  0.20115739989186215\n",
            "1017 - Perda do treino:  1.0045134778022373 / Perda da validação:  0.20109196837839094\n",
            "1018 - Perda do treino:  1.0045094322621437 / Perda da validação:  0.20102673810155586\n",
            "1019 - Perda do treino:  1.004505408088146 / Perda da validação:  0.20096170841204147\n",
            "1020 - Perda do treino:  1.0045014051560404 / Perda da validação:  0.20089687866279654\n",
            "1021 - Perda do treino:  1.0044974233423474 / Perda da validação:  0.20083224820902537\n",
            "1022 - Perda do treino:  1.0044934625243058 / Perda da validação:  0.20076781640817898\n",
            "1023 - Perda do treino:  1.0044895225798713 / Perda da validação:  0.20070358261994653\n",
            "1024 - Perda do treino:  1.0044856033877092 / Perda da validação:  0.20063954620624638\n",
            "1025 - Perda do treino:  1.0044817048271928 / Perda da validação:  0.20057570653121762\n",
            "1026 - Perda do treino:  1.0044778267783974 / Perda da validação:  0.20051206296121155\n",
            "1027 - Perda do treino:  1.0044739691220974 / Perda da validação:  0.200448614864783\n",
            "1028 - Perda do treino:  1.0044701317397617 / Perda da validação:  0.20038536161268178\n",
            "1029 - Perda do treino:  1.0044663145135497 / Perda da validação:  0.20032230257784422\n",
            "1030 - Perda do treino:  1.0044625173263078 / Perda da validação:  0.20025943713538474\n",
            "1031 - Perda do treino:  1.0044587400615643 / Perda da validação:  0.2001967646625876\n",
            "1032 - Perda do treino:  1.004454982603526 / Perda da validação:  0.2001342845388981\n",
            "1033 - Perda do treino:  1.0044512448370753 / Perda da validação:  0.20007199614591462\n",
            "1034 - Perda do treino:  1.004447526647764 / Perda da validação:  0.20000989886738027\n",
            "1035 - Perda do treino:  1.0044438279218104 / Perda da validação:  0.1999479920891743\n",
            "1036 - Perda do treino:  1.0044401485460974 / Perda da validação:  0.1998862751993044\n",
            "1037 - Perda do treino:  1.0044364884081654 / Perda da validação:  0.19982474758789806\n",
            "1038 - Perda do treino:  1.0044328473962103 / Perda da validação:  0.1997634086471946\n",
            "1039 - Perda do treino:  1.004429225399079 / Perda da validação:  0.19970225777153697\n",
            "1040 - Perda do treino:  1.0044256223062664 / Perda da validação:  0.19964129435736372\n",
            "1041 - Perda do treino:  1.0044220380079105 / Perda da validação:  0.1995805178032009\n",
            "1042 - Perda do treino:  1.0044184723947898 / Perda da validação:  0.19951992750965422\n",
            "1043 - Perda do treino:  1.0044149253583186 / Perda da validação:  0.19945952287940058\n",
            "1044 - Perda do treino:  1.0044113967905441 / Perda da validação:  0.1993993033171809\n",
            "1045 - Perda do treino:  1.0044078865841415 / Perda da validação:  0.19933926822979134\n",
            "1046 - Perda do treino:  1.0044043946324124 / Perda da validação:  0.19927941702607602\n",
            "1047 - Perda do treino:  1.0044009208292786 / Perda da validação:  0.19921974911691892\n",
            "1048 - Perda do treino:  1.0043974650692804 / Perda da validação:  0.19916026391523628\n",
            "1049 - Perda do treino:  1.0043940272475722 / Perda da validação:  0.19910096083596845\n",
            "1050 - Perda do treino:  1.0043906072599191 / Perda da validação:  0.19904183929607253\n",
            "1051 - Perda do treino:  1.0043872050026934 / Perda da validação:  0.19898289871451447\n",
            "1052 - Perda do treino:  1.0043838203728706 / Perda da validação:  0.19892413851226132\n",
            "1053 - Perda do treino:  1.0043804532680267 / Perda da validação:  0.19886555811227374\n",
            "1054 - Perda do treino:  1.0043771035863338 / Perda da validação:  0.19880715693949846\n",
            "1055 - Perda do treino:  1.0043737712265572 / Perda da validação:  0.19874893442086022\n",
            "1056 - Perda do treino:  1.0043704560880518 / Perda da validação:  0.19869088998525491\n",
            "1057 - Perda do treino:  1.0043671580707587 / Perda da validação:  0.1986330230635415\n",
            "1058 - Perda do treino:  1.0043638770752013 / Perda da validação:  0.1985753330885348\n",
            "1059 - Perda do treino:  1.0043606130024831 / Perda da validação:  0.1985178194949982\n",
            "1060 - Perda do treino:  1.0043573657542828 / Perda da validação:  0.19846048171963562\n",
            "1061 - Perda do treino:  1.0043541352328522 / Perda da validação:  0.19840331920108487\n",
            "1062 - Perda do treino:  1.0043509213410118 / Perda da validação:  0.1983463313799098\n",
            "1063 - Perda do treino:  1.0043477239821483 / Perda da validação:  0.1982895176985932\n",
            "1064 - Perda do treino:  1.0043445430602111 / Perda da validação:  0.19823287760152947\n",
            "1065 - Perda do treino:  1.0043413784797093 / Perda da validação:  0.19817641053501708\n",
            "1066 - Perda do treino:  1.004338230145707 / Perda da validação:  0.198120115947252\n",
            "1067 - Perda do treino:  1.0043350979638217 / Perda da validação:  0.1980639932883197\n",
            "1068 - Perda do treino:  1.0043319818402208 / Perda da validação:  0.19800804201018865\n",
            "1069 - Perda do treino:  1.0043288816816174 / Perda da validação:  0.19795226156670281\n",
            "1070 - Perda do treino:  1.0043257973952684 / Perda da validação:  0.19789665141357454\n",
            "1071 - Perda do treino:  1.0043227288889707 / Perda da validação:  0.19784121100837768\n",
            "1072 - Perda do treino:  1.0043196760710578 / Perda da validação:  0.19778593981054052\n",
            "1073 - Perda do treino:  1.0043166388503963 / Perda da validação:  0.1977308372813385\n",
            "1074 - Perda do treino:  1.0043136171363853 / Perda da validação:  0.19767590288388762\n",
            "1075 - Perda do treino:  1.0043106108389495 / Perda da validação:  0.19762113608313697\n",
            "1076 - Perda do treino:  1.0043076198685388 / Perda da validação:  0.19756653634586252\n",
            "1077 - Perda do treino:  1.0043046441361247 / Perda da validação:  0.1975121031406595\n",
            "1078 - Perda do treino:  1.0043016835531973 / Perda da validação:  0.19745783593793598\n",
            "1079 - Perda do treino:  1.0042987380317605 / Perda da validação:  0.19740373420990592\n",
            "1080 - Perda do treino:  1.0042958074843327 / Perda da validação:  0.19734979743058226\n",
            "1081 - Perda do treino:  1.00429289182394 / Perda da validação:  0.1972960250757703\n",
            "1082 - Perda do treino:  1.0042899909641152 / Perda da validação:  0.19724241662306077\n",
            "1083 - Perda do treino:  1.004287104818895 / Perda da validação:  0.19718897155182322\n",
            "1084 - Perda do treino:  1.0042842333028166 / Perda da validação:  0.1971356893431995\n",
            "1085 - Perda do treino:  1.004281376330914 / Perda da validação:  0.19708256948009661\n",
            "1086 - Perda do treino:  1.0042785338187161 / Perda da validação:  0.19702961144718065\n",
            "1087 - Perda do treino:  1.004275705682244 / Perda da validação:  0.19697681473086964\n",
            "1088 - Perda do treino:  1.0042728918380073 / Perda da validação:  0.19692417881932747\n",
            "1089 - Perda do treino:  1.004270092203002 / Perda da validação:  0.1968717032024569\n",
            "1090 - Perda do treino:  1.0042673066947068 / Perda da validação:  0.19681938737189336\n",
            "1091 - Perda do treino:  1.004264535231081 / Perda da validação:  0.19676723082099817\n",
            "1092 - Perda do treino:  1.0042617777305618 / Perda da validação:  0.19671523304485242\n",
            "1093 - Perda do treino:  1.004259034112061 / Perda da validação:  0.19666339354025017\n",
            "1094 - Perda do treino:  1.004256304294962 / Perda da validação:  0.1966117118056922\n",
            "1095 - Perda do treino:  1.004253588199118 / Perda da validação:  0.19656018734137976\n",
            "1096 - Perda do treino:  1.0042508857448484 / Perda da validação:  0.1965088196492078\n",
            "1097 - Perda do treino:  1.004248196852937 / Perda da validação:  0.19645760823275915\n",
            "1098 - Perda do treino:  1.0042455214446284 / Perda da validação:  0.19640655259729775\n",
            "1099 - Perda do treino:  1.0042428594416246 / Perda da validação:  0.19635565224976273\n",
            "1100 - Perda do treino:  1.0042402107660855 / Perda da validação:  0.19630490669876188\n",
            "1101 - Perda do treino:  1.0042375753406223 / Perda da validação:  0.19625431545456562\n",
            "1102 - Perda do treino:  1.004234953088297 / Perda da validação:  0.1962038780291006\n",
            "1103 - Perda do treino:  1.0042323439326197 / Perda da validação:  0.19615359393594384\n",
            "1104 - Perda do treino:  1.004229747797546 / Perda da validação:  0.19610346269031614\n",
            "1105 - Perda do treino:  1.0042271646074739 / Perda da validação:  0.19605348380907628\n",
            "1106 - Perda do treino:  1.0042245942872405 / Perda da validação:  0.19600365681071497\n",
            "1107 - Perda do treino:  1.004222036762122 / Perda da validação:  0.1959539812153484\n",
            "1108 - Perda do treino:  1.0042194919578282 / Perda da validação:  0.1959044565447126\n",
            "1109 - Perda do treino:  1.0042169598005022 / Perda da validação:  0.19585508232215715\n",
            "1110 - Perda do treino:  1.0042144402167161 / Perda da validação:  0.1958058580726393\n",
            "1111 - Perda do treino:  1.0042119331334702 / Perda da validação:  0.195756783322718\n",
            "1112 - Perda do treino:  1.0042094384781888 / Perda da validação:  0.19570785760054793\n",
            "1113 - Perda do treino:  1.0042069561787192 / Perda da validação:  0.19565908043587352\n",
            "1114 - Perda do treino:  1.0042044861633286 / Perda da validação:  0.1956104513600232\n",
            "1115 - Perda do treino:  1.0042020283607016 / Perda da validação:  0.1955619699059033\n",
            "1116 - Perda do treino:  1.0041995826999375 / Perda da validação:  0.19551363560799245\n",
            "1117 - Perda do treino:  1.0041971491105481 / Perda da validação:  0.1954654480023355\n",
            "1118 - Perda do treino:  1.0041947275224568 / Perda da validação:  0.19541740662653806\n",
            "1119 - Perda do treino:  1.0041923178659928 / Perda da validação:  0.19536951101976033\n",
            "1120 - Perda do treino:  1.0041899200718922 / Perda da validação:  0.19532176072271162\n",
            "1121 - Perda do treino:  1.0041875340712942 / Perda da validação:  0.19527415527764463\n",
            "1122 - Perda do treino:  1.0041851597957374 / Perda da validação:  0.19522669422834943\n",
            "1123 - Perda do treino:  1.00418279717716 / Perda da validação:  0.19517937712014816\n",
            "1124 - Perda do treino:  1.0041804461478958 / Perda da validação:  0.19513220349988933\n",
            "1125 - Perda do treino:  1.0041781066406728 / Perda da validação:  0.1950851729159418\n",
            "1126 - Perda do treino:  1.00417577858861 / Perda da validação:  0.19503828491818967\n",
            "1127 - Perda do treino:  1.0041734619252156 / Perda da validação:  0.1949915390580263\n",
            "1128 - Perda do treino:  1.0041711565843852 / Perda da validação:  0.19494493488834902\n",
            "1129 - Perda do treino:  1.0041688625003977 / Perda da validação:  0.19489847196355353\n",
            "1130 - Perda do treino:  1.0041665796079164 / Perda da validação:  0.19485214983952823\n",
            "1131 - Perda do treino:  1.004164307841984 / Perda da validação:  0.19480596807364886\n",
            "1132 - Perda do treino:  1.00416204713802 / Perda da validação:  0.19475992622477298\n",
            "1133 - Perda do treino:  1.0041597974318215 / Perda da validação:  0.19471402385323464\n",
            "1134 - Perda do treino:  1.0041575586595575 / Perda da validação:  0.19466826052083874\n",
            "1135 - Perda do treino:  1.00415533075777 / Perda da validação:  0.1946226357908558\n",
            "1136 - Perda do treino:  1.004153113663369 / Perda da validação:  0.19457714922801647\n",
            "1137 - Perda do treino:  1.0041509073136323 / Perda da validação:  0.19453180039850632\n",
            "1138 - Perda do treino:  1.0041487116462022 / Perda da validação:  0.1944865888699603\n",
            "1139 - Perda do treino:  1.0041465265990837 / Perda da validação:  0.1944415142114575\n",
            "1140 - Perda do treino:  1.0041443521106435 / Perda da validação:  0.19439657599351598\n",
            "1141 - Perda do treino:  1.0041421881196058 / Perda da validação:  0.19435177378808732\n",
            "1142 - Perda do treino:  1.0041400345650515 / Perda da validação:  0.19430710716855146\n",
            "1143 - Perda do treino:  1.0041378913864165 / Perda da validação:  0.19426257570971156\n",
            "1144 - Perda do treino:  1.0041357585234882 / Perda da validação:  0.19421817898778854\n",
            "1145 - Perda do treino:  1.0041336359164057 / Perda da validação:  0.1941739165804161\n",
            "1146 - Perda do treino:  1.0041315235056545 / Perda da validação:  0.19412978806663558\n",
            "1147 - Perda do treino:  1.004129421232068 / Perda da validação:  0.19408579302689064\n",
            "1148 - Perda do treino:  1.0041273290368227 / Perda da validação:  0.19404193104302228\n",
            "1149 - Perda do treino:  1.0041252468614377 / Perda da validação:  0.1939982016982636\n",
            "1150 - Perda do treino:  1.004123174647773 / Perda da validação:  0.19395460457723504\n",
            "1151 - Perda do treino:  1.0041211123380258 / Perda da validação:  0.19391113926593886\n",
            "1152 - Perda do treino:  1.0041190598747296 / Perda da validação:  0.1938678053517545\n",
            "1153 - Perda do treino:  1.0041170172007532 / Perda da validação:  0.19382460242343327\n",
            "1154 - Perda do treino:  1.0041149842592971 / Perda da validação:  0.19378153007109364\n",
            "1155 - Perda do treino:  1.0041129609938921 / Perda da validação:  0.1937385878862159\n",
            "1156 - Perda do treino:  1.0041109473483973 / Perda da validação:  0.19369577546163758\n",
            "1157 - Perda do treino:  1.0041089432669996 / Perda da validação:  0.19365309239154832\n",
            "1158 - Perda do treino:  1.0041069486942091 / Perda da validação:  0.19361053827148486\n",
            "1159 - Perda do treino:  1.0041049635748596 / Perda da validação:  0.1935681126983264\n",
            "1160 - Perda do treino:  1.0041029878541055 / Perda da validação:  0.19352581527028945\n",
            "1161 - Perda do treino:  1.00410102147742 / Perda da validação:  0.19348364558692324\n",
            "1162 - Perda do treino:  1.004099064390594 / Perda da validação:  0.19344160324910464\n",
            "1163 - Perda do treino:  1.0040971165397337 / Perda da validação:  0.19339968785903355\n",
            "1164 - Perda do treino:  1.0040951778712581 / Perda da validação:  0.1933578990202279\n",
            "1165 - Perda do treino:  1.0040932483318987 / Perda da validação:  0.1933162363375191\n",
            "1166 - Perda do treino:  1.0040913278686963 / Perda da validação:  0.19327469941704703\n",
            "1167 - Perda do treino:  1.004089416429 / Perda da validação:  0.19323328786625554\n",
            "1168 - Perda do treino:  1.004087513960465 / Perda da validação:  0.19319200129388756\n",
            "1169 - Perda do treino:  1.0040856204110513 / Perda da validação:  0.19315083930998048\n",
            "1170 - Perda do treino:  1.0040837357290209 / Perda da validação:  0.1931098015258615\n",
            "1171 - Perda do treino:  1.004081859862938 / Perda da validação:  0.1930688875541428\n",
            "1172 - Perda do treino:  1.0040799927616644 / Perda da validação:  0.1930280970087171\n",
            "1173 - Perda do treino:  1.0040781343743599 / Perda da validação:  0.1929874295047529\n",
            "1174 - Perda do treino:  1.004076284650481 / Perda da validação:  0.1929468846586901\n",
            "1175 - Perda do treino:  1.004074443539777 / Perda da validação:  0.1929064620882349\n",
            "1176 - Perda do treino:  1.0040726109922897 / Perda da validação:  0.19286616141235582\n",
            "1177 - Perda do treino:  1.0040707869583518 / Perda da validação:  0.19282598225127892\n",
            "1178 - Perda do treino:  1.0040689713885844 / Perda da validação:  0.19278592422648305\n",
            "1179 - Perda do treino:  1.0040671642338963 / Perda da validação:  0.19274598696069573\n",
            "1180 - Perda do treino:  1.0040653654454814 / Perda da validação:  0.19270617007788832\n",
            "1181 - Perda do treino:  1.004063574974818 / Perda da validação:  0.19266647320327165\n",
            "1182 - Perda do treino:  1.0040617927736657 / Perda da validação:  0.19262689596329183\n",
            "1183 - Perda do treino:  1.0040600187940654 / Perda da validação:  0.19258743798562528\n",
            "1184 - Perda do treino:  1.0040582529883366 / Perda da validação:  0.1925480988991747\n",
            "1185 - Perda do treino:  1.0040564953090763 / Perda da validação:  0.19250887833406466\n",
            "1186 - Perda do treino:  1.004054745709157 / Perda da validação:  0.19246977592163703\n",
            "1187 - Perda do treino:  1.0040530041417255 / Perda da validação:  0.1924307912944466\n",
            "1188 - Perda do treino:  1.0040512705602005 / Perda da validação:  0.19239192408625697\n",
            "1189 - Perda do treino:  1.0040495449182725 / Perda da validação:  0.19235317393203602\n",
            "1190 - Perda do treino:  1.0040478271699005 / Perda da validação:  0.19231454046795154\n",
            "1191 - Perda do treino:  1.0040461172693114 / Perda da validação:  0.19227602333136717\n",
            "1192 - Perda do treino:  1.0040444151709988 / Perda da validação:  0.1922376221608379\n",
            "1193 - Perda do treino:  1.0040427208297205 / Perda da validação:  0.19219933659610583\n",
            "1194 - Perda do treino:  1.0040410342004973 / Perda da validação:  0.19216116627809604\n",
            "1195 - Perda do treino:  1.004039355238612 / Perda da validação:  0.1921231108489124\n",
            "1196 - Perda do treino:  1.004037683899607 / Perda da validação:  0.19208516995183292\n",
            "1197 - Perda do treino:  1.0040360201392833 / Perda da validação:  0.19204734323130612\n",
            "1198 - Perda do treino:  1.0040343639136988 / Perda da validação:  0.1920096303329466\n",
            "1199 - Perda do treino:  1.004032715179168 / Perda da validação:  0.1919720309035308\n",
            "1200 - Perda do treino:  1.0040310738922575 / Perda da validação:  0.19193454459099288\n",
            "1201 - Perda do treino:  1.0040294400097882 / Perda da validação:  0.19189717104442083\n",
            "1202 - Perda do treino:  1.0040278134888312 / Perda da validação:  0.19185990991405197\n",
            "1203 - Perda do treino:  1.0040261942867073 / Perda da validação:  0.19182276085126918\n",
            "1204 - Perda do treino:  1.0040245823609855 / Perda da validação:  0.19178572350859652\n",
            "1205 - Perda do treino:  1.004022977669482 / Perda da validação:  0.1917487975396955\n",
            "1206 - Perda do treino:  1.0040213801702573 / Perda da validação:  0.19171198259936087\n",
            "1207 - Perda do treino:  1.004019789821617 / Perda da validação:  0.19167527834351636\n",
            "1208 - Perda do treino:  1.0040182065821082 / Perda da validação:  0.1916386844292111\n",
            "1209 - Perda do treino:  1.0040166304105187 / Perda da validação:  0.1916022005146152\n",
            "1210 - Perda do treino:  1.0040150612658771 / Perda da validação:  0.19156582625901625\n",
            "1211 - Perda do treino:  1.00401349910745 / Perda da validação:  0.19152956132281482\n",
            "1212 - Perda do treino:  1.00401194389474 / Perda da validação:  0.1914934053675207\n",
            "1213 - Perda do treino:  1.0040103955874853 / Perda da validação:  0.1914573580557492\n",
            "1214 - Perda do treino:  1.004008854145659 / Perda da validação:  0.19142141905121707\n",
            "1215 - Perda do treino:  1.0040073195294665 / Perda da validação:  0.19138558801873826\n",
            "1216 - Perda do treino:  1.0040057916993443 / Perda da validação:  0.19134986462422057\n",
            "1217 - Perda do treino:  1.004004270615959 / Perda da validação:  0.19131424853466156\n",
            "1218 - Perda do treino:  1.0040027562402063 / Perda da validação:  0.1912787394181445\n",
            "1219 - Perda do treino:  1.004001248533209 / Perda da validação:  0.19124333694383475\n",
            "1220 - Perda do treino:  1.0039997474563156 / Perda da validação:  0.19120804078197595\n",
            "1221 - Perda do treino:  1.0039982529710998 / Perda da validação:  0.19117285060388597\n",
            "1222 - Perda do treino:  1.0039967650393584 / Perda da validação:  0.1911377660819534\n",
            "1223 - Perda do treino:  1.003995283623111 / Perda da validação:  0.19110278688963356\n",
            "1224 - Perda do treino:  1.0039938086845965 / Perda da validação:  0.19106791270144477\n",
            "1225 - Perda do treino:  1.0039923401862745 / Perda da validação:  0.19103314319296472\n",
            "1226 - Perda do treino:  1.003990878090823 / Perda da validação:  0.19099847804082645\n",
            "1227 - Perda do treino:  1.0039894223611363 / Perda da validação:  0.19096391692271503\n",
            "1228 - Perda do treino:  1.0039879729603245 / Perda da validação:  0.19092945951736345\n",
            "1229 - Perda do treino:  1.0039865298517126 / Perda da validação:  0.19089510550454916\n",
            "1230 - Perda do treino:  1.0039850929988385 / Perda da validação:  0.19086085456509033\n",
            "1231 - Perda do treino:  1.0039836623654523 / Perda da validação:  0.19082670638084212\n",
            "1232 - Perda do treino:  1.003982237915514 / Perda da validação:  0.1907926606346932\n",
            "1233 - Perda do treino:  1.0039808196131947 / Perda da validação:  0.19075871701056188\n",
            "1234 - Perda do treino:  1.0039794074228718 / Perda da validação:  0.19072487519339273\n",
            "1235 - Perda do treino:  1.0039780013091317 / Perda da validação:  0.19069113486915268\n",
            "1236 - Perda do treino:  1.003976601236765 / Perda da validação:  0.19065749572482765\n",
            "1237 - Perda do treino:  1.0039752071707682 / Perda da validação:  0.19062395744841898\n",
            "1238 - Perda do treino:  1.0039738190763408 / Perda da validação:  0.1905905197289397\n",
            "1239 - Perda do treino:  1.0039724369188845 / Perda da validação:  0.19055718225641108\n",
            "1240 - Perda do treino:  1.0039710606640022 / Perda da validação:  0.1905239447218591\n",
            "1241 - Perda do treino:  1.003969690277497 / Perda da validação:  0.19049080681731084\n",
            "1242 - Perda do treino:  1.0039683257253702 / Perda da validação:  0.19045776823579108\n",
            "1243 - Perda do treino:  1.0039669669738214 / Perda da validação:  0.1904248286713187\n",
            "1244 - Perda do treino:  1.0039656139892459 / Perda da validação:  0.19039198781890335\n",
            "1245 - Perda do treino:  1.0039642667382354 / Perda da validação:  0.1903592453745417\n",
            "1246 - Perda do treino:  1.0039629251875748 / Perda da validação:  0.19032660103521432\n",
            "1247 - Perda do treino:  1.0039615893042426 / Perda da validação:  0.19029405449888187\n",
            "1248 - Perda do treino:  1.0039602590554089 / Perda da validação:  0.19026160546448206\n",
            "1249 - Perda do treino:  1.0039589344084348 / Perda da validação:  0.19022925363192594\n",
            "1250 - Perda do treino:  1.0039576153308711 / Perda da validação:  0.19019699870209467\n",
            "1251 - Perda do treino:  1.0039563017904576 / Perda da validação:  0.1901648403768359\n",
            "1252 - Perda do treino:  1.0039549937551207 / Perda da validação:  0.19013277835896072\n",
            "1253 - Perda do treino:  1.003953691192974 / Perda da validação:  0.19010081235223997\n",
            "1254 - Perda do treino:  1.0039523940723163 / Perda da validação:  0.19006894206140115\n",
            "Pausa na época 1254 pois val_loss não melhorou por 30 épocas), o valor seria de 0.19106791270144477\n"
          ]
        }
      ],
      "source": [
        "N_EPOCAS = 10000\n",
        "\n",
        "minha_mlp.train()\n",
        "\n",
        "for epoca in range(N_EPOCAS):\n",
        "    minha_mlp = minha_mlp.double()\n",
        "    X_treino = X_treino.double()\n",
        "    y_treino = y_treino.double()\n",
        "    y_pred = minha_mlp(X_treino)\n",
        "    otimizador.zero_grad()\n",
        "    loss_treino = fn_perda(y_treino, y_pred)\n",
        "    loss_treino.backward()\n",
        "    otimizador.step()\n",
        "\n",
        "    #Validação\n",
        "    minha_mlp.eval()\n",
        "    with torch.no_grad():\n",
        "      X_val = X_val.double()\n",
        "      y_val = y_val.double()\n",
        "      y_val_pred = minha_mlp(X_val)\n",
        "      loss_val = fn_perda(y_val, y_val_pred)\n",
        "\n",
        "    print(epoca, \"- Perda do treino: \", loss_treino.item(), \"/ Perda da validação: \", loss_val.item())\n",
        "\n",
        "    #Early Stopping\n",
        "    if loss_val.item() < melhor_val_loss - 1e-3:\n",
        "        melhor_val_loss = loss_val.item()\n",
        "        contador_paciencia = 0\n",
        "        melhores_pesos = copy.deepcopy(minha_mlp.state_dict())\n",
        "    else:\n",
        "        contador_paciencia += 1\n",
        "        if contador_paciencia >= patience:\n",
        "            print(f\"Pausa na época {epoca} pois val_loss não melhorou por {patience} épocas), o melhor valor de perda seria de {melhor_val_loss}, na epoca {epoca-patience}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frr3H-6QMwWx"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj-rjNpaMxQW"
      },
      "source": [
        "## **CONCLUSÃO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhKgHJKJM0iV"
      },
      "source": [
        "Podemos ver que a partir da época 1224, a perda da validação não melhorou, e portanto depois de 30 épocas (definida pela paciência) o treino parou. Assim, a estratégia de Early Stopping foi implementada com sucesso, com o objetivo de evitar overfitting.\n",
        "\n",
        "Com esse notebook se aprendeu sobre Early Stopping e também sobre o conceito de paciência, algo novo para os dois autores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmm2B8oiM1h8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PpW78iqJXxv"
      },
      "source": [
        "## **REFERÊNCIAS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7uugLL0KzFm"
      },
      "source": [
        "**[1]** CASSAR, Daniel. Redes Neurais e Algoritmos Genéticos. 2025. Material de Aula.\n",
        "\n",
        "**[2]** KASHYAP, Piyush. Early Stopping in Deep Learning: A Simple Guide to Prevent Overfitting. Medium. 2021. Disponível em: https://medium.com/@piyushkashyap045/early-stopping-in-deep-learning-a-simple-guide-to-prevent-overfitting-1073f56b493e.\n",
        "\n",
        "**[3]** BROWNLEE, Jason. How to Stop Training Deep Neural Networks at the Right Time Using Early Stopping. Machine Learning Mastery. 2019. Disponível em: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/.\n",
        "\n",
        "**[4]** OLAMENDY, Juan C. Real World ML: Early Stopping in Deep Learning - A Comprehensive Guide. Medium. 2023. Disponível em: https://medium.com/@juanc.olamendy/real-world-ml-early-stopping-in-deep-learning-a-comprehensive-guide-fabb1e69f8cc."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
