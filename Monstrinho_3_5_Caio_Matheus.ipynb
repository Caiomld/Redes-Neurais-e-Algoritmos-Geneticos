{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **INTRODUÇÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na atividade a seguir, o objetivo foi de implementar três novas funções de ativação diferentes da sigmoide aneteriormente utilizada — *ReLU (Rectified Linear Unit)*, *ELU (Exponential Linear Unit)* e *Swish*, nesse caso — na estrutura de rede neural construída em Python puro durante a disciplina. Cada função foi incorporada manualmente a rede de modo a substituir a função sigmoidal padrão e observar seu impacto no aprendizado da rede por meio do treino em épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TEORIA DAS FUNÇÕES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***RELU (Rectified Linear Unit)***\n",
    "\n",
    "A função de ativação ReLU (ou Rectified Linear Unit) é relativamente simples no seu âmbito matemático, dado que não envolve nenhum operação complexa durante todo o processo na rede. \n",
    "\n",
    "Em resumo, a rede atua zerando os valores de entrada quando negativos e os retorna quando positivos. A função pode ser vista abaixo:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{se } x > 0 \\\\\n",
    "0, & \\text{se } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "No caso da ReLU, há um problema que pode ser encontrado decorrente dessa relação com os valores negativo: a presença de \"neurônios mortos\". Esses neurônios são decorrentes de uma possível entrada de somente valores negativos, de modo que a derivada desses neurônios sempre vai ser igual a 0, inativando-os.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***ELU (Exponential Linear Unit)***\n",
    "\n",
    "A função de ativação ELU (ou Exponential Linear Unit) segue um princípio parecido se comparada com a ReLU, seguindo inclusive o exato mesmo comportamento para valores positivos. Entretanto, para valores negativos, a função segue um decaimento exponencial, o que possibilita impedir o problema dos neurônios mortos encontrados na função ReLU. A função de ativação pode ser dada pela relação abaixo:\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) =\n",
    "\\begin{cases}\n",
    "x, & \\text{se } x > 0 \\\\\n",
    "\\alpha(e^x - 1), & \\text{se } x \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Swish***\n",
    "\n",
    "No caso da função Swish, são permitidos os valores negativos e não existe uma diferença exata na relação sobre os valores em relação a 0, de forma que não há condições específicas para os valores se eles são positivos ou negativos. \n",
    "\n",
    "A função busca combinar os próprios valores recebidos com a sua sigmoide, com o valor de ativação sendo assim suavemente ajustado. Possui suas certas semelhanças com a função ReLU, mas por permitir os valores negativos, atua de uma forma contínua. A relação pode ser dada pela fórmula matemática abaixo:\n",
    "\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\text{sig}(x) = x \\cdot \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CÓDIGO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos primeiramente as bibliotecas que serão utilizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***CLASSES:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos as classes necessárias (Valor, Neurônio, Camada e MLP) para realizar uma rede neural MLP em Python Puro (já vistas em aula) com a adição dos métodos associados às novas funções de ativação que serão utilizadas durante o algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Valor:\n",
    "    def __init__(self, data, progenitor=(), operador_mae=\"\", rotulo=\"\"):\n",
    "        self.data = data\n",
    "        self.progenitor = progenitor\n",
    "        self.operador_mae = operador_mae\n",
    "        self.rotulo = rotulo\n",
    "        self.grad = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Valor(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self + outro_valor.\"\"\"\n",
    "        \n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data + outro_valor.data\n",
    "        operador_mae = \"+\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_adicao():\n",
    "            self.grad += resultado.grad\n",
    "            outro_valor.grad += resultado.grad\n",
    "            \n",
    "        resultado.propagar = propagar_adicao\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def __mul__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self * outro_valor.\"\"\"\n",
    "        \n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data * outro_valor.data\n",
    "        operador_mae = \"*\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_multiplicacao():\n",
    "            self.grad += resultado.grad * outro_valor.data # grad_filho * derivada filho em relação a mãe\n",
    "            outro_valor.grad += resultado.grad * self.data\n",
    "            \n",
    "        resultado.propagar = propagar_multiplicacao\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"Realiza a operação: exp(self)\"\"\"\n",
    "        progenitor = (self, )\n",
    "        data = math.exp(self.data)\n",
    "        operador_mae = \"exp\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_exp():\n",
    "            self.grad += resultado.grad * data \n",
    "        \n",
    "        resultado.propagar = propagar_exp\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def __pow__(self, expoente):\n",
    "        \"\"\"Realiza a operação: self ** expoente\"\"\"\n",
    "        assert isinstance(expoente, (int, float))\n",
    "        progenitor = (self, )\n",
    "        data = self.data ** expoente\n",
    "        operador_mae = f\"**{expoente}\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_pow():\n",
    "            self.grad += resultado.grad * (expoente * self.data ** (expoente - 1))\n",
    "        \n",
    "        resultado.propagar = propagar_pow\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def __truediv__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self / outro_valor\"\"\"\n",
    "        return self * outro_valor ** (-1)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"Realiza a operação: -self\"\"\"\n",
    "        return self * -1\n",
    "    \n",
    "    def __rsub__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: outro_valor - self\"\"\"\n",
    "        return Valor(outro_valor) - self\n",
    "    \n",
    "    def __sub__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: self - outro_valor\"\"\"\n",
    "        return self + (-outro_valor)\n",
    "    \n",
    "    def __radd__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: outro_valor + self\"\"\"\n",
    "        return self + outro_valor\n",
    "    \n",
    "    def __rmul__(self, outro_valor):\n",
    "        \"\"\"Realiza a operação: outro_valor * self\"\"\"\n",
    "        return self * outro_valor\n",
    "    \n",
    "    def sig(self):\n",
    "        \"\"\"Realiza a operação: exp(self) / (exp(self) + 1)\"\"\"\n",
    "        return self.exp() / (self.exp() + 1)\n",
    "    \n",
    "    ### TRÊS NOVAS FUNÇÕES DE ATIVAÇÃO (ReLU, ELU e Swish)\n",
    "\n",
    "    def relu(self):\n",
    "        \"\"\"Realiza a operação: ReLU(self).\"\"\"\n",
    "        if self.data > 0:\n",
    "            data = self.data\n",
    "        else:\n",
    "            data = 0\n",
    "        progenitor = (self,)\n",
    "        operador_mae = \"ReLU\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "\n",
    "        def propagar_relu():\n",
    "            if self.data > 0:\n",
    "                self.grad += resultado.grad \n",
    "            else:\n",
    "                pass\n",
    " \n",
    "        resultado.propagar = propagar_relu\n",
    "        return resultado\n",
    "\n",
    "    def elu(self, alpha=1.0):\n",
    "        \"\"\"Realiza a operação: ELU(self, alpha).\"\"\"\n",
    "        if self.data > 0:\n",
    "            data = self.data\n",
    "        else:\n",
    "            data = alpha * (math.exp(self.data) - 1)\n",
    "        progenitor = (self,)\n",
    "        operador_mae = \"ELU\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "\n",
    "        def propagar_elu():\n",
    "            if self.data > 0:\n",
    "                derivada = 1\n",
    "            else:\n",
    "                derivada = alpha * math.exp(self.data)\n",
    "            self.grad += resultado.grad * derivada\n",
    "\n",
    "        resultado.propagar = propagar_elu\n",
    "        return resultado\n",
    "\n",
    "    def swish(self):\n",
    "        \"\"\"Realiza a operação: Swish(self).\"\"\"\n",
    "        sig = self.sig()\n",
    "        resultado = self * sig\n",
    "\n",
    "        def propagar_swish():\n",
    "            self.grad += resultado.grad * (sig.data + self.data * sig.data * (1 - sig.data))\n",
    "\n",
    "        resultado.propagar = propagar_swish\n",
    "        return resultado\n",
    "    \n",
    "    def propagar(self):\n",
    "        pass\n",
    "    \n",
    "    def propagar_tudo(self):\n",
    "        \n",
    "        self.grad = 1\n",
    "        \n",
    "        ordem_topologica = []\n",
    "        \n",
    "        visitados = set()\n",
    "\n",
    "        def constroi_ordem_topologica(v):\n",
    "            if v not in visitados:\n",
    "                visitados.add(v)\n",
    "                for progenitor in v.progenitor:\n",
    "                    constroi_ordem_topologica(progenitor)\n",
    "                ordem_topologica.append(v)\n",
    "\n",
    "        constroi_ordem_topologica(self)\n",
    "        \n",
    "        for vertice in reversed(ordem_topologica):\n",
    "            vertice.propagar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuronio:\n",
    "    def __init__(self, num_dados_entrada, fn_ativacao):\n",
    "        self.vies = Valor(random.uniform(-1, 1))\n",
    "        \n",
    "        self.pesos = []\n",
    "        for i in range(num_dados_entrada):\n",
    "            self.pesos.append(Valor(random.uniform(-1, 1)))\n",
    "        self.fn_ativacao = fn_ativacao\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        assert len(x) == len(self.pesos)\n",
    "        \n",
    "        soma = 0\n",
    "        for info_entrada, peso_interno in zip(x, self.pesos):\n",
    "            soma += info_entrada * peso_interno\n",
    "            \n",
    "        soma += self.vies  \n",
    "        \n",
    "        # alteramos para que o dado de saída seja associado à função escolhida no momento e não mais só à sigmóide\n",
    "        \n",
    "        ###dado_de_saida = soma.sig()\n",
    "        if self.fn_ativacao == \"sig\":\n",
    "            dado_de_saida = soma.sig()\n",
    "        elif self.fn_ativacao == \"relu\":\n",
    "            dado_de_saida = soma.relu()\n",
    "        elif self.fn_ativacao == \"elu\":\n",
    "            dado_de_saida = soma.elu()\n",
    "        elif self.fn_ativacao == \"swish\":\n",
    "            dado_de_saida = soma.swish()\n",
    "                \n",
    "        return dado_de_saida       \n",
    "    \n",
    "    def parametros(self):\n",
    "        return self.pesos + [self.vies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camada:\n",
    "    def __init__(self, num_neuronios, num_dados_entrada, fn_ativacao):\n",
    "        neuronios = []\n",
    "        \n",
    "        for _ in range(num_neuronios):\n",
    "            neuronio = Neuronio(num_dados_entrada, fn_ativacao)\n",
    "            neuronios.append(neuronio)\n",
    "            \n",
    "        self.neuronios = neuronios     \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        dados_de_saida = []\n",
    "        \n",
    "        for neuronio in self.neuronios:\n",
    "            informacao = neuronio(x)\n",
    "            dados_de_saida.append(informacao)\n",
    "            \n",
    "        if len(dados_de_saida) == 1:\n",
    "            return dados_de_saida[0]\n",
    "        else:        \n",
    "            return dados_de_saida  \n",
    "    \n",
    "    def parametros(self):\n",
    "        params = []\n",
    "        \n",
    "        for neuronio in self.neuronios:\n",
    "            params_neuronio = neuronio.parametros()\n",
    "            params.extend(params_neuronio)\n",
    "        \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_dados_entrada, num_neuronios_por_camada, fn_ativacao):\n",
    "        \n",
    "        percurso = [num_dados_entrada] + num_neuronios_por_camada\n",
    "        \n",
    "        camadas = []\n",
    "        \n",
    "        for i in range(len(num_neuronios_por_camada)):\n",
    "            camada = Camada(num_neuronios_por_camada[i], percurso[i], fn_ativacao)\n",
    "            camadas.append(camada)\n",
    "            \n",
    "        self.camadas = camadas\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for camada in self.camadas:\n",
    "            x = camada(x)\n",
    "        return x\n",
    "    \n",
    "    def parametros(self):\n",
    "        params = []\n",
    "        \n",
    "        for camada in self.camadas:\n",
    "            parametros_camada = camada.parametros()\n",
    "            params.extend(parametros_camada)\n",
    "            \n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***TESTES:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E assim, com os valores de x e y, podemos fazer os treinamento com os valores de previsão para cada uma das funções de ativação utilizadas baseado nas épocas de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [1.2, -0.5, 2.0],\n",
    "    [-1.0, 1.5, -0.8],\n",
    "    [0.8, -0.3, 1.2],\n",
    "    [1.5, 0.3, -1.0],\n",
    "    [2.1, -1.3, 0.7]\n",
    "]\n",
    "\n",
    "y_true = [0.8, 0.1, 0.5, 0.2, 1.1]\n",
    "\n",
    "NUM_DADOS_DE_ENTRADA = 3  \n",
    "NUM_DADOS_DE_SAIDA = 1    \n",
    "CAMADAS_OCULTAS = [3, 2]  \n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TESTE COM RELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(8)\n",
    "minha_mlp_relu = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede, fn_ativacao=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4300000000000001\n",
      "1 0.4300000000000001\n",
      "2 0.4300000000000001\n",
      "3 0.4300000000000001\n",
      "4 0.4300000000000001\n",
      "5 0.4300000000000001\n",
      "6 0.4300000000000001\n",
      "7 0.4300000000000001\n",
      "8 0.4300000000000001\n",
      "9 0.4300000000000001\n",
      "10 0.4300000000000001\n",
      "11 0.4300000000000001\n",
      "12 0.4300000000000001\n",
      "13 0.4300000000000001\n",
      "14 0.4300000000000001\n",
      "15 0.4300000000000001\n",
      "16 0.4300000000000001\n",
      "17 0.4300000000000001\n",
      "18 0.4300000000000001\n",
      "19 0.4300000000000001\n",
      "20 0.4300000000000001\n",
      "21 0.4300000000000001\n",
      "22 0.4300000000000001\n",
      "23 0.4300000000000001\n",
      "24 0.4300000000000001\n",
      "25 0.4300000000000001\n",
      "26 0.4300000000000001\n",
      "27 0.4300000000000001\n",
      "28 0.4300000000000001\n",
      "29 0.4300000000000001\n",
      "30 0.4300000000000001\n",
      "31 0.4300000000000001\n",
      "32 0.4300000000000001\n",
      "33 0.4300000000000001\n",
      "34 0.4300000000000001\n",
      "35 0.4300000000000001\n",
      "36 0.4300000000000001\n",
      "37 0.4300000000000001\n",
      "38 0.4300000000000001\n",
      "39 0.4300000000000001\n",
      "40 0.4300000000000001\n",
      "41 0.4300000000000001\n",
      "42 0.4300000000000001\n",
      "43 0.4300000000000001\n",
      "44 0.4300000000000001\n",
      "45 0.4300000000000001\n",
      "46 0.4300000000000001\n",
      "47 0.4300000000000001\n",
      "48 0.4300000000000001\n",
      "49 0.4300000000000001\n",
      "50 0.4300000000000001\n",
      "51 0.4300000000000001\n",
      "52 0.4300000000000001\n",
      "53 0.4300000000000001\n",
      "54 0.4300000000000001\n",
      "55 0.4300000000000001\n",
      "56 0.4300000000000001\n",
      "57 0.4300000000000001\n",
      "58 0.4300000000000001\n",
      "59 0.4300000000000001\n",
      "60 0.4300000000000001\n",
      "61 0.4300000000000001\n",
      "62 0.4300000000000001\n",
      "63 0.4300000000000001\n",
      "64 0.4300000000000001\n",
      "65 0.4300000000000001\n",
      "66 0.4300000000000001\n",
      "67 0.4300000000000001\n",
      "68 0.4300000000000001\n",
      "69 0.4300000000000001\n",
      "70 0.4300000000000001\n",
      "71 0.4300000000000001\n",
      "72 0.4300000000000001\n",
      "73 0.4300000000000001\n",
      "74 0.4300000000000001\n",
      "75 0.4300000000000001\n",
      "76 0.4300000000000001\n",
      "77 0.4300000000000001\n",
      "78 0.4300000000000001\n",
      "79 0.4300000000000001\n",
      "80 0.4300000000000001\n",
      "81 0.4300000000000001\n",
      "82 0.4300000000000001\n",
      "83 0.4300000000000001\n",
      "84 0.4300000000000001\n",
      "85 0.4300000000000001\n",
      "86 0.4300000000000001\n",
      "87 0.4300000000000001\n",
      "88 0.4300000000000001\n",
      "89 0.4300000000000001\n",
      "90 0.4300000000000001\n",
      "91 0.4300000000000001\n",
      "92 0.4300000000000001\n",
      "93 0.4300000000000001\n",
      "94 0.4300000000000001\n",
      "95 0.4300000000000001\n",
      "96 0.4300000000000001\n",
      "97 0.4300000000000001\n",
      "98 0.4300000000000001\n",
      "99 0.4300000000000001\n",
      "100 0.4300000000000001\n",
      "101 0.4300000000000001\n",
      "102 0.4300000000000001\n",
      "103 0.4300000000000001\n",
      "104 0.4300000000000001\n",
      "105 0.4300000000000001\n",
      "106 0.4300000000000001\n",
      "107 0.4300000000000001\n",
      "108 0.4300000000000001\n",
      "109 0.4300000000000001\n",
      "110 0.4300000000000001\n",
      "111 0.4300000000000001\n",
      "112 0.4300000000000001\n",
      "113 0.4300000000000001\n",
      "114 0.4300000000000001\n",
      "115 0.4300000000000001\n",
      "116 0.4300000000000001\n",
      "117 0.4300000000000001\n",
      "118 0.4300000000000001\n",
      "119 0.4300000000000001\n",
      "120 0.4300000000000001\n",
      "121 0.4300000000000001\n",
      "122 0.4300000000000001\n",
      "123 0.4300000000000001\n",
      "124 0.4300000000000001\n",
      "125 0.4300000000000001\n",
      "126 0.4300000000000001\n",
      "127 0.4300000000000001\n",
      "128 0.4300000000000001\n",
      "129 0.4300000000000001\n",
      "130 0.4300000000000001\n",
      "131 0.4300000000000001\n",
      "132 0.4300000000000001\n",
      "133 0.4300000000000001\n",
      "134 0.4300000000000001\n",
      "135 0.4300000000000001\n",
      "136 0.4300000000000001\n",
      "137 0.4300000000000001\n",
      "138 0.4300000000000001\n",
      "139 0.4300000000000001\n",
      "140 0.4300000000000001\n",
      "141 0.4300000000000001\n",
      "142 0.4300000000000001\n",
      "143 0.4300000000000001\n",
      "144 0.4300000000000001\n",
      "145 0.4300000000000001\n",
      "146 0.4300000000000001\n",
      "147 0.4300000000000001\n",
      "148 0.4300000000000001\n",
      "149 0.4300000000000001\n",
      "150 0.4300000000000001\n",
      "151 0.4300000000000001\n",
      "152 0.4300000000000001\n",
      "153 0.4300000000000001\n",
      "154 0.4300000000000001\n",
      "155 0.4300000000000001\n",
      "156 0.4300000000000001\n",
      "157 0.4300000000000001\n",
      "158 0.4300000000000001\n",
      "159 0.4300000000000001\n",
      "160 0.4300000000000001\n",
      "161 0.4300000000000001\n",
      "162 0.4300000000000001\n",
      "163 0.4300000000000001\n",
      "164 0.4300000000000001\n",
      "165 0.4300000000000001\n",
      "166 0.4300000000000001\n",
      "167 0.4300000000000001\n",
      "168 0.4300000000000001\n",
      "169 0.4300000000000001\n",
      "170 0.4300000000000001\n",
      "171 0.4300000000000001\n",
      "172 0.4300000000000001\n",
      "173 0.4300000000000001\n",
      "174 0.4300000000000001\n",
      "175 0.4300000000000001\n",
      "176 0.4300000000000001\n",
      "177 0.4300000000000001\n",
      "178 0.4300000000000001\n",
      "179 0.4300000000000001\n",
      "180 0.4300000000000001\n",
      "181 0.4300000000000001\n",
      "182 0.4300000000000001\n",
      "183 0.4300000000000001\n",
      "184 0.4300000000000001\n",
      "185 0.4300000000000001\n",
      "186 0.4300000000000001\n",
      "187 0.4300000000000001\n",
      "188 0.4300000000000001\n",
      "189 0.4300000000000001\n",
      "190 0.4300000000000001\n",
      "191 0.4300000000000001\n",
      "192 0.4300000000000001\n",
      "193 0.4300000000000001\n",
      "194 0.4300000000000001\n",
      "195 0.4300000000000001\n",
      "196 0.4300000000000001\n",
      "197 0.4300000000000001\n",
      "198 0.4300000000000001\n",
      "199 0.4300000000000001\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 200\n",
    "TAXA_DE_APRENDIZADO = 0.2\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for exemplo in x:\n",
    "        previsao = minha_mlp_relu(exemplo)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # loss\n",
    "    erros = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        residuo = yp - yt\n",
    "        erro_quadratico = residuo ** 2\n",
    "        erros.append(erro_quadratico)        \n",
    "    loss = (sum(erros))/len(erros)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp_relu.parametros():\n",
    "        p.grad = 0\n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    for p in minha_mlp_relu.parametros():\n",
    "        p.data = p.data - p.grad * TAXA_DE_APRENDIZADO\n",
    "\n",
    "    # mostra resultado (opcional)\n",
    "    print(epoca, loss.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TESTE COM ELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(8)\n",
    "minha_mlp_elu = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede, fn_ativacao=\"elu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2331156539577761\n",
      "1 0.12322279004712496\n",
      "2 0.06867608106389582\n",
      "3 0.04339159023082039\n",
      "4 0.032683344340618176\n",
      "5 0.02531253028229287\n",
      "6 0.019460005871904565\n",
      "7 0.01734967332768972\n",
      "8 0.019114868507831936\n",
      "9 0.027844699475921688\n",
      "10 0.05047882188668168\n",
      "11 0.09189608709583783\n",
      "12 0.13194935124839863\n",
      "13 0.166375727302536\n",
      "14 0.11780457746728226\n",
      "15 0.10598079099575855\n",
      "16 0.06883665208274006\n",
      "17 0.06324671845178527\n",
      "18 0.05090183538259163\n",
      "19 0.051012630676509546\n",
      "20 0.04624020733366168\n",
      "21 0.049375534629619314\n",
      "22 0.0467631674145483\n",
      "23 0.051506131339594585\n",
      "24 0.048852830315909014\n",
      "25 0.05400389349319215\n",
      "26 0.05030881557563778\n",
      "27 0.05483883504851244\n",
      "28 0.050009100123117906\n",
      "29 0.05344099448129918\n",
      "30 0.04808426522923427\n",
      "31 0.050543788607982455\n",
      "32 0.04540041894680417\n",
      "33 0.047253524237298845\n",
      "34 0.0427534022702317\n",
      "35 0.04429111305614987\n",
      "36 0.04052030490871995\n",
      "37 0.04187252037158521\n",
      "38 0.0387322410185883\n",
      "39 0.03964179101300076\n",
      "40 0.03704935080657556\n",
      "41 0.0374643512409913\n",
      "42 0.035391813714581874\n",
      "43 0.03538965045991998\n",
      "44 0.03381165055641872\n",
      "45 0.03347124223464034\n",
      "46 0.03234099990266426\n",
      "47 0.03172189998087104\n",
      "48 0.03094740014186432\n",
      "49 0.030094775761917044\n",
      "50 0.029477579052499606\n",
      "51 0.028449085157977935\n",
      "52 0.027996847872025683\n",
      "53 0.02685417489168543\n",
      "54 0.026575378548370745\n",
      "55 0.02536779632702372\n",
      "56 0.025258652035980378\n",
      "57 0.0240161683677858\n",
      "58 0.024057789368763238\n",
      "59 0.022793494955275717\n",
      "60 0.022956807105973864\n",
      "61 0.021674260557099007\n",
      "62 0.021927846743689375\n",
      "63 0.02062869283466981\n",
      "64 0.020945515540745977\n",
      "65 0.01963424406460744\n",
      "66 0.019995076030832705\n",
      "67 0.01868009627053625\n",
      "68 0.019073439658198563\n",
      "69 0.017765468926154316\n",
      "70 0.018185198433716077\n",
      "71 0.01689474479655595\n",
      "72 0.017337199563331154\n",
      "73 0.016072615054497037\n",
      "74 0.016534418683169384\n",
      "75 0.01530114282284506\n",
      "76 0.015778201619558573\n",
      "77 0.014579050256132237\n",
      "78 0.01506652855013527\n",
      "79 0.013902527657210501\n",
      "80 0.014395336454746527\n",
      "81 0.013266613552653812\n",
      "82 0.013759988218439271\n",
      "83 0.012666426489737365\n",
      "84 0.013156339147770904\n",
      "85 0.012097908037253958\n",
      "86 0.01258123394586719\n",
      "87 0.011558052226565602\n",
      "88 0.012032527104759898\n",
      "89 0.011044772996468992\n",
      "90 0.01150882910329817\n",
      "91 0.010556606525182323\n",
      "92 0.01100917445409895\n",
      "93 0.010092406973730556\n",
      "94 0.010531601196137767\n",
      "95 0.00965019391163875\n",
      "96 0.010058256359718435\n",
      "97 0.009213315462785891\n",
      "98 0.009588305845836332\n",
      "99 0.008783493909840474\n",
      "100 0.00912835490397435\n",
      "101 0.008366750239272207\n",
      "102 0.008686269030100306\n",
      "103 0.00796907727631144\n",
      "104 0.008267214367253091\n",
      "105 0.007593839843229037\n",
      "106 0.007873424437426465\n",
      "107 0.007241941022994825\n",
      "108 0.007504764134291696\n",
      "109 0.006912484069268353\n",
      "110 0.007159567355913263\n",
      "111 0.006603530132784015\n",
      "112 0.006835423404694178\n",
      "113 0.006312737271671618\n",
      "114 0.006529767950912358\n",
      "115 0.006037803256572976\n",
      "116 0.006240250660187403\n",
      "117 0.0057767156506789066\n",
      "118 0.005964911886537867\n",
      "119 0.005527850129380606\n",
      "120 0.005702222890980648\n",
      "121 0.005289967823096725\n",
      "122 0.005451044041950086\n",
      "123 0.005062157395364173\n",
      "124 0.005210544960139942\n",
      "125 0.004843756265726177\n",
      "126 0.004980116897554395\n",
      "127 0.0046342731036538585\n",
      "128 0.004759294917610537\n",
      "129 0.00443332323282951\n",
      "130 0.004547697580861091\n",
      "131 0.004240580980300732\n",
      "132 0.00434498529403011\n",
      "133 0.004055748304807337\n",
      "134 0.004150834860043385\n",
      "135 0.003878536705089096\n",
      "136 0.0039649263169551\n",
      "137 0.00370865871583718\n",
      "138 0.0037869380746485583\n",
      "139 0.00354582556574113\n",
      "140 0.003616546986977798\n",
      "141 0.003389748276323495\n",
      "142 0.0034534308704240423\n",
      "143 0.003240140281045015\n",
      "144 0.0032972718246283683\n",
      "145 0.003096720356047617\n",
      "146 0.0031477593966978234\n",
      "147 0.0029592152022545823\n",
      "148 0.003004593125544906\n",
      "149 0.002827361395906913\n",
      "150 0.0028674843215899023\n",
      "151 0.002700906656862153\n",
      "152 0.0027361571190234763\n",
      "153 0.002579610508102579\n",
      "154 0.0026103489237720816\n",
      "155 0.002463244451590055\n",
      "156 0.002489810405892912\n",
      "157 0.0023515917937801303\n",
      "158 0.0023743051771307807\n",
      "159 0.002244447239600653\n",
      "160 0.002263609270967298\n",
      "161 0.0021416163500813374\n",
      "162 0.002157510514813329\n",
      "163 0.002042914934035922\n",
      "164 0.0020558078580681467\n",
      "165 0.0019481684222715239\n",
      "166 0.0019583112259795935\n",
      "167 0.001857062003633191\n",
      "168 0.0018646980157393943\n",
      "169 0.0017688045432344335\n",
      "170 0.0017741185147397997\n",
      "171 0.0016833821527780215\n",
      "172 0.0016865993433265284\n",
      "173 0.0016008450115332661\n",
      "174 0.0016022017073746325\n",
      "175 0.0015212603456699727\n",
      "176 0.0015209890637816067\n",
      "177 0.001444688253677062\n",
      "178 0.001443008811924205\n",
      "179 0.0013711690138166431\n",
      "180 0.0013682838023358753\n",
      "181 0.0013007180039450506\n",
      "182 0.001296810064901684\n",
      "183 0.0012333252771525745\n",
      "184 0.0012285581453720983\n",
      "185 0.0011689577194372793\n",
      "186 0.0011634762871969683\n",
      "187 0.001107562425741128\n",
      "188 0.0011014943397028593\n",
      "189 0.0010490704512255125\n",
      "190 0.0010425277285672212\n",
      "191 0.0009934004535744622\n",
      "192 0.0009864811289155642\n",
      "193 0.0009404619773506544\n",
      "194 0.0009332516750677971\n",
      "195 0.0008901582780097567\n",
      "196 0.0008827316576300065\n",
      "197 0.0008423886692270575\n",
      "198 0.0008348107240421534\n",
      "199 0.0007970504236486629\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 200\n",
    "TAXA_DE_APRENDIZADO = 0.5\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for exemplo in x:\n",
    "        previsao = minha_mlp_elu(exemplo)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # loss\n",
    "    erros = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        residuo = yp - yt\n",
    "        erro_quadratico = residuo ** 2\n",
    "        erros.append(erro_quadratico)        \n",
    "    loss = (sum(erros))/len(erros)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp_elu.parametros():\n",
    "        p.grad = 0\n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    for p in minha_mlp_elu.parametros():\n",
    "        p.data = p.data - p.grad * TAXA_DE_APRENDIZADO\n",
    "\n",
    "    # mostra resultado (opcional)\n",
    "    print(epoca, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TESTE COM SWISH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(8)\n",
    "minha_mlp_swish = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede, fn_ativacao=\"swish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5959063761944281\n",
      "1 0.3871558915745604\n",
      "2 0.20615485075489506\n",
      "3 0.1464762939964329\n",
      "4 0.13658688601161662\n",
      "5 0.13396170082821787\n",
      "6 0.13224557183173066\n",
      "7 0.13085451546712715\n",
      "8 0.1296264622373265\n",
      "9 0.12848336301412908\n",
      "10 0.12738270758049294\n",
      "11 0.12629876622056943\n",
      "12 0.12521401271283603\n",
      "13 0.12411495028576738\n",
      "14 0.12298990641585489\n",
      "15 0.12182771136906445\n",
      "16 0.1206167529936587\n",
      "17 0.11934415170866562\n",
      "18 0.11799490725958908\n",
      "19 0.11655090938295634\n",
      "20 0.11498971059052794\n",
      "21 0.11328294438901616\n",
      "22 0.11139424198646215\n",
      "23 0.10927645881601795\n",
      "24 0.10686797810217907\n",
      "25 0.10408784030333924\n",
      "26 0.10082953624790691\n",
      "27 0.09695372200423147\n",
      "28 0.09228145352936434\n",
      "29 0.08659326293533368\n",
      "30 0.07964871238505485\n",
      "31 0.07126060725886904\n",
      "32 0.06148245073844243\n",
      "33 0.05093078327226679\n",
      "34 0.04098852697754623\n",
      "35 0.03325430427216584\n",
      "36 0.028341103705910795\n",
      "37 0.025601367061482062\n",
      "38 0.024042315420246967\n",
      "39 0.022984001892922098\n",
      "40 0.02209228812470658\n",
      "41 0.02122730210549709\n",
      "42 0.02033415526300647\n",
      "43 0.01939152703337675\n",
      "44 0.0183914462720756\n",
      "45 0.017332182226914985\n",
      "46 0.01621611076742711\n",
      "47 0.015049352431560484\n",
      "48 0.01384191873112773\n",
      "49 0.012607793198247792\n",
      "50 0.01136462794507849\n",
      "51 0.010132884058412582\n",
      "52 0.00893440301020385\n",
      "53 0.007790577172458191\n",
      "54 0.006720438624862732\n",
      "55 0.0057390392724321285\n",
      "56 0.004856421415978217\n",
      "57 0.004077309749270825\n",
      "58 0.0034014701316094596\n",
      "59 0.002824548816861453\n",
      "60 0.002339157958384031\n",
      "61 0.001935996249139822\n",
      "62 0.0016048543846034328\n",
      "63 0.0013354229110574978\n",
      "64 0.0011178766041347443\n",
      "65 0.0009432482248319765\n",
      "66 0.0008036259465593018\n",
      "67 0.0006922167275853447\n",
      "68 0.0006033169253237343\n",
      "69 0.0005322255168311701\n",
      "70 0.00047512741834030054\n",
      "71 0.000428966541053708\n",
      "72 0.00039132143508189003\n",
      "73 0.00036029106110956424\n",
      "74 0.000334394389548117\n",
      "75 0.0003124849532617901\n",
      "76 0.00029367989410271133\n",
      "77 0.00027730217243730407\n",
      "78 0.00026283422136834904\n",
      "79 0.00024988124494264286\n",
      "80 0.00023814245377128614\n",
      "81 0.00022738871507661058\n",
      "82 0.00021744531163439643\n",
      "83 0.00020817872231178745\n",
      "84 0.000199486538033587\n",
      "85 0.0001912898029280064\n",
      "86 0.00018352721889136503\n",
      "87 0.00017615077398072727\n",
      "88 0.0001691224536219467\n",
      "89 0.0001624117720033667\n",
      "90 0.00015599392258247905\n",
      "91 0.00014984839456324447\n",
      "92 0.0001439579391627694\n",
      "93 0.00013830779790457556\n",
      "94 0.00013288512676598834\n",
      "95 0.00012767856653891503\n",
      "96 0.0001226779220848674\n",
      "97 0.00011787392272289093\n",
      "98 0.00011325804280394388\n",
      "99 0.0001088223671642266\n",
      "100 0.00010455948963178492\n",
      "101 0.00010046243640239629\n",
      "102 9.652460735748698e-05\n",
      "103 9.273973138159703e-05\n",
      "104 8.910183108518755e-05\n",
      "105 8.560519587824013e-05\n",
      "106 8.22443593346562e-05\n",
      "107 7.901408244004791e-05\n",
      "108 7.590933748907105e-05\n",
      "109 7.292529773886343e-05\n",
      "110 7.005732410138757e-05\n",
      "111 6.730095992504254e-05\n",
      "112 6.46519177935094e-05\n",
      "113 6.210608050582677e-05\n",
      "114 5.9659485836155675e-05\n",
      "115 5.7308338859752555e-05\n",
      "116 5.504899462798462e-05\n",
      "117 5.2877998368954106e-05\n",
      "118 5.079207835872952e-05\n",
      "119 4.8788270209390346e-05\n",
      "120 4.6863992139346266e-05\n",
      "121 4.501746217763901e-05\n",
      "122 4.324819152598984e-05\n",
      "123 4.1558519953363674e-05\n",
      "124 3.995595869769732e-05\n",
      "125 3.84592422423493e-05\n",
      "126 3.710829057885389e-05\n",
      "127 3.5989310895757134e-05\n",
      "128 3.527405619093753e-05\n",
      "129 3.532946221985422e-05\n",
      "130 3.685627169589007e-05\n",
      "131 4.140279309260637e-05\n",
      "132 5.1729618696265544e-05\n",
      "133 7.446586889937129e-05\n",
      "134 0.00011987994319516541\n",
      "135 0.00021730238813247847\n",
      "136 0.00039979621984263943\n",
      "137 0.0008033882757031978\n",
      "138 0.0014850235984257513\n",
      "139 0.0030956917743340607\n",
      "140 0.005198111221189597\n",
      "141 0.010699738149773208\n",
      "142 0.013624125723936656\n",
      "143 0.02433706629998516\n",
      "144 0.018231322649703986\n",
      "145 0.022696725292905354\n",
      "146 0.012512165946278825\n",
      "147 0.011574163585217031\n",
      "148 0.007091419880154804\n",
      "149 0.006550164845389325\n",
      "150 0.004701614566279601\n",
      "151 0.00464706756630596\n",
      "152 0.003739177593154724\n",
      "153 0.003982142587822194\n",
      "154 0.0034736060826961834\n",
      "155 0.003958163044569794\n",
      "156 0.003631943301211792\n",
      "157 0.0043531534748272056\n",
      "158 0.004076083159831051\n",
      "159 0.005025681433893709\n",
      "160 0.004671463660458424\n",
      "161 0.005792846832246851\n",
      "162 0.005238912052608444\n",
      "163 0.00641050782907891\n",
      "164 0.005592447323109931\n",
      "165 0.006671573963775799\n",
      "166 0.00563586448487817\n",
      "167 0.0065353347320398\n",
      "168 0.005411804092724632\n",
      "169 0.006129556464322637\n",
      "170 0.005045707913483391\n",
      "171 0.0056300681269419416\n",
      "172 0.004656823771395849\n",
      "173 0.005161741728157672\n",
      "174 0.0043154943164639996\n",
      "175 0.004780083931854029\n",
      "176 0.004046931492613553\n",
      "177 0.004493931821324935\n",
      "178 0.0038495100041414307\n",
      "179 0.004289732877971598\n",
      "180 0.003709578634191596\n",
      "181 0.004146266970309333\n",
      "182 0.003609950867536531\n",
      "183 0.004041873315605053\n",
      "184 0.003534180541584034\n",
      "185 0.003957775224121644\n",
      "186 0.0034686503566525593\n",
      "187 0.0038795886794264333\n",
      "188 0.003403505717802723\n",
      "189 0.0037978211346672134\n",
      "190 0.003332816013092883\n",
      "191 0.003707603866168297\n",
      "192 0.0032541317379393\n",
      "193 0.0036078244334142497\n",
      "194 0.0031676303581013366\n",
      "195 0.0034999219779334335\n",
      "196 0.003075104055138944\n",
      "197 0.0033866588457964054\n",
      "198 0.0029790304971541977\n",
      "199 0.0032711183943303305\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 200\n",
    "TAXA_DE_APRENDIZADO = 0.5\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for exemplo in x:\n",
    "        previsao = minha_mlp_swish(exemplo)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # loss\n",
    "    erros = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        residuo = yp - yt\n",
    "        erro_quadratico = residuo ** 2\n",
    "        erros.append(erro_quadratico)        \n",
    "    loss = (sum(erros))/len(erros)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp_swish.parametros():\n",
    "        p.grad = 0\n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    for p in minha_mlp_swish.parametros():\n",
    "        p.data = p.data - p.grad * TAXA_DE_APRENDIZADO\n",
    "\n",
    "    # mostra resultado (opcional)\n",
    "    print(epoca, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DISCUSSÃO DOS RESULTADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir das implementações e resultados obtidos durante as épocas com cada função de ativação, foi possível perceber o aprendizado acontecer de formas bem distintas nas diferentes funções. \n",
    "\n",
    "No caso da ***ReLU***, foi possível notar um valor constante durante todas as épocas. Isso pode ser um sinal claro de neurônios mortos encontrados na rede - problema grave encontrado comumente nas redes com essa função de ativação. Assim, vemos que para as entradas que disponibilizamos e modelo, essa função não é muito adequada.\n",
    "\n",
    "Para a ***ELU***, percebe-se uma grande faixa de \"altos e baixos\" nas primeiras épocas de aprendizado. Entretanto, com o passar das épocas, pode-se notar uma tendência de queda na perda, demonstrando um aprendimento efetivo do modelo com a função - e sem neurônios mortos dessa vez!!!!\n",
    "\n",
    "Por fim, a ***Swish***, por ser a \"mais próxima\" na teoria da já conhecida sigmoidal, o aprendizado e queda dos valores de perda observados foi similar, de modo a obter valores interessantes de queda sem muitas variações drásticas como na ELU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONCLUSÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação das funções de ativação ReLU, ELU e Swish demonstrou que é possível alterar uma rede neural a partir somente das suas funções de ativação aplicadas. Os testes mostraram que a rede continua funcional e capaz de aprender durante as épocas, evidenciando a importância da escolha da função de ativação no desempenho e comportamento do modelo. Além disso, as diferenças matemáticas e práticas em relação à função sigmoidal foram observadas, especialmente em relação às derivadas calculadas e a propagação do gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **REFERÊNCIAS**\n",
    "\n",
    "**[1]** CASSAR, Daniel. Redes Neurais e Algoritmos Genéticos. 2025. Material de Aula.\n",
    "\n",
    "**[2]** ESTEVES, Toni. A desvantagem da função ReLU. Medium. 2022. Disponível em: https://estevestoni.medium.com/a-desvantagem-de-utilizar-relu-4478589ef834\n",
    "\n",
    "**[3]** TORCH. Activation functions source code. GitHub. Disponível em: https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L422.\n",
    "\n",
    "**[4]** PAPERS WITH CODE. ELU. Disponível em: https://paperswithcode.com/method/elu. \n",
    "\n",
    "**[5]** ALMEIDA, Vitor. Funções de ativação de redes neurais e como são cobradas em concursos. Gran Cursos Online. 2023. Disponível em: https://blog.grancursosonline.com.br/funcoes-de-ativacao-de-redes-neurais-e-como-sao-cobradas-em-concursos/.\n",
    "\n",
    "**[6]** NIKHADE, Amit. Swish Activation Function. Medium. 2020. Disponível em: https://amitnikhade.medium.com/swish-activation-function-d106fe13930e."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
